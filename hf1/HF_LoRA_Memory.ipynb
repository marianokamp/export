{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f376481d-e34a-477f-bad2-b5f04a3d4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48081996-4be8-474c-985e-c77412b0a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d241390-ea0f-485f-8a3b-f1385cc09bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('png')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import boto3\n",
    "sm = boto3.client('sagemaker')\n",
    "alt.renderers.enable('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef3303fc-84ea-4638-a21c-da6d14a84562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting source/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile source/requirements.txt\n",
    "transformers\n",
    "accelerate>=0.20.1\n",
    "datasets\n",
    "evaluate\n",
    "peft\n",
    "\n",
    "pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd356777-a457-4c58-8792-9bb9a869e328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting source/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile source/train.py\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from threading import Thread\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import peft\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "logger.info(f'transformers {transformers.__version__}')\n",
    "logger.info(f'peft {peft.__version__}')\n",
    "logger.info(f'torch {torch.__version__}')\n",
    "\n",
    "def count_parameters(m, verbose=True):\n",
    "    total_count = 0\n",
    "    learnable_count = 0\n",
    "    if verbose:\n",
    "        logger.debug(\"Parameters (name, tunable, count):\")\n",
    "\n",
    "    output_width = max([len(n) for n, _ in m.named_parameters()])\n",
    "    for n, p in m.named_parameters():\n",
    "        count = p.data.numel()\n",
    "        if verbose:\n",
    "            logger.debug(f\" {n:{output_width}} {p.requires_grad:5b} {count:>11d}\")\n",
    "        total_count += count\n",
    "        if p.requires_grad:\n",
    "            learnable_count += count\n",
    "\n",
    "    logger.info(\n",
    "        f\"Total parameters: {total_count:,}, \"\n",
    "        f\"thereof learnable: {learnable_count:,} \"\n",
    "        f\"({learnable_count/total_count*100.:5.4f}%)\"\n",
    "    )\n",
    "\n",
    "    return total_count, learnable_count\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    load_accuracy = evaluate.load(\"accuracy\")\n",
    " \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\n",
    "        \"accuracy\"\n",
    "    ]\n",
    " \n",
    "    metrics = {f\"accuracy\": accuracy}\n",
    " \n",
    "    return metrics\n",
    "\n",
    "def fit(args):\n",
    "    ### model / tokenizer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.hf_ckp, num_labels=2)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.hf_ckp)\n",
    "    collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    use_lora = True if args.use_hf_lora else False\n",
    "    if args.use_hf_lora:\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            inference_mode=False,\n",
    "            r=8,\n",
    "            lora_alpha=8,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=None # That apparently becomes query and key then\n",
    "        )\n",
    "        model = get_peft_model(model, peft_config)\n",
    "\n",
    "    count_parameters(model, verbose=args.use_hf_lora)\n",
    "\n",
    "    ### data \n",
    "    datasets.logging.disable_progress_bar()\n",
    "    dataset = datasets.load_dataset(\"glue\", \"sst2\")\n",
    "    train = dataset[\"train\"]\n",
    "    valid = dataset[\"validation\"]\n",
    "\n",
    "    def preprieocess_function(examples):\n",
    "        return tokenizer(args.input_scale*examples['sentence'], padding=False, truncation=True)\n",
    "\n",
    "    tokenized_train = train.map(preprocess_function, batched=False)\n",
    "    tokenized_valid = valid.map(preprocess_function, batched=False)\n",
    "\n",
    "    mean_length = np.mean([len(v['input_ids']) for v in tokenized_train])\n",
    "    logger.info(f'Average train input length: {mean_length:5.2f}')\n",
    "\n",
    "    ### Trainer\n",
    "    \n",
    "    log_steps = len(tokenized_train) // args.batch_size // 2 # 2 log outputs per epoch\n",
    "    use_fp16 = True if args.use_fp16 and torch.cuda.is_available() else False\n",
    "    print(f'Using fp16: {use_fp16}, LoRA: {use_lora}')\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.model_dir if args.model_dir else \"out\",    \n",
    "        learning_rate=args.learning_rate,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.1,\n",
    "        push_to_hub=False,\n",
    "        fp16=use_fp16,\n",
    "        save_steps=log_steps,\n",
    "        logging_steps=log_steps,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        disable_tqdm=True,\n",
    "        metric_for_best_model='eval_accuracy',\n",
    "        load_best_model_at_end=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        # no early stopping for simplicity\n",
    "    )\n",
    "\n",
    "    ### train\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def schedule_gpu_memory_logging():\n",
    "        def log_gpu_usage():\n",
    "            if not torch.cuda.is_available():\n",
    "                return\n",
    "\n",
    "            from pynvml.smi import nvidia_smi\n",
    "\n",
    "            nvsmi = nvidia_smi.getInstance()\n",
    "            res = nvsmi.DeviceQuery(\"memory.free, memory.total, memory.used\")[\"gpu\"][0][\n",
    "                \"fb_memory_usage\"\n",
    "            ]\n",
    "            res[\"percentage\"] = res[\"used\"] / res[\"total\"] * 100\n",
    "            logger.info(\n",
    "                f'GPU Usage. Used: {res[\"used\"]:5.3f} Total: {res[\"total\"]:5.3f} ({res[\"percentage\"]:3.1f}% used). Free: {res[\"free\"]:5.3f}'\n",
    "            )\n",
    "        \n",
    "        def log_loop():\n",
    "            while True:\n",
    "                log_gpu_usage()\n",
    "                time.sleep(30)\n",
    "    \n",
    "        t = Thread(target=log_loop, daemon=True)\n",
    "        t.start()\n",
    "\n",
    "    schedule_gpu_memory_logging()\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--use-fp16\", type=int, default=1, help='1 yes, 0 no') \n",
    "    parser.add_argument(\"--use-hf-lora\", type=int, default=0, help='1 yes, 0 no')\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=4e-5)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=224)\n",
    "    parser.add_argument(\"--input-scale\", type=int, default=1)\n",
    "    parser.add_argument(\"--hf-ckp\", type=str, default='roberta-base')\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    fit(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40339088-31b7-45f0-8c39-ed11183a7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run locally with:\n",
    "#!python source/train.py --input-scale 1 --batch-size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba11b5db-5372-45d5-bbf7-dff1488edf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "hyperparameters = {\n",
    "    'use-fp16'  : 1,\n",
    "    'use-hf-lora': 1,\n",
    "    'learning-rate': 4e-5,\n",
    "    'batch-size': 16, # 224\n",
    "    'hf-ckp': 'roberta-base'\n",
    "}\n",
    "\n",
    "metric_definitions = [\n",
    "    \n",
    "    {'Name': 'train_samples_per_second', 'Regex': '\\'train_samples_per_second\\': (-?[0-9\\\\.]+)'},\n",
    "    {'Name': 'valid_acc', 'Regex': '\\'eval_accuracy\\': (-?[0-9\\\\.]+)'},\n",
    "    {'Name': 'gpu_mem', 'Regex': 'GPU Usage.*?(-?[0-9\\\\.]+)% used'}\n",
    "]\n",
    "\n",
    "estimator_parameters = dict(\n",
    "    source_dir         = 'source',\n",
    "    entry_point        = 'train.py',\n",
    "    instance_type      = 'ml.g4dn.xlarge',\n",
    "    instance_count     = 1,\n",
    "    framework_version  = '2.0',\n",
    "    py_version         = 'py310',\n",
    "    use_spot_instances = True,\n",
    "    max_run            = 24*60*60, # one day in seconds\n",
    "    max_wait           = 24*60*60, \n",
    "    role               = get_execution_role(),\n",
    "    metric_definitions = metric_definitions,\n",
    "    hyperparameters    = hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d340569-5a37-47ed-b075-7bb30299ea57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-06-26-12-18-30-213\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-06-26-12-18-31-760\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-06-26-12-18-33-098\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-06-26-12-18-34-425\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-06-26-12-18-35-678\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-06-26-12-18-36-915\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-06-26-12-18-38-166\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-06-26-12-18-39-489\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "for type in ['lora', 'full']:\n",
    "    for bits in [16, 32]:\n",
    "        for input_scale in [1, 8]:\n",
    "            est = PyTorch(**estimator_parameters)\n",
    "            est.set_hyperparameters(**{'use-fp16':      1    if bits == 16 else 0})\n",
    "            est.set_hyperparameters(**{'use-hf-lora':   1    if type == 'lora' else 0})\n",
    "            est.set_hyperparameters(**{'learning-rate': 4e-4 if type == 'lora' else 5e-5})\n",
    "            est.set_hyperparameters(**{'input-scale': input_scale})\n",
    "            est.fit(wait=False)\n",
    "            estimators.append((type, bits, est, input_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0db83178-bea5-4fc1-9980-a606c013b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_metrics(estimator, metric_names=None):\n",
    "    if isinstance(metric_names, str):\n",
    "        metrics = [metric_names]\n",
    "        \n",
    "    final_metrics = sm.describe_training_job(\n",
    "        TrainingJobName=estimator.latest_training_job.job_name)['FinalMetricDataList']\n",
    "    return {fm['MetricName']: fm['Value'] for fm in final_metrics if metric_names is None or (fm['MetricName'] in metric_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47481cc6-9a49-4cc3-ac2f-1bdec64ecb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>bits</th>\n",
       "      <th>input_scale</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>gpu_memory</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lora</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.943807</td>\n",
       "      <td>13.900000</td>\n",
       "      <td>306.411011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lora</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0.932339</td>\n",
       "      <td>95.699997</td>\n",
       "      <td>54.373001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lora</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.940367</td>\n",
       "      <td>14.700000</td>\n",
       "      <td>212.406998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lora</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>0.935780</td>\n",
       "      <td>72.900002</td>\n",
       "      <td>29.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>full</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.912844</td>\n",
       "      <td>23.799999</td>\n",
       "      <td>177.626999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>full</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0.911697</td>\n",
       "      <td>71.300003</td>\n",
       "      <td>45.835999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>full</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.919725</td>\n",
       "      <td>24.100000</td>\n",
       "      <td>120.387001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>full</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>0.813073</td>\n",
       "      <td>85.800003</td>\n",
       "      <td>23.254000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  bits  input_scale  accuracy  gpu_memory  train_samples_per_second\n",
       "0  lora    16            1  0.943807   13.900000                306.411011\n",
       "1  lora    16            8  0.932339   95.699997                 54.373001\n",
       "2  lora    32            1  0.940367   14.700000                212.406998\n",
       "3  lora    32            8  0.935780   72.900002                 29.090000\n",
       "4  full    16            1  0.912844   23.799999                177.626999\n",
       "5  full    16            8  0.911697   71.300003                 45.835999\n",
       "6  full    32            1  0.919725   24.100000                120.387001\n",
       "7  full    32            8  0.813073   85.800003                 23.254000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "# capture data\n",
    "for type, bits, est, input_scale in estimators:\n",
    "    job_name = est.latest_training_job.job_name\n",
    "    sm.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\n",
    "    metrics = get_job_metrics(est, ['valid_acc', 'gpu_mem', 'train_samples_per_second'])\n",
    "    results.append(\n",
    "        (type, \n",
    "         bits,\n",
    "         input_scale,\n",
    "         metrics['valid_acc'], \n",
    "         metrics['gpu_mem'],\n",
    "         metrics['train_samples_per_second'])\n",
    "    )\n",
    "results_df = pd.DataFrame(data=results, columns=['type', 'bits', 'input_scale', 'accuracy', 'gpu_memory', 'train_samples_per_second']); results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee6f617-b88b-474a-be22-7c6e3dc96564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|    | type   |   bits |   input_scale |   accuracy |   gpu_memory |   train_samples_per_second |\\n|---:|:-------|-------:|--------------:|-----------:|-------------:|---------------------------:|\\n|  0 | lora   |     16 |             1 |   0.943807 |         13.9 |                    306.411 |\\n|  1 | lora   |     16 |             8 |   0.932339 |         95.7 |                     54.373 |\\n|  2 | lora   |     32 |             1 |   0.940367 |         14.7 |                    212.407 |\\n|  3 | lora   |     32 |             8 |   0.93578  |         72.9 |                     29.09  |\\n|  4 | full   |     16 |             1 |   0.912844 |         23.8 |                    177.627 |\\n|  5 | full   |     16 |             8 |   0.911697 |         71.3 |                     45.836 |\\n|  6 | full   |     32 |             1 |   0.919725 |         24.1 |                    120.387 |\\n|  7 | full   |     32 |             8 |   0.813073 |         85.8 |                     23.254 |'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.to_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab683a06-cc3f-4198-9fa4-77dfed7d2d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for title, field in [\n",
    "    ('GPU Memory', 'gpu_memory'),\n",
    "    ('Model Performance', 'accuracy'),\n",
    "    ('Training Speed', 'train_samples_per_second'),\n",
    "]:\n",
    "   display(alt.Chart(results_df, title=title, width=400).mark_bar(color='orange').encode(\n",
    "        x='type:N',\n",
    "        y=alt.Y(f'{field}:Q').scale(zero=False),\n",
    "    ).facet(column='bits:O', row='input_scale:O'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f4d4980-9a04-433e-966c-08c2b385f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All experiments use the same batch-size\n",
    "# All experiments ran on their own machines/GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c132e86-54da-4a1e-8bb8-0e2572bd46df",
   "metadata": {},
   "source": [
    "## Full Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0998e5d6-3abf-43a0-ab7f-74fac2a925dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "##### Log of pytorch-training-2023-06-26-12-18-30-213 (lora, 16):\n",
      "\n",
      "2023-06-26 12:44:38 Starting - Preparing the instances for training\n",
      "2023-06-26 12:44:38 Downloading - Downloading input data\n",
      "2023-06-26 12:44:38 Training - Training image download completed. Training in progress.\n",
      "2023-06-26 12:44:38 Uploading - Uploading generated training model\n",
      "2023-06-26 12:44:38 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:07,394 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:07,412 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:07,421 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:07,429 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:09,429 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 62.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate>=0.20.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.20.3-py3-none-any.whl (227 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.6/227.6 kB 44.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.13.1-py3-none-any.whl (486 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 486.2/486.2 kB 66.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 23.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.3.0-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 16.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 17.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.14.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.8/236.8 kB 41.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 770.4/770.4 kB 66.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 69.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 95.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 42.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 88.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 28.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 52.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 38.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, accelerate, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.20.3 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.13.1 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 peft-0.3.0 pynvml-11.5.0 regex-2023.6.3 responses-0.18.0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,131 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,131 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,149 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,176 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,203 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,213 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"1\",\n",
      "        \"learning-rate\": \"0.0004\",\n",
      "        \"use-fp16\": \"1\",\n",
      "        \"use-hf-lora\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-06-26-12-18-30-213\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-30-213/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"1\",\"learning-rate\":\"0.0004\",\"use-fp16\":\"1\",\"use-hf-lora\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-30-213/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"1\",\"learning-rate\":\"0.0004\",\"use-fp16\":\"1\",\"use-hf-lora\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-06-26-12-18-30-213\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-30-213/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"1\",\"--learning-rate\",\"0.0004\",\"--use-fp16\",\"1\",\"--use-hf-lora\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-SCALE=1\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.0004\u001b[0m\n",
      "\u001b[34mSM_HP_USE-FP16=1\u001b[0m\n",
      "\u001b[34mSM_HP_USE-HF-LORA=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 1 --learning-rate 0.0004 --use-fp16 1 --use-hf-lora 1\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,245 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:__main__:transformers 4.30.2\u001b[0m\n",
      "\u001b[34mINFO:__main__:peft 0.3.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:torch 2.0.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 481/481 [00:00<00:00, 3.56MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   2%|▏         | 10.5M/499M [00:00<00:11, 41.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   4%|▍         | 21.0M/499M [00:00<00:10, 44.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   6%|▋         | 31.5M/499M [00:00<00:10, 46.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   8%|▊         | 41.9M/499M [00:00<00:09, 49.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  11%|█         | 52.4M/499M [00:01<00:08, 50.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  13%|█▎        | 62.9M/499M [00:01<00:08, 49.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  15%|█▍        | 73.4M/499M [00:01<00:08, 51.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  17%|█▋        | 83.9M/499M [00:01<00:09, 46.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  19%|█▉        | 94.4M/499M [00:01<00:08, 47.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  21%|██        | 105M/499M [00:02<00:08, 46.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  23%|██▎       | 115M/499M [00:02<00:08, 47.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  25%|██▌       | 126M/499M [00:02<00:07, 49.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  27%|██▋       | 136M/499M [00:02<00:07, 48.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  29%|██▉       | 147M/499M [00:03<00:07, 46.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  32%|███▏      | 157M/499M [00:03<00:08, 40.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  34%|███▎      | 168M/499M [00:03<00:07, 42.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  36%|███▌      | 178M/499M [00:03<00:07, 44.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  38%|███▊      | 189M/499M [00:04<00:06, 45.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  40%|███▉      | 199M/499M [00:04<00:06, 48.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  42%|████▏     | 210M/499M [00:04<00:06, 47.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  44%|████▍     | 220M/499M [00:04<00:05, 49.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  46%|████▌     | 231M/499M [00:04<00:05, 46.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  48%|████▊     | 241M/499M [00:05<00:05, 47.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  50%|█████     | 252M/499M [00:05<00:05, 45.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  53%|█████▎    | 262M/499M [00:05<00:05, 47.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  55%|█████▍    | 273M/499M [00:05<00:05, 45.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  57%|█████▋    | 283M/499M [00:06<00:04, 46.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  59%|█████▉    | 294M/499M [00:06<00:04, 48.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  61%|██████    | 304M/499M [00:06<00:04, 48.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  63%|██████▎   | 315M/499M [00:06<00:03, 48.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  65%|██████▌   | 325M/499M [00:06<00:03, 50.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  67%|██████▋   | 336M/499M [00:07<00:03, 47.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  69%|██████▉   | 346M/499M [00:07<00:03, 46.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  71%|███████▏  | 357M/499M [00:07<00:02, 50.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  74%|███████▎  | 367M/499M [00:07<00:02, 51.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  76%|███████▌  | 377M/499M [00:07<00:02, 51.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  78%|███████▊  | 388M/499M [00:08<00:02, 47.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  80%|███████▉  | 398M/499M [00:08<00:02, 47.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  82%|████████▏ | 409M/499M [00:08<00:01, 49.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  84%|████████▍ | 419M/499M [00:08<00:01, 49.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  86%|████████▌ | 430M/499M [00:09<00:01, 44.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  88%|████████▊ | 440M/499M [00:09<00:01, 40.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  90%|█████████ | 451M/499M [00:09<00:01, 41.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  92%|█████████▏| 461M/499M [00:09<00:00, 43.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  95%|█████████▍| 472M/499M [00:10<00:00, 42.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  97%|█████████▋| 482M/499M [00:10<00:00, 42.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  99%|█████████▉| 493M/499M [00:10<00:00, 45.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|██████████| 499M/499M [00:10<00:00, 45.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|██████████| 499M/499M [00:10<00:00, 46.5MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.30MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.29MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 320MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 146MB/s]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:Parameters (name, tunable, count):\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.word_embeddings.weight                               0    38603520\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.position_embeddings.weight                           0      394752\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.token_type_embeddings.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.weight                                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.bias                                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.dense.weight                                 1      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.dense.bias                                   1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.weight                              1        1536\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.bias                                1           2\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.weight                         1      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.bias                           1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.weight                      1        1536\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.bias                        1           2\u001b[0m\n",
      "\u001b[34mINFO:__main__:Total parameters: 125,534,212, thereof learnable: 1,479,172 (1.1783%)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset glue/sst2 to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[34mDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average train input length: 14.36\u001b[0m\n",
      "\u001b[34mUsing fp16: True, LoRA: True\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2115.188 Total: 15360.000 (13.8% used). Free: 13244.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2115.188 Total: 15360.000 (13.8% used). Free: 13244.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2115.188 Total: 15360.000 (13.8% used). Free: 13244.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.2867, 'learning_rate': 0.00036005700712589075, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.2296, 'learning_rate': 0.0003200950118764846, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 3.51MB/s]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.209969162940979, 'eval_accuracy': 0.9369266055045872, 'eval_runtime': 1.6043, 'eval_samples_per_second': 543.525, 'eval_steps_per_second': 34.282, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.2005, 'learning_rate': 0.00028015201900237534, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.2008, 'learning_rate': 0.00024017102137767222, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.21349579095840454, 'eval_accuracy': 0.9415137614678899, 'eval_runtime': 1.6057, 'eval_samples_per_second': 543.059, 'eval_steps_per_second': 34.253, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.177, 'learning_rate': 0.00020020902612826605, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1787, 'learning_rate': 0.00016024703087885985, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.19986553490161896, 'eval_accuracy': 0.9392201834862385, 'eval_runtime': 1.5809, 'eval_samples_per_second': 551.602, 'eval_steps_per_second': 34.791, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1612, 'learning_rate': 0.00012026603325415677, 'epoch': 3.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.161, 'learning_rate': 8.030403800475059e-05, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.20631280541419983, 'eval_accuracy': 0.9438073394495413, 'eval_runtime': 1.5955, 'eval_samples_per_second': 546.536, 'eval_steps_per_second': 34.472, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1531, 'learning_rate': 4.0342042755344425e-05, 'epoch': 4.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2135.188 Total: 15360.000 (13.9% used). Free: 13224.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1462, 'learning_rate': 3.800475059382423e-07, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.22376775741577148, 'eval_accuracy': 0.9415137614678899, 'eval_runtime': 1.5642, 'eval_samples_per_second': 557.46, 'eval_steps_per_second': 35.161, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 1098.9969, 'train_samples_per_second': 306.411, 'train_steps_per_second': 19.154, 'train_loss': 0.18949472780182355, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.20631280541419983, 'eval_accuracy': 0.9438073394495413, 'eval_runtime': 1.5671, 'eval_samples_per_second': 556.425, 'eval_steps_per_second': 35.096, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m2023-06-26 12:44:21,798 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:44:21,798 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:44:21,799 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 1497\n",
      "Billable seconds: 539\n",
      "Managed Spot Training savings: 64.0%\n",
      "\n",
      "\n",
      "\n",
      "##### Log of pytorch-training-2023-06-26-12-18-31-760 (lora, 16):\n",
      "\n",
      "2023-06-26 14:09:58 Starting - Preparing the instances for training\n",
      "2023-06-26 14:09:58 Downloading - Downloading input data\n",
      "2023-06-26 14:09:58 Training - Training image download completed. Training in progress.\n",
      "2023-06-26 14:09:58 Uploading - Uploading generated training model\n",
      "2023-06-26 14:09:58 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:06,969 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:06,987 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:06,997 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:07,004 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:09,113 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 61.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate>=0.20.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.20.3-py3-none-any.whl (227 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.6/227.6 kB 46.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.13.1-py3-none-any.whl (486 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 486.2/486.2 kB 64.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 22.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.3.0-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 13.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 15.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.14.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.8/236.8 kB 52.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 770.4/770.4 kB 65.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 75.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 70.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 43.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 91.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 29.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 50.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 17.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, accelerate, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.20.3 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.13.1 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 peft-0.3.0 pynvml-11.5.0 regex-2023.6.3 responses-0.18.0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,236 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,236 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,255 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,283 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,310 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,321 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"8\",\n",
      "        \"learning-rate\": \"0.0004\",\n",
      "        \"use-fp16\": \"1\",\n",
      "        \"use-hf-lora\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-06-26-12-18-31-760\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-31-760/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"0.0004\",\"use-fp16\":\"1\",\"use-hf-lora\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-31-760/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"0.0004\",\"use-fp16\":\"1\",\"use-hf-lora\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-06-26-12-18-31-760\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-31-760/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"8\",\"--learning-rate\",\"0.0004\",\"--use-fp16\",\"1\",\"--use-hf-lora\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-SCALE=8\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.0004\u001b[0m\n",
      "\u001b[34mSM_HP_USE-FP16=1\u001b[0m\n",
      "\u001b[34mSM_HP_USE-HF-LORA=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 8 --learning-rate 0.0004 --use-fp16 1 --use-hf-lora 1\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,353 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:__main__:transformers 4.30.2\u001b[0m\n",
      "\u001b[34mINFO:__main__:peft 0.3.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:torch 2.0.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 481/481 [00:00<00:00, 2.42MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   4%|▍         | 21.0M/499M [00:00<00:06, 75.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   6%|▋         | 31.5M/499M [00:00<00:07, 59.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   8%|▊         | 41.9M/499M [00:00<00:07, 57.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  11%|█         | 52.4M/499M [00:00<00:08, 53.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  13%|█▎        | 62.9M/499M [00:01<00:08, 53.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  15%|█▍        | 73.4M/499M [00:01<00:07, 53.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  17%|█▋        | 83.9M/499M [00:01<00:08, 47.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  19%|█▉        | 94.4M/499M [00:01<00:08, 48.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  21%|██        | 105M/499M [00:02<00:08, 47.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  23%|██▎       | 115M/499M [00:02<00:07, 48.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  25%|██▌       | 126M/499M [00:02<00:07, 47.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  27%|██▋       | 136M/499M [00:02<00:07, 50.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  29%|██▉       | 147M/499M [00:02<00:07, 46.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  32%|███▏      | 157M/499M [00:03<00:08, 40.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  34%|███▎      | 168M/499M [00:03<00:07, 42.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  36%|███▌      | 178M/499M [00:03<00:07, 44.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  38%|███▊      | 189M/499M [00:03<00:07, 43.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  40%|███▉      | 199M/499M [00:04<00:06, 49.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  42%|████▏     | 210M/499M [00:04<00:05, 48.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  44%|████▍     | 220M/499M [00:04<00:05, 49.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  46%|████▌     | 231M/499M [00:04<00:05, 46.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  48%|████▊     | 241M/499M [00:04<00:05, 47.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  50%|█████     | 252M/499M [00:05<00:05, 44.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  53%|█████▎    | 262M/499M [00:05<00:05, 46.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  55%|█████▍    | 273M/499M [00:05<00:04, 45.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  57%|█████▋    | 283M/499M [00:05<00:04, 46.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  59%|█████▉    | 294M/499M [00:06<00:04, 48.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  61%|██████    | 304M/499M [00:06<00:04, 47.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  63%|██████▎   | 315M/499M [00:06<00:03, 48.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  65%|██████▌   | 325M/499M [00:06<00:03, 50.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  67%|██████▋   | 336M/499M [00:06<00:03, 49.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  69%|██████▉   | 346M/499M [00:07<00:03, 46.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  71%|███████▏  | 357M/499M [00:07<00:02, 49.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  74%|███████▎  | 367M/499M [00:07<00:02, 50.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  76%|███████▌  | 377M/499M [00:07<00:02, 51.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  78%|███████▊  | 388M/499M [00:08<00:02, 47.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  80%|███████▉  | 398M/499M [00:08<00:02, 47.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  82%|████████▏ | 409M/499M [00:08<00:01, 49.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  84%|████████▍ | 419M/499M [00:08<00:01, 49.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  86%|████████▌ | 430M/499M [00:08<00:01, 44.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  88%|████████▊ | 440M/499M [00:09<00:01, 40.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  90%|█████████ | 451M/499M [00:09<00:01, 41.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  92%|█████████▏| 461M/499M [00:09<00:00, 43.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  95%|█████████▍| 472M/499M [00:09<00:00, 43.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  97%|█████████▋| 482M/499M [00:10<00:00, 42.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  99%|█████████▉| 493M/499M [00:10<00:00, 45.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|██████████| 499M/499M [00:10<00:00, 44.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|██████████| 499M/499M [00:10<00:00, 47.2MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.31MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.31MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 3.45MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 3.43MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 4.01MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 4.00MB/s]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:Parameters (name, tunable, count):\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.word_embeddings.weight                               0    38603520\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.position_embeddings.weight                           0      394752\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.token_type_embeddings.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.weight                                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.bias                                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.dense.weight                                 1      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.dense.bias                                   1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.weight                              1        1536\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.bias                                1           2\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.weight                         1      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.bias                           1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.weight                      1        1536\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.bias                        1           2\u001b[0m\n",
      "\u001b[34mINFO:__main__:Total parameters: 125,534,212, thereof learnable: 1,479,172 (1.1783%)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset glue/sst2 to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[34mDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average train input length: 92.13\u001b[0m\n",
      "\u001b[34mUsing fp16: True, LoRA: True\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8905.188 Total: 15360.000 (58.0% used). Free: 6454.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8905.188 Total: 15360.000 (58.0% used). Free: 6454.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8905.188 Total: 15360.000 (58.0% used). Free: 6454.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11725.188 Total: 15360.000 (76.3% used). Free: 3634.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11767.188 Total: 15360.000 (76.6% used). Free: 3592.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11767.188 Total: 15360.000 (76.6% used). Free: 3592.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11767.188 Total: 15360.000 (76.6% used). Free: 3592.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11767.188 Total: 15360.000 (76.6% used). Free: 3592.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11767.188 Total: 15360.000 (76.6% used). Free: 3592.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11767.188 Total: 15360.000 (76.6% used). Free: 3592.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11767.188 Total: 15360.000 (76.6% used). Free: 3592.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11767.188 Total: 15360.000 (76.6% used). Free: 3592.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11767.188 Total: 15360.000 (76.6% used). Free: 3592.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11767.188 Total: 15360.000 (76.6% used). Free: 3592.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11767.188 Total: 15360.000 (76.6% used). Free: 3592.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11767.188 Total: 15360.000 (76.6% used). Free: 3592.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11767.188 Total: 15360.000 (76.6% used). Free: 3592.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11767.188 Total: 15360.000 (76.6% used). Free: 3592.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.2519, 'learning_rate': 0.00036005700712589075, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14647.188 Total: 15360.000 (95.4% used). Free: 712.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1911, 'learning_rate': 0.0003200950118764846, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 3.16MB/s]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.24987472593784332, 'eval_accuracy': 0.926605504587156, 'eval_runtime': 9.1814, 'eval_samples_per_second': 94.975, 'eval_steps_per_second': 5.99, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1562, 'learning_rate': 0.0002801330166270784, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1579, 'learning_rate': 0.00024017102137767222, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.23152056336402893, 'eval_accuracy': 0.9323394495412844, 'eval_runtime': 9.0761, 'eval_samples_per_second': 96.076, 'eval_steps_per_second': 6.06, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1298, 'learning_rate': 0.00020020902612826605, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1341, 'learning_rate': 0.00016024703087885985, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.24397265911102295, 'eval_accuracy': 0.9288990825688074, 'eval_runtime': 9.1092, 'eval_samples_per_second': 95.727, 'eval_steps_per_second': 6.038, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1101, 'learning_rate': 0.00012028503562945368, 'epoch': 3.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1091, 'learning_rate': 8.032304038004751e-05, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2835121750831604, 'eval_accuracy': 0.9323394495412844, 'eval_runtime': 9.0997, 'eval_samples_per_second': 95.828, 'eval_steps_per_second': 6.044, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.0968, 'learning_rate': 4.036104513064133e-05, 'epoch': 4.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.0941, 'learning_rate': 3.990498812351544e-07, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14703.188 Total: 15360.000 (95.7% used). Free: 656.812\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2935769259929657, 'eval_accuracy': 0.9323394495412844, 'eval_runtime': 9.0886, 'eval_samples_per_second': 95.945, 'eval_steps_per_second': 6.052, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 6193.2267, 'train_samples_per_second': 54.373, 'train_steps_per_second': 3.399, 'train_loss': 0.14308219899757457, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.23152056336402893, 'eval_accuracy': 0.9323394495412844, 'eval_runtime': 9.0594, 'eval_samples_per_second': 96.254, 'eval_steps_per_second': 6.071, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m2023-06-26 14:09:44,802 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-06-26 14:09:44,802 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-06-26 14:09:44,803 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 6613\n",
      "Billable seconds: 2128\n",
      "Managed Spot Training savings: 67.8%\n",
      "\n",
      "\n",
      "\n",
      "##### Log of pytorch-training-2023-06-26-12-18-33-098 (lora, 32):\n",
      "\n",
      "2023-06-26 12:52:41 Starting - Preparing the instances for training\n",
      "2023-06-26 12:52:41 Downloading - Downloading input data\n",
      "2023-06-26 12:52:41 Training - Training image download completed. Training in progress.\n",
      "2023-06-26 12:52:41 Uploading - Uploading generated training model\n",
      "2023-06-26 12:52:41 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:13,788 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:13,805 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:13,815 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:13,821 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:15,779 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 53.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate>=0.20.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.20.3-py3-none-any.whl (227 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.6/227.6 kB 35.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.13.1-py3-none-any.whl (486 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 486.2/486.2 kB 69.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 21.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.3.0-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 17.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 15.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.14.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.8/236.8 kB 51.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 770.4/770.4 kB 77.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 86.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 91.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 38.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 89.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 32.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 52.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 38.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, accelerate, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.20.3 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.13.1 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 peft-0.3.0 pynvml-11.5.0 regex-2023.6.3 responses-0.18.0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:27,607 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:27,607 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:27,625 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:27,652 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:27,678 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:27,689 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"1\",\n",
      "        \"learning-rate\": \"0.0004\",\n",
      "        \"use-fp16\": \"0\",\n",
      "        \"use-hf-lora\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-06-26-12-18-33-098\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-33-098/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"1\",\"learning-rate\":\"0.0004\",\"use-fp16\":\"0\",\"use-hf-lora\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-33-098/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"1\",\"learning-rate\":\"0.0004\",\"use-fp16\":\"0\",\"use-hf-lora\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-06-26-12-18-33-098\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-33-098/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"1\",\"--learning-rate\",\"0.0004\",\"--use-fp16\",\"0\",\"--use-hf-lora\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-SCALE=1\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.0004\u001b[0m\n",
      "\u001b[34mSM_HP_USE-FP16=0\u001b[0m\n",
      "\u001b[34mSM_HP_USE-HF-LORA=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 1 --learning-rate 0.0004 --use-fp16 0 --use-hf-lora 1\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:27,720 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:__main__:transformers 4.30.2\u001b[0m\n",
      "\u001b[34mINFO:__main__:peft 0.3.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:torch 2.0.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 481/481 [00:00<00:00, 3.48MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   6%|▋         | 31.5M/499M [00:00<00:01, 256MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  13%|█▎        | 62.9M/499M [00:00<00:01, 269MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  21%|██        | 105M/499M [00:00<00:01, 307MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  29%|██▉       | 147M/499M [00:00<00:01, 330MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  38%|███▊      | 189M/499M [00:00<00:00, 324MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  46%|████▌     | 231M/499M [00:00<00:00, 296MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  53%|█████▎    | 262M/499M [00:00<00:00, 283MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  59%|█████▉    | 294M/499M [00:01<00:00, 254MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  65%|██████▌   | 325M/499M [00:01<00:00, 249MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  71%|███████▏  | 357M/499M [00:01<00:00, 260MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  78%|███████▊  | 388M/499M [00:01<00:00, 266MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  84%|████████▍ | 419M/499M [00:01<00:00, 249MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  90%|█████████ | 451M/499M [00:01<00:00, 241MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  99%|█████████▉| 493M/499M [00:01<00:00, 266MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|██████████| 499M/499M [00:01<00:00, 272MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 6.59MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 6.56MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.23MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.22MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 19.6MB/s]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:Parameters (name, tunable, count):\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.word_embeddings.weight                               0    38603520\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.position_embeddings.weight                           0      394752\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.token_type_embeddings.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.weight                                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.bias                                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.dense.weight                                 1      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.dense.bias                                   1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.weight                              1        1536\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.bias                                1           2\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.weight                         1      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.bias                           1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.weight                      1        1536\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.bias                        1           2\u001b[0m\n",
      "\u001b[34mINFO:__main__:Total parameters: 125,534,212, thereof learnable: 1,479,172 (1.1783%)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset glue/sst2 to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[34mDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average train input length: 14.36\u001b[0m\n",
      "\u001b[34mUsing fp16: False, LoRA: True\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1861.188 Total: 15360.000 (12.1% used). Free: 13498.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2239.188 Total: 15360.000 (14.6% used). Free: 13120.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2239.188 Total: 15360.000 (14.6% used). Free: 13120.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2239.188 Total: 15360.000 (14.6% used). Free: 13120.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.2865, 'learning_rate': 0.0003600190023752969, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.2307, 'learning_rate': 0.0003200380047505938, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 3.52MB/s]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2067914456129074, 'eval_accuracy': 0.9311926605504587, 'eval_runtime': 3.0197, 'eval_samples_per_second': 288.77, 'eval_steps_per_second': 18.214, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.2016, 'learning_rate': 0.00028005700712589076, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.2006, 'learning_rate': 0.00024007600950118766, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.225970059633255, 'eval_accuracy': 0.9392201834862385, 'eval_runtime': 2.9768, 'eval_samples_per_second': 292.931, 'eval_steps_per_second': 18.476, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1776, 'learning_rate': 0.00020009501187648457, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1784, 'learning_rate': 0.00016011401425178147, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.20776806771755219, 'eval_accuracy': 0.9357798165137615, 'eval_runtime': 2.9628, 'eval_samples_per_second': 294.317, 'eval_steps_per_second': 18.564, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.16, 'learning_rate': 0.00012013301662707839, 'epoch': 3.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1612, 'learning_rate': 8.01520190023753e-05, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.22221356630325317, 'eval_accuracy': 0.9380733944954128, 'eval_runtime': 2.9643, 'eval_samples_per_second': 294.166, 'eval_steps_per_second': 18.554, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1522, 'learning_rate': 4.0171021377672214e-05, 'epoch': 4.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 2253.188 Total: 15360.000 (14.7% used). Free: 13106.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1463, 'learning_rate': 1.9002375296912116e-07, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.22931142151355743, 'eval_accuracy': 0.9403669724770642, 'eval_runtime': 2.9681, 'eval_samples_per_second': 293.793, 'eval_steps_per_second': 18.531, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 1585.3755, 'train_samples_per_second': 212.407, 'train_steps_per_second': 13.278, 'train_loss': 0.18951314961258986, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.22931142151355743, 'eval_accuracy': 0.9403669724770642, 'eval_runtime': 2.9472, 'eval_samples_per_second': 295.878, 'eval_steps_per_second': 18.662, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m2023-06-26 12:52:27,254 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:52:27,255 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:52:27,255 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 1969\n",
      "Billable seconds: 633\n",
      "Managed Spot Training savings: 67.9%\n",
      "\n",
      "\n",
      "\n",
      "##### Log of pytorch-training-2023-06-26-12-18-34-425 (lora, 32):\n",
      "\n",
      "2023-06-26 17:17:21 Starting - Preparing the instances for training\n",
      "2023-06-26 17:17:21 Downloading - Downloading input data\n",
      "2023-06-26 17:17:21 Training - Training image download completed. Training in progress.\n",
      "2023-06-26 17:17:21 Interrupted - Training job interrupted\n",
      "2023-06-26 17:17:21 Starting - Preparing the instances for training\n",
      "2023-06-26 17:17:21 Downloading - Downloading input data\n",
      "2023-06-26 17:17:21 Training - Training image download completed. Training in progress.\n",
      "2023-06-26 17:17:21 Uploading - Uploading generated training model\n",
      "2023-06-26 17:17:21 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:16,835 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:16,851 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:16,860 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:16,866 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:16,835 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:16,851 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:16,860 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:16,866 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:18,791 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 82.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:18,791 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 82.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate>=0.20.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.20.3-py3-none-any.whl (227 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.6/227.6 kB 48.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.13.1-py3-none-any.whl (486 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 486.2/486.2 kB 72.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 27.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.3.0-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 18.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 18.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.14.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.8/236.8 kB 52.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[35mCollecting accelerate>=0.20.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading accelerate-0.20.3-py3-none-any.whl (227 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.6/227.6 kB 48.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.13.1-py3-none-any.whl (486 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 486.2/486.2 kB 72.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[35mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 27.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mDownloading peft-0.3.0-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 18.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[35mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 18.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0,>=0.14.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.8/236.8 kB 52.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 770.4/770.4 kB 79.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 79.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 62.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[35mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 770.4/770.4 kB 79.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[35mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 79.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 62.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 51.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 95.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[35mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 51.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.5.0)\u001b[0m\n",
      "\u001b[35mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 95.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[35mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[35mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 30.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 58.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 40.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 30.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[35mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 58.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 40.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, accelerate, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[35mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, accelerate, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.20.3 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.13.1 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 peft-0.3.0 pynvml-11.5.0 regex-2023.6.3 responses-0.18.0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[35mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[35mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[35mSuccessfully installed accelerate-0.20.3 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.13.1 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 peft-0.3.0 pynvml-11.5.0 regex-2023.6.3 responses-0.18.0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:30,635 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:30,635 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:30,654 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:30,635 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:30,635 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:30,654 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:30,681 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:30,708 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:30,719 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:30,681 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:30,708 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:30,719 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"8\",\n",
      "        \"learning-rate\": \"0.0004\",\n",
      "        \"use-fp16\": \"0\",\n",
      "        \"use-hf-lora\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-06-26-12-18-34-425\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-34-425/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"0.0004\",\"use-fp16\":\"0\",\"use-hf-lora\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"8\",\n",
      "        \"learning-rate\": \"0.0004\",\n",
      "        \"use-fp16\": \"0\",\n",
      "        \"use-hf-lora\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-06-26-12-18-34-425\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-34-425/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"0.0004\",\"use-fp16\":\"0\",\"use-hf-lora\":\"1\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-34-425/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"0.0004\",\"use-fp16\":\"0\",\"use-hf-lora\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-06-26-12-18-34-425\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-34-425/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"8\",\"--learning-rate\",\"0.0004\",\"--use-fp16\",\"0\",\"--use-hf-lora\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-SCALE=8\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.0004\u001b[0m\n",
      "\u001b[34mSM_HP_USE-FP16=0\u001b[0m\n",
      "\u001b[34mSM_HP_USE-HF-LORA=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 8 --learning-rate 0.0004 --use-fp16 0 --use-hf-lora 1\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:30,751 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-34-425/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"0.0004\",\"use-fp16\":\"0\",\"use-hf-lora\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-06-26-12-18-34-425\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-34-425/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"8\",\"--learning-rate\",\"0.0004\",\"--use-fp16\",\"0\",\"--use-hf-lora\",\"1\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[35mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[35mSM_HP_INPUT-SCALE=8\u001b[0m\n",
      "\u001b[35mSM_HP_LEARNING-RATE=0.0004\u001b[0m\n",
      "\u001b[35mSM_HP_USE-FP16=0\u001b[0m\n",
      "\u001b[35mSM_HP_USE-HF-LORA=1\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 8 --learning-rate 0.0004 --use-fp16 0 --use-hf-lora 1\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:30,751 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:__main__:transformers 4.30.2\u001b[0m\n",
      "\u001b[34mINFO:__main__:peft 0.3.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:torch 2.0.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:transformers 4.30.2\u001b[0m\n",
      "\u001b[35mINFO:__main__:peft 0.3.0\u001b[0m\n",
      "\u001b[35mINFO:__main__:torch 2.0.0\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 481/481 [00:00<00:00, 2.61MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   6%|▋         | 31.5M/499M [00:00<00:01, 235MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  15%|█▍        | 73.4M/499M [00:00<00:01, 283MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  21%|██        | 105M/499M [00:00<00:01, 277MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  32%|███▏      | 157M/499M [00:00<00:01, 305MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  38%|███▊      | 189M/499M [00:00<00:01, 304MB/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)lve/main/config.json: 100%|██████████| 481/481 [00:00<00:00, 2.61MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:   6%|▋         | 31.5M/499M [00:00<00:01, 235MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  15%|█▍        | 73.4M/499M [00:00<00:01, 283MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  21%|██        | 105M/499M [00:00<00:01, 277MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  32%|███▏      | 157M/499M [00:00<00:01, 305MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  38%|███▊      | 189M/499M [00:00<00:01, 304MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  46%|████▌     | 231M/499M [00:00<00:00, 317MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  55%|█████▍    | 273M/499M [00:00<00:00, 315MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  63%|██████▎   | 315M/499M [00:01<00:00, 326MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  46%|████▌     | 231M/499M [00:00<00:00, 317MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  55%|█████▍    | 273M/499M [00:00<00:00, 315MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  63%|██████▎   | 315M/499M [00:01<00:00, 326MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  71%|███████▏  | 357M/499M [00:01<00:00, 334MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  80%|███████▉  | 398M/499M [00:01<00:00, 339MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  88%|████████▊ | 440M/499M [00:01<00:00, 343MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  99%|█████████▉| 493M/499M [00:01<00:00, 383MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|██████████| 499M/499M [00:01<00:00, 333MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  71%|███████▏  | 357M/499M [00:01<00:00, 334MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  80%|███████▉  | 398M/499M [00:01<00:00, 339MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  88%|████████▊ | 440M/499M [00:01<00:00, 343MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  99%|█████████▉| 493M/499M [00:01<00:00, 383MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors: 100%|██████████| 499M/499M [00:01<00:00, 333MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\u001b[0m\n",
      "\u001b[35m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[35m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[35mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\u001b[0m\n",
      "\u001b[35m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[35m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[35mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 70.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 229MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 9.91MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 9.85MB/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 70.3MB/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 229MB/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 9.91MB/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 9.85MB/s]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:Parameters (name, tunable, count):\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.word_embeddings.weight                               0    38603520\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.position_embeddings.weight                           0      394752\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.token_type_embeddings.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.weight                                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.bias                                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__:Parameters (name, tunable, count):\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.embeddings.word_embeddings.weight                               0    38603520\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.embeddings.position_embeddings.weight                           0      394752\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.embeddings.token_type_embeddings.weight                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.weight                                     0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.bias                                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.dense.weight                                 1      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.dense.bias                                   1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.weight                              1        1536\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.bias                                1           2\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.weight                         1      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.bias                           1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.weight                      1        1536\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.bias                        1           2\u001b[0m\n",
      "\u001b[34mINFO:__main__:Total parameters: 125,534,212, thereof learnable: 1,479,172 (1.1783%)\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.classifier.original_module.dense.weight                                 1      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.classifier.original_module.dense.bias                                   1         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.weight                              1        1536\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.bias                                1           2\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.weight                         1      589824\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.bias                           1         768\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.weight                      1        1536\u001b[0m\n",
      "\u001b[35mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.bias                        1           2\u001b[0m\n",
      "\u001b[35mINFO:__main__:Total parameters: 125,534,212, thereof learnable: 1,479,172 (1.1783%)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset glue/sst2 to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[35mDownloading and preparing dataset glue/sst2 to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[34mDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[35mDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average train input length: 92.13\u001b[0m\n",
      "\u001b[34mUsing fp16: False, LoRA: True\u001b[0m\n",
      "\u001b[35mINFO:__main__:Average train input length: 92.13\u001b[0m\n",
      "\u001b[35mUsing fp16: False, LoRA: True\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 12527.188 Total: 15360.000 (81.6% used). Free: 2832.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 12527.188 Total: 15360.000 (81.6% used). Free: 2832.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 12527.188 Total: 15360.000 (81.6% used). Free: 2832.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 12527.188 Total: 15360.000 (81.6% used). Free: 2832.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 12527.188 Total: 15360.000 (81.6% used). Free: 2832.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 12527.188 Total: 15360.000 (81.6% used). Free: 2832.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 12527.188 Total: 15360.000 (81.6% used). Free: 2832.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 12527.188 Total: 15360.000 (81.6% used). Free: 2832.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 12527.188 Total: 15360.000 (81.6% used). Free: 2832.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 12527.188 Total: 15360.000 (81.6% used). Free: 2832.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11163.188 Total: 15360.000 (72.7% used). Free: 4196.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11163.188 Total: 15360.000 (72.7% used). Free: 4196.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11163.188 Total: 15360.000 (72.7% used). Free: 4196.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11163.188 Total: 15360.000 (72.7% used). Free: 4196.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.2506, 'learning_rate': 0.0003600190023752969, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[35m{'loss': 0.2506, 'learning_rate': 0.0003600190023752969, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1914, 'learning_rate': 0.0003200380047505938, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[35m{'loss': 0.1914, 'learning_rate': 0.0003200380047505938, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 3.72MB/s]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.22927628457546234, 'eval_accuracy': 0.9254587155963303, 'eval_runtime': 17.5702, 'eval_samples_per_second': 49.629, 'eval_steps_per_second': 3.13, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[35mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 3.72MB/s]\u001b[0m\n",
      "\u001b[35m{'eval_loss': 0.22927628457546234, 'eval_accuracy': 0.9254587155963303, 'eval_runtime': 17.5702, 'eval_samples_per_second': 49.629, 'eval_steps_per_second': 3.13, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1557, 'learning_rate': 0.00028005700712589076, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[35m{'loss': 0.1557, 'learning_rate': 0.00028005700712589076, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[32mbash: no job control in this shell\u001b[0m\n",
      "\u001b[32m2023-06-26 14:02:44,791 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[32m2023-06-26 14:02:44,808 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-06-26 14:02:44,817 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[32m2023-06-26 14:02:44,825 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[32m2023-06-26 14:02:46,879 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[32m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[32mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[32mDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 49.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting accelerate>=0.20.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[32mDownloading accelerate-0.20.3-py3-none-any.whl (227 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.6/227.6 kB 46.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[32mDownloading datasets-2.13.1-py3-none-any.whl (486 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 486.2/486.2 kB 62.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[32mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 21.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[32mDownloading peft-0.3.0-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 16.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[32mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 14.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[32mCollecting huggingface-hub<1.0,>=0.14.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[32mDownloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.8/236.8 kB 49.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[32mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[32mDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 770.4/770.4 kB 61.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[32mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[32mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 71.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[32mDownloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 45.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[32mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[32mDownloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 47.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.5.0)\u001b[0m\n",
      "\u001b[32mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[32mDownloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 83.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[32mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[32mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[32mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 28.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[32mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[32mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[32mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 44.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[32mDownloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 36.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[32mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[32mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, accelerate, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[32mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[32mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[32mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[32mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[32mSuccessfully installed accelerate-0.20.3 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.13.1 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 peft-0.3.0 pynvml-11.5.0 regex-2023.6.3 responses-0.18.0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[32mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[32m2023-06-26 14:02:58,868 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[32m2023-06-26 14:02:58,868 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[32m2023-06-26 14:02:58,888 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-06-26 14:02:58,916 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-06-26 14:02:58,943 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-06-26 14:02:58,954 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[32mTraining Env:\u001b[0m\n",
      "\u001b[32m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"8\",\n",
      "        \"learning-rate\": \"0.0004\",\n",
      "        \"use-fp16\": \"0\",\n",
      "        \"use-hf-lora\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-06-26-12-18-34-425\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-34-425/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[32m}\u001b[0m\n",
      "\u001b[32mEnvironment variables:\u001b[0m\n",
      "\u001b[32mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[32mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[32mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"0.0004\",\"use-fp16\":\"0\",\"use-hf-lora\":\"1\"}\u001b[0m\n",
      "\u001b[32mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[32mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[32mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[32mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[32mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[32mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[32mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[32mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[32mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[32mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[32mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[32mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[32mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[32mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[32mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[32mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[32mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[32mSM_MODULE_DIR=s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-34-425/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[32mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"0.0004\",\"use-fp16\":\"0\",\"use-hf-lora\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-06-26-12-18-34-425\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-34-425/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[32mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"8\",\"--learning-rate\",\"0.0004\",\"--use-fp16\",\"0\",\"--use-hf-lora\",\"1\"]\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[32mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[32mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[32mSM_HP_INPUT-SCALE=8\u001b[0m\n",
      "\u001b[32mSM_HP_LEARNING-RATE=0.0004\u001b[0m\n",
      "\u001b[32mSM_HP_USE-FP16=0\u001b[0m\n",
      "\u001b[32mSM_HP_USE-HF-LORA=1\u001b[0m\n",
      "\u001b[32mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[32mInvoking script with the following command:\u001b[0m\n",
      "\u001b[32m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 8 --learning-rate 0.0004 --use-fp16 0 --use-hf-lora 1\u001b[0m\n",
      "\u001b[32m2023-06-26 14:02:58,986 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[32mINFO:__main__:transformers 4.30.2\u001b[0m\n",
      "\u001b[32mINFO:__main__:peft 0.3.0\u001b[0m\n",
      "\u001b[32mINFO:__main__:torch 2.0.0\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[32mDownloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[32mDownloading (…)lve/main/config.json: 100%|██████████| 481/481 [00:00<00:00, 2.50MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:   2%|▏         | 10.5M/499M [00:00<00:06, 78.8MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:   4%|▍         | 21.0M/499M [00:00<00:05, 88.0MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  11%|█         | 52.4M/499M [00:00<00:02, 161MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  19%|█▉        | 94.4M/499M [00:00<00:01, 241MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  27%|██▋       | 136M/499M [00:00<00:01, 275MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  36%|███▌      | 178M/499M [00:00<00:01, 294MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  42%|████▏     | 210M/499M [00:00<00:01, 285MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  48%|████▊     | 241M/499M [00:00<00:00, 266MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  55%|█████▍    | 273M/499M [00:01<00:00, 258MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  61%|██████    | 304M/499M [00:01<00:00, 272MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  67%|██████▋   | 336M/499M [00:01<00:00, 280MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  74%|███████▎  | 367M/499M [00:01<00:00, 281MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  80%|███████▉  | 398M/499M [00:01<00:00, 286MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  88%|████████▊ | 440M/499M [00:01<00:00, 301MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  97%|█████████▋| 482M/499M [00:01<00:00, 319MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors: 100%|██████████| 499M/499M [00:01<00:00, 272MB/s]\u001b[0m\n",
      "\u001b[32mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\u001b[0m\n",
      "\u001b[32m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[32m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[32mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[32mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[32mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\u001b[0m\n",
      "\u001b[32m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[32m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[32mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[32mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[32mDownloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[32mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.32MB/s]\u001b[0m\n",
      "\u001b[32mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.31MB/s]\u001b[0m\n",
      "\u001b[32mDownloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[32mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.23MB/s]\u001b[0m\n",
      "\u001b[32mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.23MB/s]\u001b[0m\n",
      "\u001b[32mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[32mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 3.99MB/s]\u001b[0m\n",
      "\u001b[32mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 3.98MB/s]\u001b[0m\n",
      "\u001b[32mDEBUG:__main__:Parameters (name, tunable, count):\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.embeddings.word_embeddings.weight                               0    38603520\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.embeddings.position_embeddings.weight                           0      394752\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.embeddings.token_type_embeddings.weight                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.weight                                     0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.bias                                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight      1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight     1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight     1        6144\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.classifier.original_module.dense.weight                                 1      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.classifier.original_module.dense.bias                                   1         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.weight                              1        1536\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.bias                                1           2\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.weight                         1      589824\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.bias                           1         768\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.weight                      1        1536\u001b[0m\n",
      "\u001b[32mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.bias                        1           2\u001b[0m\n",
      "\u001b[32mINFO:__main__:Total parameters: 125,534,212, thereof learnable: 1,479,172 (1.1783%)\u001b[0m\n",
      "\u001b[32mDownloading and preparing dataset glue/sst2 to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[32mDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:Average train input length: 92.13\u001b[0m\n",
      "\u001b[32mUsing fp16: False, LoRA: True\u001b[0m\n",
      "\u001b[32m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[32mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[32mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 12527.188 Total: 15360.000 (81.6% used). Free: 2832.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 12527.188 Total: 15360.000 (81.6% used). Free: 2832.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 12527.188 Total: 15360.000 (81.6% used). Free: 2832.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 12527.188 Total: 15360.000 (81.6% used). Free: 2832.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 12527.188 Total: 15360.000 (81.6% used). Free: 2832.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11163.188 Total: 15360.000 (72.7% used). Free: 4196.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[32m{'loss': 0.2501, 'learning_rate': 0.0003600190023752969, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11111.188 Total: 15360.000 (72.3% used). Free: 4248.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32m{'loss': 0.19, 'learning_rate': 0.0003200380047505938, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[32mDownloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 3.31MB/s]\u001b[0m\n",
      "\u001b[32m{'eval_loss': 0.2559873163700104, 'eval_accuracy': 0.9220183486238532, 'eval_runtime': 19.2506, 'eval_samples_per_second': 45.297, 'eval_steps_per_second': 2.857, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32m{'loss': 0.1569, 'learning_rate': 0.00028005700712589076, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32m{'loss': 0.1582, 'learning_rate': 0.00024007600950118766, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[32m{'eval_loss': 0.21471326053142548, 'eval_accuracy': 0.9357798165137615, 'eval_runtime': 19.0467, 'eval_samples_per_second': 45.782, 'eval_steps_per_second': 2.888, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32m{'loss': 0.1316, 'learning_rate': 0.00020009501187648457, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32m{'loss': 0.1337, 'learning_rate': 0.00016011401425178147, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[32m{'eval_loss': 0.23944522440433502, 'eval_accuracy': 0.9311926605504587, 'eval_runtime': 18.9663, 'eval_samples_per_second': 45.976, 'eval_steps_per_second': 2.9, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32m{'loss': 0.112, 'learning_rate': 0.00012013301662707839, 'epoch': 3.5}\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11151.188 Total: 15360.000 (72.6% used). Free: 4208.812\u001b[0m\n",
      "\u001b[32m{'loss': 0.1111, 'learning_rate': 8.01520190023753e-05, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32m{'eval_loss': 0.27599307894706726, 'eval_accuracy': 0.926605504587156, 'eval_runtime': 19.0653, 'eval_samples_per_second': 45.738, 'eval_steps_per_second': 2.885, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32m{'loss': 0.0987, 'learning_rate': 4.0171021377672214e-05, 'epoch': 4.5}\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32m{'loss': 0.0944, 'learning_rate': 1.9002375296912116e-07, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 11203.188 Total: 15360.000 (72.9% used). Free: 4156.812\u001b[0m\n",
      "\u001b[32m{'eval_loss': 0.27413082122802734, 'eval_accuracy': 0.9334862385321101, 'eval_runtime': 19.0179, 'eval_samples_per_second': 45.852, 'eval_steps_per_second': 2.892, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[32m{'train_runtime': 11575.838, 'train_samples_per_second': 29.09, 'train_steps_per_second': 1.818, 'train_loss': 0.14363363761516762, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[32m{'eval_loss': 0.21471326053142548, 'eval_accuracy': 0.9357798165137615, 'eval_runtime': 19.0283, 'eval_samples_per_second': 45.826, 'eval_steps_per_second': 2.89, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[32m2023-06-26 17:17:05,004 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[32m2023-06-26 17:17:05,004 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[32m2023-06-26 17:17:05,005 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 16731\n",
      "Billable seconds: 5563\n",
      "Managed Spot Training savings: 66.8%\n",
      "\n",
      "\n",
      "\n",
      "##### Log of pytorch-training-2023-06-26-12-18-35-678 (full, 16):\n",
      "\n",
      "2023-06-26 13:00:05 Starting - Preparing the instances for training\n",
      "2023-06-26 13:00:05 Downloading - Downloading input data\n",
      "2023-06-26 13:00:05 Training - Training image download completed. Training in progress.\n",
      "2023-06-26 13:00:05 Uploading - Uploading generated training model\n",
      "2023-06-26 13:00:05 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:11,006 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:11,024 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:11,033 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:11,039 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:13,010 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 58.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate>=0.20.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.20.3-py3-none-any.whl (227 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.6/227.6 kB 44.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.13.1-py3-none-any.whl (486 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 486.2/486.2 kB 69.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 19.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.3.0-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 16.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 15.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.14.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.8/236.8 kB 37.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 770.4/770.4 kB 70.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 112.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 87.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 38.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 95.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 32.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 48.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 35.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, accelerate, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.20.3 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.13.1 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 peft-0.3.0 pynvml-11.5.0 regex-2023.6.3 responses-0.18.0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:24,634 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:24,634 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:24,653 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:24,679 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:24,704 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:24,714 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"1\",\n",
      "        \"learning-rate\": \"5e-05\",\n",
      "        \"use-fp16\": \"1\",\n",
      "        \"use-hf-lora\": \"0\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-06-26-12-18-35-678\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-35-678/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"1\",\"learning-rate\":\"5e-05\",\"use-fp16\":\"1\",\"use-hf-lora\":\"0\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-35-678/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"1\",\"learning-rate\":\"5e-05\",\"use-fp16\":\"1\",\"use-hf-lora\":\"0\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-06-26-12-18-35-678\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-35-678/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"1\",\"--learning-rate\",\"5e-05\",\"--use-fp16\",\"1\",\"--use-hf-lora\",\"0\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-SCALE=1\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=5e-05\u001b[0m\n",
      "\u001b[34mSM_HP_USE-FP16=1\u001b[0m\n",
      "\u001b[34mSM_HP_USE-HF-LORA=0\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 1 --learning-rate 5e-05 --use-fp16 1 --use-hf-lora 0\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:24,748 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:__main__:transformers 4.30.2\u001b[0m\n",
      "\u001b[34mINFO:__main__:peft 0.3.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:torch 2.0.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 481/481 [00:00<00:00, 3.60MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   6%|▋         | 31.5M/499M [00:00<00:01, 263MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  15%|█▍        | 73.4M/499M [00:00<00:01, 287MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  21%|██        | 105M/499M [00:00<00:01, 289MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  27%|██▋       | 136M/499M [00:00<00:01, 286MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  34%|███▎      | 168M/499M [00:00<00:01, 278MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  40%|███▉      | 199M/499M [00:00<00:01, 288MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  46%|████▌     | 231M/499M [00:00<00:00, 284MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  53%|█████▎    | 262M/499M [00:00<00:00, 277MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  59%|█████▉    | 294M/499M [00:01<00:00, 258MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  65%|██████▌   | 325M/499M [00:01<00:00, 249MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  71%|███████▏  | 357M/499M [00:01<00:00, 228MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  80%|███████▉  | 398M/499M [00:01<00:00, 252MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  86%|████████▌ | 430M/499M [00:01<00:00, 241MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  92%|█████████▏| 461M/499M [00:01<00:00, 249MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  99%|█████████▉| 493M/499M [00:01<00:00, 262MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|██████████| 499M/499M [00:01<00:00, 263MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.32MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.31MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.25MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.25MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 6.57MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 6.54MB/s]\u001b[0m\n",
      "\u001b[34mINFO:__main__:Total parameters: 124,647,170, thereof learnable: 124,647,170 (100.0000%)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset glue/sst2 to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[34mDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average train input length: 14.36\u001b[0m\n",
      "\u001b[34mUsing fp16: True, LoRA: False\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3647.188 Total: 15360.000 (23.7% used). Free: 11712.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.3889, 'learning_rate': 4.501425178147269e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.3241, 'learning_rate': 4.0021377672209026e-05, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 3.69MB/s]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3675265610218048, 'eval_accuracy': 0.8761467889908257, 'eval_runtime': 1.3899, 'eval_samples_per_second': 627.4, 'eval_steps_per_second': 39.572, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.3498, 'learning_rate': 3.5030878859857484e-05, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.2975, 'learning_rate': 3.0035629453681712e-05, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3617643713951111, 'eval_accuracy': 0.8841743119266054, 'eval_runtime': 1.3034, 'eval_samples_per_second': 668.998, 'eval_steps_per_second': 42.196, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.2338, 'learning_rate': 2.5038004750593825e-05, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.2443, 'learning_rate': 2.0042755344418054e-05, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.33064547181129456, 'eval_accuracy': 0.9025229357798165, 'eval_runtime': 1.2878, 'eval_samples_per_second': 677.109, 'eval_steps_per_second': 42.708, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1849, 'learning_rate': 1.5045130641330166e-05, 'epoch': 3.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1643, 'learning_rate': 1.0049881235154395e-05, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3319872319698334, 'eval_accuracy': 0.908256880733945, 'eval_runtime': 1.303, 'eval_samples_per_second': 669.223, 'eval_steps_per_second': 42.21, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1345, 'learning_rate': 5.057007125890737e-06, 'epoch': 4.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1223, 'learning_rate': 6.175771971496438e-08, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3357405662536621, 'eval_accuracy': 0.9128440366972477, 'eval_runtime': 1.296, 'eval_samples_per_second': 672.832, 'eval_steps_per_second': 42.438, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3655.188 Total: 15360.000 (23.8% used). Free: 11704.812\u001b[0m\n",
      "\u001b[34m{'train_runtime': 1895.8022, 'train_samples_per_second': 177.627, 'train_steps_per_second': 11.103, 'train_loss': 0.2443636450914759, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3357405662536621, 'eval_accuracy': 0.9128440366972477, 'eval_runtime': 1.267, 'eval_samples_per_second': 688.258, 'eval_steps_per_second': 43.411, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m2023-06-26 12:57:33,508 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:57:33,508 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:57:33,508 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 2416\n",
      "Billable seconds: 870\n",
      "Managed Spot Training savings: 64.0%\n",
      "\n",
      "\n",
      "\n",
      "##### Log of pytorch-training-2023-06-26-12-18-36-915 (full, 16):\n",
      "\n",
      "2023-06-26 17:20:49 Starting - Preparing the instances for training\n",
      "2023-06-26 17:20:49 Downloading - Downloading input data\n",
      "2023-06-26 17:20:49 Training - Training image download completed. Training in progress.\n",
      "2023-06-26 17:20:49 Interrupted - Training job interrupted\n",
      "2023-06-26 17:20:49 Starting - Preparing the instances for training\n",
      "2023-06-26 17:20:49 Downloading - Downloading input data\n",
      "2023-06-26 17:20:49 Training - Training image download completed. Training in progress.\n",
      "2023-06-26 17:20:49 Interrupted - Training job interrupted\n",
      "2023-06-26 17:20:49 Starting - Preparing the instances for training\n",
      "2023-06-26 17:20:49 Downloading - Downloading input data\n",
      "2023-06-26 17:20:49 Training - Training image download completed. Training in progress.\n",
      "2023-06-26 17:20:49 Interrupted - Training job interrupted\n",
      "2023-06-26 17:20:49 Starting - Preparing the instances for training\n",
      "2023-06-26 17:20:49 Downloading - Downloading input data\n",
      "2023-06-26 17:20:49 Training - Training image download completed. Training in progress.\n",
      "2023-06-26 17:20:49 Uploading - Uploading generated training model\n",
      "2023-06-26 17:20:49 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:16,628 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:16,645 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:16,654 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:16,663 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:16,628 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:16,645 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:16,654 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:16,663 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:19,041 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 78.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate>=0.20.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.20.3-py3-none-any.whl (227 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.6/227.6 kB 49.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:19,041 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 78.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting accelerate>=0.20.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading accelerate-0.20.3-py3-none-any.whl (227 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.6/227.6 kB 49.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.13.1-py3-none-any.whl (486 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 486.2/486.2 kB 65.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 23.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.3.0-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 15.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 14.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.13.1-py3-none-any.whl (486 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 486.2/486.2 kB 65.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[35mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 23.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mDownloading peft-0.3.0-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 15.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[35mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 14.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.14.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.8/236.8 kB 39.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0,>=0.14.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.8/236.8 kB 39.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 770.4/770.4 kB 71.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 86.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 76.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 42.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.5.0)\u001b[0m\n",
      "\u001b[35mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 770.4/770.4 kB 71.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[35mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 86.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 76.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[35mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 42.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 73.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[35mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 73.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[35mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 29.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 38.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[35mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 29.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[35mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 38.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 35.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[35mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 35.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, accelerate, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[35mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, accelerate, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[35mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[35mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[35mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.20.3 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.13.1 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 peft-0.3.0 pynvml-11.5.0 regex-2023.6.3 responses-0.18.0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:30,818 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:30,818 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:30,838 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:30,865 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:30,891 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:30,902 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[35mSuccessfully installed accelerate-0.20.3 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.13.1 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 peft-0.3.0 pynvml-11.5.0 regex-2023.6.3 responses-0.18.0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:30,818 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:30,818 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:30,838 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:30,865 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:30,891 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:30,902 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"8\",\n",
      "        \"learning-rate\": \"5e-05\",\n",
      "        \"use-fp16\": \"1\",\n",
      "        \"use-hf-lora\": \"0\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-06-26-12-18-36-915\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-36-915/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"5e-05\",\"use-fp16\":\"1\",\"use-hf-lora\":\"0\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"8\",\n",
      "        \"learning-rate\": \"5e-05\",\n",
      "        \"use-fp16\": \"1\",\n",
      "        \"use-hf-lora\": \"0\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-06-26-12-18-36-915\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-36-915/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"5e-05\",\"use-fp16\":\"1\",\"use-hf-lora\":\"0\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-36-915/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"5e-05\",\"use-fp16\":\"1\",\"use-hf-lora\":\"0\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-06-26-12-18-36-915\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-36-915/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"8\",\"--learning-rate\",\"5e-05\",\"--use-fp16\",\"1\",\"--use-hf-lora\",\"0\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-SCALE=8\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=5e-05\u001b[0m\n",
      "\u001b[34mSM_HP_USE-FP16=1\u001b[0m\n",
      "\u001b[34mSM_HP_USE-HF-LORA=0\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 8 --learning-rate 5e-05 --use-fp16 1 --use-hf-lora 0\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:30,940 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-36-915/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"5e-05\",\"use-fp16\":\"1\",\"use-hf-lora\":\"0\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-06-26-12-18-36-915\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-36-915/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"8\",\"--learning-rate\",\"5e-05\",\"--use-fp16\",\"1\",\"--use-hf-lora\",\"0\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[35mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[35mSM_HP_INPUT-SCALE=8\u001b[0m\n",
      "\u001b[35mSM_HP_LEARNING-RATE=5e-05\u001b[0m\n",
      "\u001b[35mSM_HP_USE-FP16=1\u001b[0m\n",
      "\u001b[35mSM_HP_USE-HF-LORA=0\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 8 --learning-rate 5e-05 --use-fp16 1 --use-hf-lora 0\u001b[0m\n",
      "\u001b[35m2023-06-26 12:25:30,940 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:__main__:transformers 4.30.2\u001b[0m\n",
      "\u001b[34mINFO:__main__:peft 0.3.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:torch 2.0.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 481/481 [00:00<00:00, 3.51MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   2%|▏         | 10.5M/499M [00:00<00:10, 46.7MB/s]\u001b[0m\n",
      "\u001b[35mINFO:__main__:transformers 4.30.2\u001b[0m\n",
      "\u001b[35mINFO:__main__:peft 0.3.0\u001b[0m\n",
      "\u001b[35mINFO:__main__:torch 2.0.0\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[35mDownloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)lve/main/config.json: 100%|██████████| 481/481 [00:00<00:00, 3.51MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:   2%|▏         | 10.5M/499M [00:00<00:10, 46.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   8%|▊         | 41.9M/499M [00:00<00:03, 149MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  17%|█▋        | 83.9M/499M [00:00<00:01, 221MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  23%|██▎       | 115M/499M [00:00<00:01, 242MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  29%|██▉       | 147M/499M [00:00<00:01, 257MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  36%|███▌      | 178M/499M [00:00<00:01, 255MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  42%|████▏     | 210M/499M [00:00<00:01, 260MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  48%|████▊     | 241M/499M [00:01<00:01, 254MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  55%|█████▍    | 273M/499M [00:01<00:00, 234MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:   8%|▊         | 41.9M/499M [00:00<00:03, 149MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  17%|█▋        | 83.9M/499M [00:00<00:01, 221MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  23%|██▎       | 115M/499M [00:00<00:01, 242MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  29%|██▉       | 147M/499M [00:00<00:01, 257MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  36%|███▌      | 178M/499M [00:00<00:01, 255MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  42%|████▏     | 210M/499M [00:00<00:01, 260MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  48%|████▊     | 241M/499M [00:01<00:01, 254MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  55%|█████▍    | 273M/499M [00:01<00:00, 234MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  61%|██████    | 304M/499M [00:01<00:00, 233MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  67%|██████▋   | 336M/499M [00:01<00:00, 233MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  61%|██████    | 304M/499M [00:01<00:00, 233MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  67%|██████▋   | 336M/499M [00:01<00:00, 233MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  74%|███████▎  | 367M/499M [00:01<00:00, 246MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  80%|███████▉  | 398M/499M [00:01<00:00, 253MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  86%|████████▌ | 430M/499M [00:01<00:00, 247MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  92%|█████████▏| 461M/499M [00:01<00:00, 246MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  99%|█████████▉| 493M/499M [00:02<00:00, 252MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|██████████| 499M/499M [00:02<00:00, 236MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  74%|███████▎  | 367M/499M [00:01<00:00, 246MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  80%|███████▉  | 398M/499M [00:01<00:00, 253MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  86%|████████▌ | 430M/499M [00:01<00:00, 247MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  92%|█████████▏| 461M/499M [00:01<00:00, 246MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors:  99%|█████████▉| 493M/499M [00:02<00:00, 252MB/s]\u001b[0m\n",
      "\u001b[35mDownloading model.safetensors: 100%|██████████| 499M/499M [00:02<00:00, 236MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\u001b[0m\n",
      "\u001b[35m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[35m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[35mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\u001b[0m\n",
      "\u001b[35m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[35m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[35mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\u001b[0m\n",
      "\u001b[35mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[35mDownloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.32MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.31MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.25MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.24MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 4.03MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 4.02MB/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.32MB/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.31MB/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.25MB/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.24MB/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 4.03MB/s]\u001b[0m\n",
      "\u001b[35mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 4.02MB/s]\u001b[0m\n",
      "\u001b[34mINFO:__main__:Total parameters: 124,647,170, thereof learnable: 124,647,170 (100.0000%)\u001b[0m\n",
      "\u001b[35mINFO:__main__:Total parameters: 124,647,170, thereof learnable: 124,647,170 (100.0000%)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset glue/sst2 to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[35mDownloading and preparing dataset glue/sst2 to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[34mDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[35mDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average train input length: 92.13\u001b[0m\n",
      "\u001b[34mUsing fp16: True, LoRA: False\u001b[0m\n",
      "\u001b[35mINFO:__main__:Average train input length: 92.13\u001b[0m\n",
      "\u001b[35mUsing fp16: True, LoRA: False\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[35mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10335.188 Total: 15360.000 (67.3% used). Free: 5024.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10335.188 Total: 15360.000 (67.3% used). Free: 5024.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10335.188 Total: 15360.000 (67.3% used). Free: 5024.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10335.188 Total: 15360.000 (67.3% used). Free: 5024.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10335.188 Total: 15360.000 (67.3% used). Free: 5024.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10335.188 Total: 15360.000 (67.3% used). Free: 5024.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10335.188 Total: 15360.000 (67.3% used). Free: 5024.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10335.188 Total: 15360.000 (67.3% used). Free: 5024.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13155.188 Total: 15360.000 (85.6% used). Free: 2204.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 13155.188 Total: 15360.000 (85.6% used). Free: 2204.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.3129, 'learning_rate': 4.501187648456057e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[35m{'loss': 0.3129, 'learning_rate': 4.501187648456057e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.2491, 'learning_rate': 4.001900237529692e-05, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[35m{'loss': 0.2491, 'learning_rate': 4.001900237529692e-05, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 3.19MB/s]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2872145175933838, 'eval_accuracy': 0.9094036697247706, 'eval_runtime': 9.0954, 'eval_samples_per_second': 95.873, 'eval_steps_per_second': 6.047, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[35mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 3.19MB/s]\u001b[0m\n",
      "\u001b[35m{'eval_loss': 0.2872145175933838, 'eval_accuracy': 0.9094036697247706, 'eval_runtime': 9.0954, 'eval_samples_per_second': 95.873, 'eval_steps_per_second': 6.047, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1899, 'learning_rate': 3.502137767220903e-05, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[35m{'loss': 0.1899, 'learning_rate': 3.502137767220903e-05, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1821, 'learning_rate': 3.002850356294537e-05, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[35m{'loss': 0.1821, 'learning_rate': 3.002850356294537e-05, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3941206634044647, 'eval_accuracy': 0.8899082568807339, 'eval_runtime': 9.459, 'eval_samples_per_second': 92.188, 'eval_steps_per_second': 5.815, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[35m{'eval_loss': 0.3941206634044647, 'eval_accuracy': 0.8899082568807339, 'eval_runtime': 9.459, 'eval_samples_per_second': 92.188, 'eval_steps_per_second': 5.815, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1424, 'learning_rate': 2.5033254156769597e-05, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[35m{'loss': 0.1424, 'learning_rate': 2.5033254156769597e-05, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1509, 'learning_rate': 2.0038004750593826e-05, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[35m{'loss': 0.1509, 'learning_rate': 2.0038004750593826e-05, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.27899688482284546, 'eval_accuracy': 0.9174311926605505, 'eval_runtime': 9.4971, 'eval_samples_per_second': 91.818, 'eval_steps_per_second': 5.791, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[35m{'eval_loss': 0.27899688482284546, 'eval_accuracy': 0.9174311926605505, 'eval_runtime': 9.4971, 'eval_samples_per_second': 91.818, 'eval_steps_per_second': 5.791, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[35mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[32mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[32mbash: no job control in this shell\u001b[0m\n",
      "\u001b[32m2023-06-26 14:01:09,120 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[32m2023-06-26 14:01:09,137 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-06-26 14:01:09,145 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[32m2023-06-26 14:01:09,154 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[32m2023-06-26 14:01:11,208 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[32m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[32mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[32mDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 55.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting accelerate>=0.20.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[32mDownloading accelerate-0.20.3-py3-none-any.whl (227 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.6/227.6 kB 37.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[32mDownloading datasets-2.13.1-py3-none-any.whl (486 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 486.2/486.2 kB 63.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[32mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 18.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[32mDownloading peft-0.3.0-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 17.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[32mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 13.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[32mCollecting huggingface-hub<1.0,>=0.14.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[32mDownloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.8/236.8 kB 42.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[32mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[32mDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 770.4/770.4 kB 59.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[32mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[32mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 54.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[32mDownloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 81.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[32mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[32mDownloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 40.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.5.0)\u001b[0m\n",
      "\u001b[32mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[32mDownloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 70.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[32mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[32mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[32mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 29.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[32mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[32mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[32mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 44.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[32mDownloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\u001b[0m\n",
      "\u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 34.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[32mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[32mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[32mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, accelerate, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[32mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[32mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[32mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[32mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[32mSuccessfully installed accelerate-0.20.3 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.13.1 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 peft-0.3.0 pynvml-11.5.0 regex-2023.6.3 responses-0.18.0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[32mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[32m2023-06-26 14:01:22,872 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[32m2023-06-26 14:01:22,872 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[32m2023-06-26 14:01:22,890 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-06-26 14:01:22,916 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-06-26 14:01:22,941 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-06-26 14:01:22,951 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[32mTraining Env:\u001b[0m\n",
      "\u001b[32m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"8\",\n",
      "        \"learning-rate\": \"5e-05\",\n",
      "        \"use-fp16\": \"1\",\n",
      "        \"use-hf-lora\": \"0\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-06-26-12-18-36-915\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-36-915/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[32m}\u001b[0m\n",
      "\u001b[32mEnvironment variables:\u001b[0m\n",
      "\u001b[32mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[32mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[32mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"5e-05\",\"use-fp16\":\"1\",\"use-hf-lora\":\"0\"}\u001b[0m\n",
      "\u001b[32mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[32mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[32mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[32mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[32mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[32mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[32mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[32mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[32mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[32mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[32mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[32mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[32mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[32mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[32mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[32mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[32mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[32mSM_MODULE_DIR=s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-36-915/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[32mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"5e-05\",\"use-fp16\":\"1\",\"use-hf-lora\":\"0\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-06-26-12-18-36-915\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-36-915/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[32mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"8\",\"--learning-rate\",\"5e-05\",\"--use-fp16\",\"1\",\"--use-hf-lora\",\"0\"]\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[32mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[32mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[32mSM_HP_INPUT-SCALE=8\u001b[0m\n",
      "\u001b[32mSM_HP_LEARNING-RATE=5e-05\u001b[0m\n",
      "\u001b[32mSM_HP_USE-FP16=1\u001b[0m\n",
      "\u001b[32mSM_HP_USE-HF-LORA=0\u001b[0m\n",
      "\u001b[32mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[32mInvoking script with the following command:\u001b[0m\n",
      "\u001b[32m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 8 --learning-rate 5e-05 --use-fp16 1 --use-hf-lora 0\u001b[0m\n",
      "\u001b[32m2023-06-26 14:01:22,982 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[32mINFO:__main__:transformers 4.30.2\u001b[0m\n",
      "\u001b[32mINFO:__main__:peft 0.3.0\u001b[0m\n",
      "\u001b[32mINFO:__main__:torch 2.0.0\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[32mDownloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[32mDownloading (…)lve/main/config.json: 100%|██████████| 481/481 [00:00<00:00, 2.62MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:   8%|▊         | 41.9M/499M [00:00<00:01, 322MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  17%|█▋        | 83.9M/499M [00:00<00:01, 323MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  25%|██▌       | 126M/499M [00:00<00:01, 308MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  34%|███▎      | 168M/499M [00:00<00:00, 342MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  42%|████▏     | 210M/499M [00:00<00:00, 337MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  50%|█████     | 252M/499M [00:00<00:00, 352MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  61%|██████    | 304M/499M [00:00<00:00, 379MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  69%|██████▉   | 346M/499M [00:00<00:00, 376MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  78%|███████▊  | 388M/499M [00:01<00:00, 366MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  86%|████████▌ | 430M/499M [00:01<00:00, 376MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors:  95%|█████████▍| 472M/499M [00:01<00:00, 381MB/s]\u001b[0m\n",
      "\u001b[32mDownloading model.safetensors: 100%|██████████| 499M/499M [00:01<00:00, 355MB/s]\u001b[0m\n",
      "\u001b[32mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\u001b[0m\n",
      "\u001b[32m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[32m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[32mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[32mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[32mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\u001b[0m\n",
      "\u001b[32m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[32m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[32mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[32mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[32mDownloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[32mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 6.56MB/s]\u001b[0m\n",
      "\u001b[32mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 6.53MB/s]\u001b[0m\n",
      "\u001b[32mDownloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[32mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.24MB/s]\u001b[0m\n",
      "\u001b[32mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.23MB/s]\u001b[0m\n",
      "\u001b[32mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[32mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 4.05MB/s]\u001b[0m\n",
      "\u001b[32mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 4.04MB/s]\u001b[0m\n",
      "\u001b[32mINFO:__main__:Total parameters: 124,647,170, thereof learnable: 124,647,170 (100.0000%)\u001b[0m\n",
      "\u001b[32mDownloading and preparing dataset glue/sst2 to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[32mDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:Average train input length: 92.13\u001b[0m\n",
      "\u001b[32mUsing fp16: True, LoRA: False\u001b[0m\n",
      "\u001b[32m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[32mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[32mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 10335.188 Total: 15360.000 (67.3% used). Free: 5024.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 10335.188 Total: 15360.000 (67.3% used). Free: 5024.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 10335.188 Total: 15360.000 (67.3% used). Free: 5024.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 13155.188 Total: 15360.000 (85.6% used). Free: 2204.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[32m{'loss': 0.3118, 'learning_rate': 4.501425178147269e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[32mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[36mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[36mbash: no job control in this shell\u001b[0m\n",
      "\u001b[36m2023-06-26 14:34:32,636 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[36m2023-06-26 14:34:32,653 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[36m2023-06-26 14:34:32,663 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[36m2023-06-26 14:34:32,669 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[36m2023-06-26 14:34:34,836 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[36m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[36mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[36mDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\u001b[0m\n",
      "\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 67.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[36mCollecting accelerate>=0.20.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[36mDownloading accelerate-0.20.3-py3-none-any.whl (227 kB)\u001b[0m\n",
      "\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.6/227.6 kB 53.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[36mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[36mDownloading datasets-2.13.1-py3-none-any.whl (486 kB)\u001b[0m\n",
      "\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 486.2/486.2 kB 77.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[36mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[36mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 25.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[36mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[36mDownloading peft-0.3.0-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 18.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[36mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[36mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 19.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[36mCollecting huggingface-hub<1.0,>=0.14.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[36mDownloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\u001b[0m\n",
      "\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.8/236.8 kB 57.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[36mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[36mDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\u001b[0m\n",
      "\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 770.4/770.4 kB 89.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[36mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[36mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 68.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[36mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[36mDownloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 107.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[36mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[36mDownloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 44.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.5.0)\u001b[0m\n",
      "\u001b[36mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[36mDownloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 97.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[36mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[36mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[36mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[36mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 32.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[36mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[36mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[36mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[36mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 55.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[36mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[36mDownloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\u001b[0m\n",
      "\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 40.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[36mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[36mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[36mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, accelerate, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[36mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[36mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[36mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[36mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[36mSuccessfully installed accelerate-0.20.3 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.13.1 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 peft-0.3.0 pynvml-11.5.0 regex-2023.6.3 responses-0.18.0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[36mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[36m2023-06-26 14:34:46,730 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[36m2023-06-26 14:34:46,731 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[36m2023-06-26 14:34:46,749 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[36m2023-06-26 14:34:46,776 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[36m2023-06-26 14:34:46,803 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[36m2023-06-26 14:34:46,814 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[36mTraining Env:\u001b[0m\n",
      "\u001b[36m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"8\",\n",
      "        \"learning-rate\": \"5e-05\",\n",
      "        \"use-fp16\": \"1\",\n",
      "        \"use-hf-lora\": \"0\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-06-26-12-18-36-915\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-36-915/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[36m}\u001b[0m\n",
      "\u001b[36mEnvironment variables:\u001b[0m\n",
      "\u001b[36mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[36mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[36mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"5e-05\",\"use-fp16\":\"1\",\"use-hf-lora\":\"0\"}\u001b[0m\n",
      "\u001b[36mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[36mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[36mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[36mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[36mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[36mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[36mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[36mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[36mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[36mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[36mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[36mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[36mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[36mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[36mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[36mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[36mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[36mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[36mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[36mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[36mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[36mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[36mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[36mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[36mSM_MODULE_DIR=s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-36-915/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[36mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"5e-05\",\"use-fp16\":\"1\",\"use-hf-lora\":\"0\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-06-26-12-18-36-915\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-36-915/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[36mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"8\",\"--learning-rate\",\"5e-05\",\"--use-fp16\",\"1\",\"--use-hf-lora\",\"0\"]\u001b[0m\n",
      "\u001b[36mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[36mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[36mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[36mSM_HP_INPUT-SCALE=8\u001b[0m\n",
      "\u001b[36mSM_HP_LEARNING-RATE=5e-05\u001b[0m\n",
      "\u001b[36mSM_HP_USE-FP16=1\u001b[0m\n",
      "\u001b[36mSM_HP_USE-HF-LORA=0\u001b[0m\n",
      "\u001b[36mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[36mInvoking script with the following command:\u001b[0m\n",
      "\u001b[36m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 8 --learning-rate 5e-05 --use-fp16 1 --use-hf-lora 0\u001b[0m\n",
      "\u001b[36m2023-06-26 14:34:46,847 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[36mINFO:__main__:transformers 4.30.2\u001b[0m\n",
      "\u001b[36mINFO:__main__:peft 0.3.0\u001b[0m\n",
      "\u001b[36mINFO:__main__:torch 2.0.0\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[36mDownloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[36mDownloading (…)lve/main/config.json: 100%|██████████| 481/481 [00:00<00:00, 2.67MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:   2%|▏         | 10.5M/499M [00:00<00:19, 25.3MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:   4%|▍         | 21.0M/499M [00:00<00:17, 27.1MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:   6%|▋         | 31.5M/499M [00:01<00:14, 31.3MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:   8%|▊         | 41.9M/499M [00:01<00:14, 32.4MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  11%|█         | 52.4M/499M [00:01<00:13, 32.9MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  13%|█▎        | 62.9M/499M [00:01<00:13, 33.4MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  15%|█▍        | 73.4M/499M [00:02<00:11, 36.2MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  17%|█▋        | 83.9M/499M [00:02<00:11, 35.4MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  19%|█▉        | 94.4M/499M [00:02<00:12, 31.6MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  21%|██        | 105M/499M [00:03<00:11, 33.6MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  23%|██▎       | 115M/499M [00:03<00:12, 31.4MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  25%|██▌       | 126M/499M [00:03<00:11, 32.5MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  27%|██▋       | 136M/499M [00:04<00:11, 32.7MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  29%|██▉       | 147M/499M [00:04<00:10, 33.3MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  32%|███▏      | 157M/499M [00:04<00:09, 37.5MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  34%|███▎      | 168M/499M [00:04<00:09, 36.7MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  36%|███▌      | 178M/499M [00:05<00:09, 35.1MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  38%|███▊      | 189M/499M [00:05<00:08, 35.5MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  40%|███▉      | 199M/499M [00:05<00:08, 35.9MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  42%|████▏     | 210M/499M [00:06<00:07, 36.9MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  44%|████▍     | 220M/499M [00:06<00:07, 37.9MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  46%|████▌     | 231M/499M [00:06<00:06, 41.2MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  48%|████▊     | 241M/499M [00:06<00:06, 40.5MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  50%|█████     | 252M/499M [00:07<00:07, 35.1MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  53%|█████▎    | 262M/499M [00:07<00:07, 30.3MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  55%|█████▍    | 273M/499M [00:08<00:07, 30.6MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  57%|█████▋    | 283M/499M [00:08<00:06, 31.6MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  59%|█████▉    | 294M/499M [00:08<00:06, 32.2MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  61%|██████    | 304M/499M [00:08<00:05, 33.8MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  63%|██████▎   | 315M/499M [00:09<00:05, 35.1MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  65%|██████▌   | 325M/499M [00:09<00:04, 36.3MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  67%|██████▋   | 336M/499M [00:09<00:04, 33.2MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  69%|██████▉   | 346M/499M [00:10<00:05, 29.8MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  71%|███████▏  | 357M/499M [00:10<00:05, 27.2MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  74%|███████▎  | 367M/499M [00:11<00:05, 25.0MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  76%|███████▌  | 377M/499M [00:11<00:04, 25.7MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  78%|███████▊  | 388M/499M [00:12<00:04, 27.2MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  80%|███████▉  | 398M/499M [00:12<00:03, 26.5MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  82%|████████▏ | 409M/499M [00:12<00:03, 27.7MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  84%|████████▍ | 419M/499M [00:13<00:02, 29.5MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  86%|████████▌ | 430M/499M [00:13<00:02, 29.9MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  88%|████████▊ | 440M/499M [00:13<00:01, 30.4MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  90%|█████████ | 451M/499M [00:14<00:01, 31.5MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  92%|█████████▏| 461M/499M [00:14<00:01, 30.3MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  95%|█████████▍| 472M/499M [00:15<00:01, 24.9MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  97%|█████████▋| 482M/499M [00:15<00:00, 23.3MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors:  99%|█████████▉| 493M/499M [00:15<00:00, 24.1MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors: 100%|██████████| 499M/499M [00:16<00:00, 24.5MB/s]\u001b[0m\n",
      "\u001b[36mDownloading model.safetensors: 100%|██████████| 499M/499M [00:16<00:00, 30.9MB/s]\u001b[0m\n",
      "\u001b[36mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\u001b[0m\n",
      "\u001b[36m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[36m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[36mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\u001b[0m\n",
      "\u001b[36mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[36mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\u001b[0m\n",
      "\u001b[36m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[36m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[36mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\u001b[0m\n",
      "\u001b[36mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[36mDownloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[36mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 4.41MB/s]\u001b[0m\n",
      "\u001b[36mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 4.39MB/s]\u001b[0m\n",
      "\u001b[36mDownloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[36mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.24MB/s]\u001b[0m\n",
      "\u001b[36mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.23MB/s]\u001b[0m\n",
      "\u001b[36mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[36mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 3.98MB/s]\u001b[0m\n",
      "\u001b[36mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 3.97MB/s]\u001b[0m\n",
      "\u001b[36mINFO:__main__:Total parameters: 124,647,170, thereof learnable: 124,647,170 (100.0000%)\u001b[0m\n",
      "\u001b[36mDownloading and preparing dataset glue/sst2 to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[36mDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:Average train input length: 92.13\u001b[0m\n",
      "\u001b[36mUsing fp16: True, LoRA: False\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 275.188 Total: 15360.000 (1.8% used). Free: 15084.812\u001b[0m\n",
      "\u001b[36m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[36mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[36mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 10335.188 Total: 15360.000 (67.3% used). Free: 5024.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 10335.188 Total: 15360.000 (67.3% used). Free: 5024.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 10335.188 Total: 15360.000 (67.3% used). Free: 5024.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 13155.188 Total: 15360.000 (85.6% used). Free: 2204.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[36m{'loss': 0.3588, 'learning_rate': 4.501900237529692e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[36mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[33mbash: no job control in this shell\u001b[0m\n",
      "\u001b[33m2023-06-26 15:14:31,988 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[33m2023-06-26 15:14:32,006 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[33m2023-06-26 15:14:32,015 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[33m2023-06-26 15:14:32,021 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[33m2023-06-26 15:14:33,998 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[33m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[33mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[33mDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\u001b[0m\n",
      "\u001b[33m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 78.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[33mCollecting accelerate>=0.20.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[33mDownloading accelerate-0.20.3-py3-none-any.whl (227 kB)\u001b[0m\n",
      "\u001b[33m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.6/227.6 kB 51.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[33mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[33mDownloading datasets-2.13.1-py3-none-any.whl (486 kB)\u001b[0m\n",
      "\u001b[33m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 486.2/486.2 kB 75.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[33mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[33mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[33m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 23.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[33mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[33mDownloading peft-0.3.0-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[33m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 17.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[33mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[33mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[33m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 17.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[33mCollecting huggingface-hub<1.0,>=0.14.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[33mDownloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\u001b[0m\n",
      "\u001b[33m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.8/236.8 kB 49.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[33mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[33mDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\u001b[0m\n",
      "\u001b[33m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 770.4/770.4 kB 78.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[33mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[33mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[33m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 102.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[33mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[33mDownloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[33m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 75.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[33mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[33mDownloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[33m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 50.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.5.0)\u001b[0m\n",
      "\u001b[33mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[33mDownloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[33m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 92.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[33mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[33mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[33mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[33mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[33m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 33.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[33mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[33mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[33mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[33mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[33m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 56.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[33mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[33mDownloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\u001b[0m\n",
      "\u001b[33m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 37.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[33mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[33mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[33mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[33mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, accelerate, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[33mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[33mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[33mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[33mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[33mSuccessfully installed accelerate-0.20.3 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.13.1 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 peft-0.3.0 pynvml-11.5.0 regex-2023.6.3 responses-0.18.0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33m2023-06-26 15:14:45,969 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[33m2023-06-26 15:14:45,969 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[33m2023-06-26 15:14:45,987 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[33m2023-06-26 15:14:46,014 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[33m2023-06-26 15:14:46,040 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[33m2023-06-26 15:14:46,051 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[33mTraining Env:\u001b[0m\n",
      "\u001b[33m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"8\",\n",
      "        \"learning-rate\": \"5e-05\",\n",
      "        \"use-fp16\": \"1\",\n",
      "        \"use-hf-lora\": \"0\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-06-26-12-18-36-915\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-36-915/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[33m}\u001b[0m\n",
      "\u001b[33mEnvironment variables:\u001b[0m\n",
      "\u001b[33mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[33mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[33mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"5e-05\",\"use-fp16\":\"1\",\"use-hf-lora\":\"0\"}\u001b[0m\n",
      "\u001b[33mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[33mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[33mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[33mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[33mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[33mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[33mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[33mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[33mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[33mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[33mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[33mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[33mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[33mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[33mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[33mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[33mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[33mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[33mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[33mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[33mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[33mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[33mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[33mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[33mSM_MODULE_DIR=s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-36-915/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[33mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"5e-05\",\"use-fp16\":\"1\",\"use-hf-lora\":\"0\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-06-26-12-18-36-915\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-36-915/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[33mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"8\",\"--learning-rate\",\"5e-05\",\"--use-fp16\",\"1\",\"--use-hf-lora\",\"0\"]\u001b[0m\n",
      "\u001b[33mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[33mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[33mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[33mSM_HP_INPUT-SCALE=8\u001b[0m\n",
      "\u001b[33mSM_HP_LEARNING-RATE=5e-05\u001b[0m\n",
      "\u001b[33mSM_HP_USE-FP16=1\u001b[0m\n",
      "\u001b[33mSM_HP_USE-HF-LORA=0\u001b[0m\n",
      "\u001b[33mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[33mInvoking script with the following command:\u001b[0m\n",
      "\u001b[33m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 8 --learning-rate 5e-05 --use-fp16 1 --use-hf-lora 0\u001b[0m\n",
      "\u001b[33m2023-06-26 15:14:46,084 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[33mINFO:__main__:transformers 4.30.2\u001b[0m\n",
      "\u001b[33mINFO:__main__:peft 0.3.0\u001b[0m\n",
      "\u001b[33mINFO:__main__:torch 2.0.0\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[33mDownloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[33mDownloading (…)lve/main/config.json: 100%|██████████| 481/481 [00:00<00:00, 2.59MB/s]\u001b[0m\n",
      "\u001b[33mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[33mDownloading model.safetensors:   6%|▋         | 31.5M/499M [00:00<00:01, 302MB/s]\u001b[0m\n",
      "\u001b[33mDownloading model.safetensors:  17%|█▋        | 83.9M/499M [00:00<00:01, 401MB/s]\u001b[0m\n",
      "\u001b[33mDownloading model.safetensors:  25%|██▌       | 126M/499M [00:00<00:00, 400MB/s]\u001b[0m\n",
      "\u001b[33mDownloading model.safetensors:  34%|███▎      | 168M/499M [00:00<00:00, 382MB/s]\u001b[0m\n",
      "\u001b[33mDownloading model.safetensors:  42%|████▏     | 210M/499M [00:00<00:00, 382MB/s]\u001b[0m\n",
      "\u001b[33mDownloading model.safetensors:  50%|█████     | 252M/499M [00:00<00:00, 380MB/s]\u001b[0m\n",
      "\u001b[33mDownloading model.safetensors:  59%|█████▉    | 294M/499M [00:00<00:00, 336MB/s]\u001b[0m\n",
      "\u001b[33mDownloading model.safetensors:  67%|██████▋   | 336M/499M [00:00<00:00, 323MB/s]\u001b[0m\n",
      "\u001b[33mDownloading model.safetensors:  76%|███████▌  | 377M/499M [00:01<00:00, 311MB/s]\u001b[0m\n",
      "\u001b[33mDownloading model.safetensors:  84%|████████▍ | 419M/499M [00:01<00:00, 309MB/s]\u001b[0m\n",
      "\u001b[33mDownloading model.safetensors:  92%|█████████▏| 461M/499M [00:01<00:00, 324MB/s]\u001b[0m\n",
      "\u001b[33mDownloading model.safetensors: 100%|██████████| 499M/499M [00:01<00:00, 344MB/s]\u001b[0m\n",
      "\u001b[33mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\u001b[0m\n",
      "\u001b[33m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[33m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[33mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[33mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[33mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\u001b[0m\n",
      "\u001b[33m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[33m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[33mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[33mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[33mDownloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[33mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.33MB/s]\u001b[0m\n",
      "\u001b[33mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.32MB/s]\u001b[0m\n",
      "\u001b[33mDownloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[33mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.25MB/s]\u001b[0m\n",
      "\u001b[33mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.25MB/s]\u001b[0m\n",
      "\u001b[33mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[33mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 4.02MB/s]\u001b[0m\n",
      "\u001b[33mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 4.01MB/s]\u001b[0m\n",
      "\u001b[33mINFO:__main__:Total parameters: 124,647,170, thereof learnable: 124,647,170 (100.0000%)\u001b[0m\n",
      "\u001b[33mDownloading and preparing dataset glue/sst2 to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[33mDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:Average train input length: 92.13\u001b[0m\n",
      "\u001b[33mUsing fp16: True, LoRA: False\u001b[0m\n",
      "\u001b[33m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[33mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[33mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10335.188 Total: 15360.000 (67.3% used). Free: 5024.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10335.188 Total: 15360.000 (67.3% used). Free: 5024.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10335.188 Total: 15360.000 (67.3% used). Free: 5024.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10335.188 Total: 15360.000 (67.3% used). Free: 5024.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 13177.188 Total: 15360.000 (85.8% used). Free: 2182.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[33m{'loss': 0.3457, 'learning_rate': 4.501425178147269e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10833.188 Total: 15360.000 (70.5% used). Free: 4526.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10857.188 Total: 15360.000 (70.7% used). Free: 4502.812\u001b[0m\n",
      "\u001b[33m{'loss': 0.2857, 'learning_rate': 4.0021377672209026e-05, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[33mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[33mDownloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 3.16MB/s]\u001b[0m\n",
      "\u001b[33m{'eval_loss': 0.3479735255241394, 'eval_accuracy': 0.8727064220183486, 'eval_runtime': 8.9696, 'eval_samples_per_second': 97.217, 'eval_steps_per_second': 6.132, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33m{'loss': 0.2752, 'learning_rate': 3.502850356294537e-05, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33m{'loss': 0.2498, 'learning_rate': 3.0030878859857487e-05, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[33m{'eval_loss': 0.33682459592819214, 'eval_accuracy': 0.8956422018348624, 'eval_runtime': 8.8882, 'eval_samples_per_second': 98.108, 'eval_steps_per_second': 6.188, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33m{'loss': 0.1926, 'learning_rate': 2.5033254156769597e-05, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33m{'loss': 0.175, 'learning_rate': 2.0038004750593826e-05, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[33m{'eval_loss': 0.31505411863327026, 'eval_accuracy': 0.8922018348623854, 'eval_runtime': 8.953, 'eval_samples_per_second': 97.397, 'eval_steps_per_second': 6.143, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33m{'loss': 0.1349, 'learning_rate': 1.5040380047505939e-05, 'epoch': 3.5}\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33m{'loss': 0.13, 'learning_rate': 1.0045130641330166e-05, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33m{'eval_loss': 0.3609813153743744, 'eval_accuracy': 0.9071100917431193, 'eval_runtime': 8.6046, 'eval_samples_per_second': 101.341, 'eval_steps_per_second': 6.392, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33m{'loss': 0.0991, 'learning_rate': 5.047505938242281e-06, 'epoch': 4.5}\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33m{'loss': 0.0889, 'learning_rate': 5.4631828978622335e-08, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[33mINFO:__main__:GPU Usage. Used: 10953.188 Total: 15360.000 (71.3% used). Free: 4406.812\u001b[0m\n",
      "\u001b[33m{'eval_loss': 0.3624002933502197, 'eval_accuracy': 0.911697247706422, 'eval_runtime': 8.9294, 'eval_samples_per_second': 97.655, 'eval_steps_per_second': 6.159, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[33m{'train_runtime': 7346.6881, 'train_samples_per_second': 45.836, 'train_steps_per_second': 2.865, 'train_loss': 0.19763943424134242, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[33m{'eval_loss': 0.3624002933502197, 'eval_accuracy': 0.911697247706422, 'eval_runtime': 8.95, 'eval_samples_per_second': 97.43, 'eval_steps_per_second': 6.145, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[33m2023-06-26 17:18:13,265 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[33m2023-06-26 17:18:13,265 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[33m2023-06-26 17:18:13,265 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 13283\n",
      "Billable seconds: 4478\n",
      "Managed Spot Training savings: 66.3%\n",
      "\n",
      "\n",
      "\n",
      "##### Log of pytorch-training-2023-06-26-12-18-38-166 (full, 32):\n",
      "\n",
      "2023-06-26 13:15:36 Starting - Preparing the instances for training\n",
      "2023-06-26 13:15:36 Downloading - Downloading input data\n",
      "2023-06-26 13:15:36 Training - Training image download completed. Training in progress.\n",
      "2023-06-26 13:15:36 Uploading - Uploading generated training model\n",
      "2023-06-26 13:15:36 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:08,572 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:08,588 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:08,597 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:08,603 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:10,587 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 91.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate>=0.20.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.20.3-py3-none-any.whl (227 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.6/227.6 kB 47.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.13.1-py3-none-any.whl (486 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 486.2/486.2 kB 75.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 25.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.3.0-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 20.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 18.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.14.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.8/236.8 kB 54.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 770.4/770.4 kB 77.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 84.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 83.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 51.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 93.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 35.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 56.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 43.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, accelerate, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.20.3 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.13.1 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 peft-0.3.0 pynvml-11.5.0 regex-2023.6.3 responses-0.18.0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:22,079 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:22,079 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:22,096 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:22,121 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:22,145 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:22,155 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"1\",\n",
      "        \"learning-rate\": \"5e-05\",\n",
      "        \"use-fp16\": \"0\",\n",
      "        \"use-hf-lora\": \"0\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-06-26-12-18-38-166\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-38-166/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"1\",\"learning-rate\":\"5e-05\",\"use-fp16\":\"0\",\"use-hf-lora\":\"0\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-38-166/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"1\",\"learning-rate\":\"5e-05\",\"use-fp16\":\"0\",\"use-hf-lora\":\"0\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-06-26-12-18-38-166\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-38-166/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"1\",\"--learning-rate\",\"5e-05\",\"--use-fp16\",\"0\",\"--use-hf-lora\",\"0\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-SCALE=1\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=5e-05\u001b[0m\n",
      "\u001b[34mSM_HP_USE-FP16=0\u001b[0m\n",
      "\u001b[34mSM_HP_USE-HF-LORA=0\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 1 --learning-rate 5e-05 --use-fp16 0 --use-hf-lora 0\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:22,186 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:__main__:transformers 4.30.2\u001b[0m\n",
      "\u001b[34mINFO:__main__:peft 0.3.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:torch 2.0.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 481/481 [00:00<00:00, 2.71MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   6%|▋         | 31.5M/499M [00:00<00:01, 251MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  13%|█▎        | 62.9M/499M [00:00<00:01, 282MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  21%|██        | 105M/499M [00:00<00:01, 298MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  29%|██▉       | 147M/499M [00:00<00:01, 314MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  38%|███▊      | 189M/499M [00:00<00:00, 336MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  46%|████▌     | 231M/499M [00:00<00:00, 341MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  55%|█████▍    | 273M/499M [00:00<00:00, 314MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  63%|██████▎   | 315M/499M [00:01<00:00, 254MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  71%|███████▏  | 357M/499M [00:01<00:00, 274MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  78%|███████▊  | 388M/499M [00:01<00:00, 249MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  84%|████████▍ | 419M/499M [00:01<00:00, 263MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  90%|█████████ | 451M/499M [00:01<00:00, 253MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  99%|█████████▉| 493M/499M [00:01<00:00, 291MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|██████████| 499M/499M [00:01<00:00, 285MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.30MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.29MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.49MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.49MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 6.61MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 6.58MB/s]\u001b[0m\n",
      "\u001b[34mINFO:__main__:Total parameters: 124,647,170, thereof learnable: 124,647,170 (100.0000%)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset glue/sst2 to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[34mDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average train input length: 14.36\u001b[0m\n",
      "\u001b[34mUsing fp16: False, LoRA: False\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3705.188 Total: 15360.000 (24.1% used). Free: 11654.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3705.188 Total: 15360.000 (24.1% used). Free: 11654.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3705.188 Total: 15360.000 (24.1% used). Free: 11654.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3705.188 Total: 15360.000 (24.1% used). Free: 11654.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.3523, 'learning_rate': 4.500237529691211e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.2858, 'learning_rate': 4.000475059382423e-05, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 6.09MB/s]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.28165820240974426, 'eval_accuracy': 0.9036697247706422, 'eval_runtime': 2.9417, 'eval_samples_per_second': 296.429, 'eval_steps_per_second': 18.697, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.2337, 'learning_rate': 3.5007125890736345e-05, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.2116, 'learning_rate': 3.0009501187648458e-05, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3038114607334137, 'eval_accuracy': 0.9128440366972477, 'eval_runtime': 2.7884, 'eval_samples_per_second': 312.722, 'eval_steps_per_second': 19.724, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1777, 'learning_rate': 2.501187648456057e-05, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1713, 'learning_rate': 2.0014251781472684e-05, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.31996288895606995, 'eval_accuracy': 0.9185779816513762, 'eval_runtime': 2.4516, 'eval_samples_per_second': 355.685, 'eval_steps_per_second': 22.434, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.1263, 'learning_rate': 1.5016627078384798e-05, 'epoch': 3.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.121, 'learning_rate': 1.0019002375296913e-05, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.30805593729019165, 'eval_accuracy': 0.9197247706422018, 'eval_runtime': 2.8851, 'eval_samples_per_second': 302.239, 'eval_steps_per_second': 19.063, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.091, 'learning_rate': 5.021377672209027e-06, 'epoch': 4.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.0832, 'learning_rate': 2.3752969121140145e-08, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.33643898367881775, 'eval_accuracy': 0.9197247706422018, 'eval_runtime': 2.8064, 'eval_samples_per_second': 310.724, 'eval_steps_per_second': 19.598, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 2797.1822, 'train_samples_per_second': 120.387, 'train_steps_per_second': 7.525, 'train_loss': 0.18532511410690544, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3707.188 Total: 15360.000 (24.1% used). Free: 11652.812\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.30805593729019165, 'eval_accuracy': 0.9197247706422018, 'eval_runtime': 2.7525, 'eval_samples_per_second': 316.805, 'eval_steps_per_second': 19.982, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m2023-06-26 13:12:32,914 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-06-26 13:12:32,914 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-06-26 13:12:32,914 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 3344\n",
      "Billable seconds: 1204\n",
      "Managed Spot Training savings: 64.0%\n",
      "\n",
      "\n",
      "\n",
      "##### Log of pytorch-training-2023-06-26-12-18-39-489 (full, 32):\n",
      "\n",
      "2023-06-26 16:30:34 Starting - Preparing the instances for training\n",
      "2023-06-26 16:30:34 Downloading - Downloading input data\n",
      "2023-06-26 16:30:34 Training - Training image download completed. Training in progress.\n",
      "2023-06-26 16:30:34 Uploading - Uploading generated training model\n",
      "2023-06-26 16:30:34 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:07,849 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:07,866 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:07,875 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:07,881 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:09,835 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 82.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate>=0.20.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.20.3-py3-none-any.whl (227 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.6/227.6 kB 44.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.13.1-py3-none-any.whl (486 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 486.2/486.2 kB 75.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 21.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.3.0-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 17.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 17.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.14.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 236.8/236.8 kB 47.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 770.4/770.4 kB 72.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 99.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 106.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.1->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 33.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 76.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 31.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 51.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 37.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate>=0.20.1->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, accelerate, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.20.3 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.13.1 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 peft-0.3.0 pynvml-11.5.0 regex-2023.6.3 responses-0.18.0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,555 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,556 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,574 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,601 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,628 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,639 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"8\",\n",
      "        \"learning-rate\": \"5e-05\",\n",
      "        \"use-fp16\": \"0\",\n",
      "        \"use-hf-lora\": \"0\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-06-26-12-18-39-489\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-39-489/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"5e-05\",\"use-fp16\":\"0\",\"use-hf-lora\":\"0\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-39-489/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"5e-05\",\"use-fp16\":\"0\",\"use-hf-lora\":\"0\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-06-26-12-18-39-489\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-811243659808/pytorch-training-2023-06-26-12-18-39-489/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"8\",\"--learning-rate\",\"5e-05\",\"--use-fp16\",\"0\",\"--use-hf-lora\",\"0\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-SCALE=8\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=5e-05\u001b[0m\n",
      "\u001b[34mSM_HP_USE-FP16=0\u001b[0m\n",
      "\u001b[34mSM_HP_USE-HF-LORA=0\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 8 --learning-rate 5e-05 --use-fp16 0 --use-hf-lora 0\u001b[0m\n",
      "\u001b[34m2023-06-26 12:25:21,670 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:__main__:transformers 4.30.2\u001b[0m\n",
      "\u001b[34mINFO:__main__:peft 0.3.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:torch 2.0.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 481/481 [00:00<00:00, 3.39MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   4%|▍         | 21.0M/499M [00:00<00:02, 202MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   8%|▊         | 41.9M/499M [00:00<00:04, 113MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  13%|█▎        | 62.9M/499M [00:00<00:06, 71.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  15%|█▍        | 73.4M/499M [00:00<00:06, 67.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  17%|█▋        | 83.9M/499M [00:01<00:07, 55.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  19%|█▉        | 94.4M/499M [00:01<00:07, 52.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  21%|██        | 105M/499M [00:01<00:07, 52.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  23%|██▎       | 115M/499M [00:01<00:07, 51.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  25%|██▌       | 126M/499M [00:02<00:07, 51.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  27%|██▋       | 136M/499M [00:02<00:07, 51.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  29%|██▉       | 147M/499M [00:02<00:07, 47.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  32%|███▏      | 157M/499M [00:02<00:08, 40.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  34%|███▎      | 168M/499M [00:03<00:07, 42.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  36%|███▌      | 178M/499M [00:03<00:07, 44.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  38%|███▊      | 189M/499M [00:03<00:06, 45.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  40%|███▉      | 199M/499M [00:03<00:06, 48.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  42%|████▏     | 210M/499M [00:03<00:06, 47.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  44%|████▍     | 220M/499M [00:04<00:05, 49.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  46%|████▌     | 231M/499M [00:04<00:05, 46.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  48%|████▊     | 241M/499M [00:04<00:05, 47.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  50%|█████     | 252M/499M [00:04<00:05, 45.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  53%|█████▎    | 262M/499M [00:05<00:05, 46.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  55%|█████▍    | 273M/499M [00:05<00:05, 44.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  57%|█████▋    | 283M/499M [00:05<00:04, 46.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  59%|█████▉    | 294M/499M [00:05<00:04, 48.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  61%|██████    | 304M/499M [00:05<00:04, 48.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  63%|██████▎   | 315M/499M [00:06<00:03, 48.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  65%|██████▌   | 325M/499M [00:06<00:03, 49.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  67%|██████▋   | 336M/499M [00:06<00:03, 49.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  69%|██████▉   | 346M/499M [00:06<00:03, 46.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  71%|███████▏  | 357M/499M [00:07<00:02, 49.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  74%|███████▎  | 367M/499M [00:07<00:02, 50.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  76%|███████▌  | 377M/499M [00:07<00:02, 51.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  78%|███████▊  | 388M/499M [00:07<00:02, 47.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  80%|███████▉  | 398M/499M [00:07<00:02, 47.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  82%|████████▏ | 409M/499M [00:08<00:01, 49.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  84%|████████▍ | 419M/499M [00:08<00:01, 47.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  86%|████████▌ | 430M/499M [00:08<00:01, 45.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  88%|████████▊ | 440M/499M [00:08<00:01, 40.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  90%|█████████ | 451M/499M [00:09<00:01, 41.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  92%|█████████▏| 461M/499M [00:09<00:00, 43.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  95%|█████████▍| 472M/499M [00:09<00:00, 44.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  97%|█████████▋| 482M/499M [00:09<00:00, 42.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  99%|█████████▉| 493M/499M [00:10<00:00, 45.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|██████████| 499M/499M [00:10<00:00, 45.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|██████████| 499M/499M [00:10<00:00, 49.0MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.33MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.32MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.24MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.24MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 4.01MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 4.00MB/s]\u001b[0m\n",
      "\u001b[34mINFO:__main__:Total parameters: 124,647,170, thereof learnable: 124,647,170 (100.0000%)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset glue/sst2 to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[34mDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 261.188 Total: 15360.000 (1.7% used). Free: 15098.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average train input length: 92.13\u001b[0m\n",
      "\u001b[34mUsing fp16: False, LoRA: False\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14479.188 Total: 15360.000 (94.3% used). Free: 880.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14479.188 Total: 15360.000 (94.3% used). Free: 880.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14479.188 Total: 15360.000 (94.3% used). Free: 880.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14479.188 Total: 15360.000 (94.3% used). Free: 880.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14479.188 Total: 15360.000 (94.3% used). Free: 880.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14479.188 Total: 15360.000 (94.3% used). Free: 880.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14479.188 Total: 15360.000 (94.3% used). Free: 880.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13381.188 Total: 15360.000 (87.1% used). Free: 1978.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13381.188 Total: 15360.000 (87.1% used). Free: 1978.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13403.188 Total: 15360.000 (87.3% used). Free: 1956.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13075.188 Total: 15360.000 (85.1% used). Free: 2284.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13075.188 Total: 15360.000 (85.1% used). Free: 2284.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13075.188 Total: 15360.000 (85.1% used). Free: 2284.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13075.188 Total: 15360.000 (85.1% used). Free: 2284.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13075.188 Total: 15360.000 (85.1% used). Free: 2284.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13075.188 Total: 15360.000 (85.1% used). Free: 2284.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13075.188 Total: 15360.000 (85.1% used). Free: 2284.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13075.188 Total: 15360.000 (85.1% used). Free: 2284.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.6884, 'learning_rate': 4.500237529691211e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13075.188 Total: 15360.000 (85.1% used). Free: 2284.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13075.188 Total: 15360.000 (85.1% used). Free: 2284.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.6842, 'learning_rate': 4.000475059382423e-05, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 3.23MB/s]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.7004882097244263, 'eval_accuracy': 0.5091743119266054, 'eval_runtime': 17.4718, 'eval_samples_per_second': 49.909, 'eval_steps_per_second': 3.148, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.6886, 'learning_rate': 3.5007125890736345e-05, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.6888, 'learning_rate': 3.0009501187648458e-05, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.7060642838478088, 'eval_accuracy': 0.5091743119266054, 'eval_runtime': 16.0292, 'eval_samples_per_second': 54.401, 'eval_steps_per_second': 3.431, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.6883, 'learning_rate': 2.501187648456057e-05, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.6869, 'learning_rate': 2.0014251781472684e-05, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.6999753713607788, 'eval_accuracy': 0.5091743119266054, 'eval_runtime': 16.0238, 'eval_samples_per_second': 54.419, 'eval_steps_per_second': 3.432, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.6872, 'learning_rate': 1.5016627078384798e-05, 'epoch': 3.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13097.188 Total: 15360.000 (85.3% used). Free: 2262.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.675, 'learning_rate': 1.0019002375296913e-05, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.6159432530403137, 'eval_accuracy': 0.7052752293577982, 'eval_runtime': 16.42, 'eval_samples_per_second': 53.106, 'eval_steps_per_second': 3.35, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.4998, 'learning_rate': 5.021377672209027e-06, 'epoch': 4.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34m{'loss': 0.3478, 'learning_rate': 2.3752969121140145e-08, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.433819055557251, 'eval_accuracy': 0.8130733944954128, 'eval_runtime': 17.4292, 'eval_samples_per_second': 50.031, 'eval_steps_per_second': 3.156, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13183.188 Total: 15360.000 (85.8% used). Free: 2176.812\u001b[0m\n",
      "\u001b[34m{'train_runtime': 14481.0082, 'train_samples_per_second': 23.254, 'train_steps_per_second': 1.454, 'train_loss': 0.6333508260233102, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.433819055557251, 'eval_accuracy': 0.8130733944954128, 'eval_runtime': 17.391, 'eval_samples_per_second': 50.141, 'eval_steps_per_second': 3.163, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m2023-06-26 16:28:00,460 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-06-26 16:28:00,460 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-06-26 16:28:00,460 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 15043\n",
      "Billable seconds: 4840\n",
      "Managed Spot Training savings: 67.8%\n"
     ]
    }
   ],
   "source": [
    "for type, bits, est, input_scale in estimators:\n",
    "    job_name = est.latest_training_job.job_name\n",
    "    sm.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\n",
    "    print(f'\\n\\n\\n##### Log of {job_name} ({type}, {bits}):\\n')\n",
    "    est.logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505b4f2-cfb9-4952-a94e-a0517ade8750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
