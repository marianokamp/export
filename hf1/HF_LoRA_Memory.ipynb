{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f376481d-e34a-477f-bad2-b5f04a3d4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48081996-4be8-474c-985e-c77412b0a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d241390-ea0f-485f-8a3b-f1385cc09bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('png')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import boto3\n",
    "sm = boto3.client('sagemaker')\n",
    "alt.renderers.enable('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef3303fc-84ea-4638-a21c-da6d14a84562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting source/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile source/requirements.txt\n",
    "transformers\n",
    "accelerate\n",
    "datasets\n",
    "evaluate\n",
    "peft\n",
    "\n",
    "pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd356777-a457-4c58-8792-9bb9a869e328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting source/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile source/train.py\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from threading import Thread\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import peft\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "logger.info(f'transformers {transformers.__version__}')\n",
    "logger.info(f'peft {peft.__version__}')\n",
    "logger.info(f'torch {torch.__version__}')\n",
    "\n",
    "def count_parameters(m, verbose=True):\n",
    "    total_count = 0\n",
    "    learnable_count = 0\n",
    "    if verbose:\n",
    "        logger.debug(\"Parameters (name, tunable, count):\")\n",
    "\n",
    "    output_width = max([len(n) for n, _ in m.named_parameters()])\n",
    "    for n, p in m.named_parameters():\n",
    "        count = p.data.numel()\n",
    "        if verbose:\n",
    "            logger.debug(f\" {n:{output_width}} {p.requires_grad:5b} {count:>11d}\")\n",
    "        total_count += count\n",
    "        if p.requires_grad:\n",
    "            learnable_count += count\n",
    "\n",
    "    logger.info(\n",
    "        f\"Total parameters: {total_count:,}, \"\n",
    "        f\"thereof learnable: {learnable_count:,} \"\n",
    "        f\"({learnable_count/total_count*100.:5.4f}%)\"\n",
    "    )\n",
    "\n",
    "    return total_count, learnable_count\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    load_accuracy = evaluate.load(\"accuracy\")\n",
    " \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\n",
    "        \"accuracy\"\n",
    "    ]\n",
    " \n",
    "    metrics = {f\"accuracy\": accuracy}\n",
    " \n",
    "    return metrics\n",
    "\n",
    "def fit(args):\n",
    "    ### model / tokenizer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.hf_ckp, num_labels=2)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.hf_ckp)\n",
    "    collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    use_lora = True if args.use_hf_lora else False\n",
    "    if args.use_hf_lora:\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=1\n",
    "        )\n",
    "        model = get_peft_model(model, peft_config)\n",
    "\n",
    "    count_parameters(model, verbose=args.use_hf_lora)\n",
    "\n",
    "    ### data \n",
    "    datasets.logging.disable_progress_bar()\n",
    "    dataset = datasets.load_dataset(\"glue\", \"sst2\")\n",
    "    train = dataset[\"train\"]\n",
    "    valid = dataset[\"validation\"]\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(args.input_scale*examples['sentence'], padding=False, truncation=True)\n",
    "\n",
    "    tokenized_train = train.map(preprocess_function, batched=False)\n",
    "    tokenized_valid = valid.map(preprocess_function, batched=False)\n",
    "\n",
    "    mean_length = np.mean([len(v['input_ids']) for v in tokenized_train])\n",
    "    logger.info(f'Average train input length: {mean_length:5.2f}')\n",
    "\n",
    "    ### Trainer\n",
    "    \n",
    "    log_steps = len(tokenized_train) // args.batch_size // 2 # 2 log outputs per epoch\n",
    "    use_bf16 = True if args.use_bf16 and torch.cuda.is_available() else False\n",
    "    print(f'Using bf16: {use_bf16}, LoRA: {use_lora}')\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.model_dir if args.model_dir else \"out\",    \n",
    "        learning_rate=args.learning_rate,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.1,\n",
    "        push_to_hub=False,\n",
    "        bf16=use_bf16,\n",
    "        save_steps=log_steps,\n",
    "        logging_steps=log_steps,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        disable_tqdm=True,\n",
    "        metric_for_best_model='eval_accuracy',\n",
    "        load_best_model_at_end=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        # no early stopping for simplicity\n",
    "    )\n",
    "\n",
    "    ### train\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def schedule_gpu_memory_logging():\n",
    "        def log_gpu_usage():\n",
    "            if not torch.cuda.is_available():\n",
    "                return\n",
    "\n",
    "            from pynvml.smi import nvidia_smi\n",
    "\n",
    "            nvsmi = nvidia_smi.getInstance()\n",
    "            res = nvsmi.DeviceQuery(\"memory.free, memory.total, memory.used\")[\"gpu\"][0][\n",
    "                \"fb_memory_usage\"\n",
    "            ]\n",
    "            res[\"percentage\"] = res[\"used\"] / res[\"total\"] * 100\n",
    "            logger.info(\n",
    "                f'GPU Usage. Used: {res[\"used\"]:5.3f} Total: {res[\"total\"]:5.3f} ({res[\"percentage\"]:3.1f}% used). Free: {res[\"free\"]:5.3f}'\n",
    "            )\n",
    "        \n",
    "        def log_loop():\n",
    "            while True:\n",
    "                log_gpu_usage()\n",
    "                time.sleep(30)\n",
    "    \n",
    "        t = Thread(target=log_loop, daemon=True)\n",
    "        t.start()\n",
    "\n",
    "    schedule_gpu_memory_logging()\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--use-bf16\", type=int, default=1, help='1 yes, 0 no') \n",
    "    parser.add_argument(\"--use-hf-lora\", type=int, default=0, help='1 yes, 0 no')\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=4e-5)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=224)\n",
    "    parser.add_argument(\"--input-scale\", type=int, default=1)\n",
    "    parser.add_argument(\"--hf-ckp\", type=str, default='roberta-base')\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    fit(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40339088-31b7-45f0-8c39-ed11183a7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run locally with:\n",
    "#!python source/train.py --input-scale 1 --batch-size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba11b5db-5372-45d5-bbf7-dff1488edf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "hyperparameters = {\n",
    "    'use-bf16'  : 1,\n",
    "    'use-hf-lora': 1,\n",
    "    'learning-rate': 4e-5,\n",
    "    'batch-size': 16, # 224\n",
    "    'hf-ckp': 'roberta-base'\n",
    "}\n",
    "\n",
    "metric_definitions = [\n",
    "    \n",
    "    {'Name': 'train_samples_per_second', 'Regex': '\\'train_samples_per_second\\': (-?[0-9\\\\.]+)'},\n",
    "    {'Name': 'valid_acc', 'Regex': '\\'eval_accuracy\\': (-?[0-9\\\\.]+)'},\n",
    "    {'Name': 'gpu_mem', 'Regex': 'GPU Usage.*?(-?[0-9\\\\.]+)% used'}\n",
    "]\n",
    "\n",
    "estimator_parameters = dict(\n",
    "    source_dir         = 'source',\n",
    "    entry_point        = 'train.py',\n",
    "    instance_type      = 'ml.g5.xlarge',\n",
    "    instance_count     = 1,\n",
    "    framework_version  = '2.0',\n",
    "    py_version         = 'py310',\n",
    "    use_spot_instances = True,\n",
    "    max_run            = 24*60*60, # one day in seconds\n",
    "    max_wait           = 24*60*60, \n",
    "    role               = get_execution_role(),\n",
    "    metric_definitions = metric_definitions,\n",
    "    hyperparameters    = hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d340569-5a37-47ed-b075-7bb30299ea57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-09-18-15-23-54-927\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-09-18-15-23-56-404\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-09-18-15-23-58-943\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-09-18-15-24-00-434\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-09-18-15-24-01-863\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-09-18-15-24-03-176\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-09-18-15-24-04-500\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-09-18-15-24-05-911\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "for type in ['lora', 'full']:\n",
    "    for bits in [16, 32]:\n",
    "        for input_scale in [1, 8]:\n",
    "            est = PyTorch(**estimator_parameters)\n",
    "            est.set_hyperparameters(**{'use-bf16':      1    if bits == 16 else 0})\n",
    "            est.set_hyperparameters(**{'use-hf-lora':   1    if type == 'lora' else 0})\n",
    "            est.set_hyperparameters(**{'learning-rate': 4e-4 if type == 'lora' else 5e-5})\n",
    "            est.set_hyperparameters(**{'input-scale': input_scale})\n",
    "            est.fit(wait=False)\n",
    "            estimators.append((type, bits, est, input_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0db83178-bea5-4fc1-9980-a606c013b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_metrics(estimator, metric_names=None):\n",
    "    if isinstance(metric_names, str):\n",
    "        metrics = [metric_names]\n",
    "        \n",
    "    final_metrics = sm.describe_training_job(\n",
    "        TrainingJobName=estimator.latest_training_job.job_name)['FinalMetricDataList']\n",
    "    return {fm['MetricName']: fm['Value'] for fm in final_metrics if metric_names is None or (fm['MetricName'] in metric_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47481cc6-9a49-4cc3-ac2f-1bdec64ecb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>bits</th>\n",
       "      <th>input_scale</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>gpu_memory</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lora</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.936927</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>382.018005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lora</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0.932339</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>136.339996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lora</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.934633</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>449.243988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lora</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>0.931193</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>91.526001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>full</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.918578</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>267.427002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>full</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0.910550</td>\n",
       "      <td>45.799999</td>\n",
       "      <td>111.926003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>full</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.916284</td>\n",
       "      <td>16.799999</td>\n",
       "      <td>262.984009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>full</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>0.917431</td>\n",
       "      <td>79.800003</td>\n",
       "      <td>72.971001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  bits  input_scale  accuracy  gpu_memory  train_samples_per_second\n",
       "0  lora    16            1  0.936927    7.600000                382.018005\n",
       "1  lora    16            8  0.932339   35.000000                136.339996\n",
       "2  lora    32            1  0.934633    7.800000                449.243988\n",
       "3  lora    32            8  0.931193   43.000000                 91.526001\n",
       "4  full    16            1  0.918578   16.900000                267.427002\n",
       "5  full    16            8  0.910550   45.799999                111.926003\n",
       "6  full    32            1  0.916284   16.799999                262.984009\n",
       "7  full    32            8  0.917431   79.800003                 72.971001"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "# capture data\n",
    "for type, bits, est, input_scale in estimators:\n",
    "    job_name = est.latest_training_job.job_name\n",
    "    sm.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\n",
    "    metrics = get_job_metrics(est, ['valid_acc', 'gpu_mem', 'train_samples_per_second'])\n",
    "    results.append(\n",
    "        (type, \n",
    "         bits,\n",
    "         input_scale,\n",
    "         metrics['valid_acc'], \n",
    "         metrics['gpu_mem'],\n",
    "         metrics['train_samples_per_second'])\n",
    "    )\n",
    "results_df = pd.DataFrame(data=results, columns=['type', 'bits', 'input_scale', 'accuracy', 'gpu_memory', 'train_samples_per_second']); results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ee6f617-b88b-474a-be22-7c6e3dc96564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|    | type   |   bits |   input_scale |   accuracy |   gpu_memory |   train_samples_per_second |\\n|---:|:-------|-------:|--------------:|-----------:|-------------:|---------------------------:|\\n|  0 | lora   |     16 |             1 |   0.936927 |          7.6 |                    382.018 |\\n|  1 | lora   |     16 |             8 |   0.932339 |         35   |                    136.34  |\\n|  2 | lora   |     32 |             1 |   0.934633 |          7.8 |                    449.244 |\\n|  3 | lora   |     32 |             8 |   0.931193 |         43   |                     91.526 |\\n|  4 | full   |     16 |             1 |   0.918578 |         16.9 |                    267.427 |\\n|  5 | full   |     16 |             8 |   0.91055  |         45.8 |                    111.926 |\\n|  6 | full   |     32 |             1 |   0.916284 |         16.8 |                    262.984 |\\n|  7 | full   |     32 |             8 |   0.917431 |         79.8 |                     72.971 |'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.to_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab683a06-cc3f-4198-9fa4-77dfed7d2d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---- GPU Memory ----'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'---- Model Performance ----'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'---- Training Speed ----'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for title, field in [\n",
    "    ('GPU Memory', 'gpu_memory'),\n",
    "    ('Model Performance', 'accuracy'),\n",
    "    ('Training Speed', 'train_samples_per_second'),\n",
    "]:\n",
    "    display(f'---- {title} ----')\n",
    "    display(alt.Chart(results_df, title=title, width=400).mark_bar(color='orange').encode(\n",
    "        x='type:N',\n",
    "        y=alt.Y(f'{field}:Q').scale(zero=False),\n",
    "    ).facet(column='bits:O', row='input_scale:O'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4d4980-9a04-433e-966c-08c2b385f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All experiments use the same batch-size\n",
    "# All experiments ran on their own machines/GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c132e86-54da-4a1e-8bb8-0e2572bd46df",
   "metadata": {},
   "source": [
    "## Full Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0998e5d6-3abf-43a0-ab7f-74fac2a925dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "##### Log of pytorch-training-2023-09-18-15-23-54-927 (lora, 16):\n",
      "\n",
      "2023-09-18 15:46:09 Starting - Preparing the instances for training\n",
      "2023-09-18 15:46:09 Downloading - Downloading input data\n",
      "2023-09-18 15:46:09 Training - Training image download completed. Training in progress.\n",
      "2023-09-18 15:46:09 Uploading - Uploading generated training model\n",
      "2023-09-18 15:46:09 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:28,843 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:28,857 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:28,865 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:28,872 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:30,625 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for transformers from https://files.pythonhosted.org/packages/1a/06/3817f9bb923437ead9a794f0ac0d03b8b5e0478ab112db4c413dd37c09da/transformers-4.33.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\u001b[0m\n",
      "\u001b[34m 119.9/119.9 kB 9.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.22.0)\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m 81.4/81.4 kB 23.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for peft from https://files.pythonhosted.org/packages/37/1a/8d20e8704da9fa070eb909265584b960da57be1d833d550c59f50906dc5c/peft-0.5.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl.metadata (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m 53.1/53.1 kB 17.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.15.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for huggingface-hub<1.0,>=0.15.1 from https://files.pythonhosted.org/packages/72/21/51cddb8850ed3f4dbc21e57c3dabc49e64d5577857ddda7b2eb0ffc2ec0e/huggingface_hub-0.17.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.2-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/d1/df/460ca6171a8494fcf37af43f52f6fac23e38784bb4a26563f6fa01ef6faf/regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m 40.9/40.9 kB 10.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m 7.8/7.8 MB 84.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/6c/f0/c17bbdb1e5f9dab29d44cade445135789f75f8f08ea2728d04493ea8412b/safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (13.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.3)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xxhash from https://files.pythonhosted.org/packages/13/c3/e942893f4864a424514c81640f114980cfd5aff7e7414d1e0255f4571111/xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m 114.5/114.5 kB 29.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34m 268.8/268.8 kB 53.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\u001b[0m\n",
      "\u001b[34m 7.6/7.6 MB 116.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.5-py3-none-any.whl (519 kB)\u001b[0m\n",
      "\u001b[34m 519.6/519.6 kB 67.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34m 85.6/85.6 kB 25.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m 1.0/1.0 MB 88.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\u001b[0m\n",
      "\u001b[34m 294.9/294.9 kB 57.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\u001b[0m\n",
      "\u001b[34m 771.9/771.9 kB 83.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m 1.3/1.3 MB 90.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m 194.1/194.1 kB 46.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\u001b[0m\n",
      "\u001b[34m 225.7/225.7 kB 45.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.5 evaluate-0.4.0 frozenlist-1.4.0 huggingface-hub-0.17.2 multidict-6.0.4 peft-0.5.0 pynvml-11.5.0 regex-2023.8.8 responses-0.18.0 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2 xxhash-3.3.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:42,072 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:42,072 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:42,104 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:42,127 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:42,150 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:42,159 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"1\",\n",
      "        \"learning-rate\": \"0.0004\",\n",
      "        \"use-bf16\": \"1\",\n",
      "        \"use-hf-lora\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-09-18-15-23-54-927\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-23-54-927/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"1\",\"learning-rate\":\"0.0004\",\"use-bf16\":\"1\",\"use-hf-lora\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-23-54-927/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"1\",\"learning-rate\":\"0.0004\",\"use-bf16\":\"1\",\"use-hf-lora\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-09-18-15-23-54-927\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-23-54-927/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"1\",\"--learning-rate\",\"0.0004\",\"--use-bf16\",\"1\",\"--use-hf-lora\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-SCALE=1\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.0004\u001b[0m\n",
      "\u001b[34mSM_HP_USE-BF16=1\u001b[0m\n",
      "\u001b[34mSM_HP_USE-HF-LORA=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 1 --learning-rate 0.0004 --use-bf16 1 --use-hf-lora 1\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:42,181 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:__main__:transformers 4.33.2\u001b[0m\n",
      "\u001b[34mINFO:__main__:peft 0.5.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:torch 2.0.1\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 309.562 Total: 23028.000 (1.3% used). Free: 22718.438\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json: 100%|| 481/481 [00:00<00:00, 5.09MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   2%|         | 10.5M/499M [00:00<00:07, 68.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   6%|         | 31.5M/499M [00:00<00:03, 126MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  11%|         | 52.4M/499M [00:00<00:02, 150MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  15%|        | 73.4M/499M [00:00<00:02, 159MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  19%|        | 94.4M/499M [00:00<00:02, 166MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  23%|       | 115M/499M [00:00<00:02, 170MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  27%|       | 136M/499M [00:00<00:02, 173MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  32%|      | 157M/499M [00:00<00:01, 174MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  36%|      | 178M/499M [00:01<00:01, 175MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  40%|      | 199M/499M [00:01<00:01, 177MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  44%|     | 220M/499M [00:01<00:01, 178MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  48%|     | 241M/499M [00:01<00:01, 178MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  53%|    | 262M/499M [00:01<00:01, 179MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  57%|    | 283M/499M [00:01<00:01, 179MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  61%|    | 304M/499M [00:01<00:01, 179MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  65%|   | 325M/499M [00:01<00:00, 180MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  69%|   | 346M/499M [00:02<00:00, 180MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  74%|  | 367M/499M [00:02<00:00, 179MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  78%|  | 388M/499M [00:02<00:00, 179MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  82%| | 409M/499M [00:02<00:00, 179MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  86%| | 430M/499M [00:02<00:00, 179MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  90%| | 451M/499M [00:02<00:00, 179MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  95%|| 472M/499M [00:02<00:00, 179MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  99%|| 493M/499M [00:02<00:00, 179MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|| 499M/499M [00:02<00:00, 173MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json: 100%|| 899k/899k [00:00<00:00, 2.76MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json: 100%|| 899k/899k [00:00<00:00, 2.75MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt: 100%|| 456k/456k [00:00<00:00, 1.89MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt: 100%|| 456k/456k [00:00<00:00, 1.89MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json: 100%|| 1.36M/1.36M [00:00<00:00, 8.15MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json: 100%|| 1.36M/1.36M [00:00<00:00, 8.12MB/s]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:Parameters (name, tunable, count):\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.word_embeddings.weight                               0    38603520\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.position_embeddings.weight                           0      394752\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.token_type_embeddings.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.weight                                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.bias                                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.dense.weight                                 1      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.dense.bias                                   1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.weight                              1        1536\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.bias                                1           2\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.weight                         1      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.bias                           1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.weight                      1        1536\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.bias                        1           2\u001b[0m\n",
      "\u001b[34mINFO:__main__:Total parameters: 125,276,164, thereof learnable: 1,221,124 (0.9747%)\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average train input length: 14.36\u001b[0m\n",
      "\u001b[34mUsing bf16: True, LoRA: True\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1690.500 Total: 23028.000 (7.3% used). Free: 21337.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1730.500 Total: 23028.000 (7.5% used). Free: 21297.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.2943, 'learning_rate': 0.0003600190023752969, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.2413, 'learning_rate': 0.0003200380047505938, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|| 4.20k/4.20k [00:00<00:00, 6.23MB/s]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.25038424134254456, 'eval_accuracy': 0.9311926605504587, 'eval_runtime': 1.5064, 'eval_samples_per_second': 578.882, 'eval_steps_per_second': 36.512, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.2176, 'learning_rate': 0.00028005700712589076, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.2153, 'learning_rate': 0.00024007600950118766, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.20365166664123535, 'eval_accuracy': 0.9369266055045872, 'eval_runtime': 1.5377, 'eval_samples_per_second': 567.098, 'eval_steps_per_second': 35.769, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1973, 'learning_rate': 0.00020009501187648457, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1962, 'learning_rate': 0.00016011401425178147, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2219955325126648, 'eval_accuracy': 0.9334862385321101, 'eval_runtime': 1.7509, 'eval_samples_per_second': 498.032, 'eval_steps_per_second': 31.413, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1881, 'learning_rate': 0.00012013301662707839, 'epoch': 3.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1836, 'learning_rate': 8.01520190023753e-05, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2549245357513428, 'eval_accuracy': 0.9357798165137615, 'eval_runtime': 1.3518, 'eval_samples_per_second': 645.084, 'eval_steps_per_second': 40.688, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1769, 'learning_rate': 4.0171021377672214e-05, 'epoch': 4.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1752.500 Total: 23028.000 (7.6% used). Free: 21275.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1751, 'learning_rate': 1.9002375296912116e-07, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.25381696224212646, 'eval_accuracy': 0.9323394495412844, 'eval_runtime': 1.3648, 'eval_samples_per_second': 638.904, 'eval_steps_per_second': 40.298, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 881.489, 'train_samples_per_second': 382.018, 'train_steps_per_second': 23.88, 'train_loss': 0.2085630615363495, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.20365166664123535, 'eval_accuracy': 0.9369266055045872, 'eval_runtime': 1.3465, 'eval_samples_per_second': 647.583, 'eval_steps_per_second': 40.845, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m2023-09-18 15:45:56,315 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:45:56,315 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:45:56,316 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 1261\n",
      "Billable seconds: 379\n",
      "Managed Spot Training savings: 69.9%\n",
      "\n",
      "\n",
      "\n",
      "##### Log of pytorch-training-2023-09-18-15-23-56-404 (lora, 16):\n",
      "\n",
      "2023-09-18 16:13:15 Starting - Preparing the instances for training\n",
      "2023-09-18 16:13:15 Downloading - Downloading input data\n",
      "2023-09-18 16:13:15 Training - Training image download completed. Training in progress.\n",
      "2023-09-18 16:13:15 Uploading - Uploading generated training model\n",
      "2023-09-18 16:13:15 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:32,359 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:32,372 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:32,381 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:32,387 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:34,090 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for transformers from https://files.pythonhosted.org/packages/1a/06/3817f9bb923437ead9a794f0ac0d03b8b5e0478ab112db4c413dd37c09da/transformers-4.33.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\u001b[0m\n",
      "\u001b[34m 119.9/119.9 kB 8.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.22.0)\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m 81.4/81.4 kB 19.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for peft from https://files.pythonhosted.org/packages/37/1a/8d20e8704da9fa070eb909265584b960da57be1d833d550c59f50906dc5c/peft-0.5.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl.metadata (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m 53.1/53.1 kB 15.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.15.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for huggingface-hub<1.0,>=0.15.1 from https://files.pythonhosted.org/packages/72/21/51cddb8850ed3f4dbc21e57c3dabc49e64d5577857ddda7b2eb0ffc2ec0e/huggingface_hub-0.17.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.2-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/d1/df/460ca6171a8494fcf37af43f52f6fac23e38784bb4a26563f6fa01ef6faf/regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m 40.9/40.9 kB 11.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m 7.8/7.8 MB 85.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/6c/f0/c17bbdb1e5f9dab29d44cade445135789f75f8f08ea2728d04493ea8412b/safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (13.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.3)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xxhash from https://files.pythonhosted.org/packages/13/c3/e942893f4864a424514c81640f114980cfd5aff7e7414d1e0255f4571111/xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m 114.5/114.5 kB 32.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34m 268.8/268.8 kB 58.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\u001b[0m\n",
      "\u001b[34m 7.6/7.6 MB 126.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.5-py3-none-any.whl (519 kB)\u001b[0m\n",
      "\u001b[34m 519.6/519.6 kB 61.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34m 85.6/85.6 kB 27.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m 1.0/1.0 MB 95.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\u001b[0m\n",
      "\u001b[34m 294.9/294.9 kB 54.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\u001b[0m\n",
      "\u001b[34m 771.9/771.9 kB 81.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m 1.3/1.3 MB 97.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m 194.1/194.1 kB 48.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\u001b[0m\n",
      "\u001b[34m 225.7/225.7 kB 55.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.5 evaluate-0.4.0 frozenlist-1.4.0 huggingface-hub-0.17.2 multidict-6.0.4 peft-0.5.0 pynvml-11.5.0 regex-2023.8.8 responses-0.18.0 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2 xxhash-3.3.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:45,675 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:45,675 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:45,709 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:45,732 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:45,755 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:45,765 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"8\",\n",
      "        \"learning-rate\": \"0.0004\",\n",
      "        \"use-bf16\": \"1\",\n",
      "        \"use-hf-lora\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-09-18-15-23-56-404\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-23-56-404/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"0.0004\",\"use-bf16\":\"1\",\"use-hf-lora\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-23-56-404/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"0.0004\",\"use-bf16\":\"1\",\"use-hf-lora\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-09-18-15-23-56-404\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-23-56-404/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"8\",\"--learning-rate\",\"0.0004\",\"--use-bf16\",\"1\",\"--use-hf-lora\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-SCALE=8\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.0004\u001b[0m\n",
      "\u001b[34mSM_HP_USE-BF16=1\u001b[0m\n",
      "\u001b[34mSM_HP_USE-HF-LORA=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 8 --learning-rate 0.0004 --use-bf16 1 --use-hf-lora 1\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:45,787 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:__main__:transformers 4.33.2\u001b[0m\n",
      "\u001b[34mINFO:__main__:peft 0.5.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:torch 2.0.1\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 309.562 Total: 23028.000 (1.3% used). Free: 22718.438\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json: 100%|| 481/481 [00:00<00:00, 4.37MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   2%|         | 10.5M/499M [00:00<00:18, 26.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   4%|         | 21.0M/499M [00:00<00:17, 27.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   6%|         | 31.5M/499M [00:01<00:14, 31.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   8%|         | 41.9M/499M [00:01<00:14, 32.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  11%|         | 52.4M/499M [00:01<00:12, 35.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  13%|        | 62.9M/499M [00:01<00:11, 38.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  15%|        | 73.4M/499M [00:02<00:11, 36.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  17%|        | 83.9M/499M [00:02<00:11, 36.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  19%|        | 94.4M/499M [00:02<00:12, 33.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  21%|        | 105M/499M [00:03<00:11, 33.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  23%|       | 115M/499M [00:03<00:11, 33.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  25%|       | 126M/499M [00:03<00:11, 33.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  27%|       | 136M/499M [00:03<00:10, 35.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  29%|       | 147M/499M [00:04<00:09, 35.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  32%|      | 157M/499M [00:04<00:09, 36.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  34%|      | 168M/499M [00:04<00:08, 37.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  36%|      | 178M/499M [00:05<00:08, 35.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  38%|      | 189M/499M [00:05<00:08, 35.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  40%|      | 199M/499M [00:05<00:08, 34.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  42%|     | 210M/499M [00:06<00:08, 36.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  44%|     | 220M/499M [00:06<00:07, 37.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  46%|     | 231M/499M [00:06<00:07, 37.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  48%|     | 241M/499M [00:06<00:06, 39.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  50%|     | 252M/499M [00:06<00:05, 42.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  53%|    | 262M/499M [00:07<00:05, 43.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  55%|    | 273M/499M [00:07<00:05, 41.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  57%|    | 283M/499M [00:07<00:05, 42.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  59%|    | 294M/499M [00:08<00:05, 40.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  61%|    | 304M/499M [00:08<00:04, 41.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  63%|   | 315M/499M [00:08<00:04, 41.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  65%|   | 325M/499M [00:08<00:04, 41.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  67%|   | 336M/499M [00:09<00:03, 41.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  69%|   | 346M/499M [00:09<00:03, 43.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  71%|  | 357M/499M [00:09<00:03, 45.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  74%|  | 367M/499M [00:09<00:02, 44.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  76%|  | 377M/499M [00:09<00:02, 43.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  78%|  | 388M/499M [00:10<00:02, 42.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  80%|  | 398M/499M [00:10<00:02, 37.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  82%| | 409M/499M [00:10<00:02, 33.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  84%| | 419M/499M [00:11<00:02, 37.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  86%| | 430M/499M [00:11<00:01, 36.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  88%| | 440M/499M [00:11<00:01, 35.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  90%| | 451M/499M [00:12<00:01, 35.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  92%|| 461M/499M [00:12<00:00, 37.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  95%|| 472M/499M [00:12<00:00, 38.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  97%|| 482M/499M [00:12<00:00, 38.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  99%|| 493M/499M [00:13<00:00, 38.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|| 499M/499M [00:13<00:00, 36.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|| 499M/499M [00:13<00:00, 37.5MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json: 100%|| 899k/899k [00:00<00:00, 5.70MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json: 100%|| 899k/899k [00:00<00:00, 5.68MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt: 100%|| 456k/456k [00:00<00:00, 2.80MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt: 100%|| 456k/456k [00:00<00:00, 2.79MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json: 100%|| 1.36M/1.36M [00:00<00:00, 13.8MB/s]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:Parameters (name, tunable, count):\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.word_embeddings.weight                               0    38603520\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.position_embeddings.weight                           0      394752\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.token_type_embeddings.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.weight                                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.bias                                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.dense.weight                                 1      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.dense.bias                                   1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.weight                              1        1536\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.bias                                1           2\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.weight                         1      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.bias                           1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.weight                      1        1536\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.bias                        1           2\u001b[0m\n",
      "\u001b[34mINFO:__main__:Total parameters: 125,276,164, thereof learnable: 1,221,124 (0.9747%)\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 309.562 Total: 23028.000 (1.3% used). Free: 22718.438\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average train input length: 92.13\u001b[0m\n",
      "\u001b[34mUsing bf16: True, LoRA: True\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8134.500 Total: 23028.000 (35.3% used). Free: 14893.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 12394.500 Total: 23028.000 (53.8% used). Free: 10633.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 14734.500 Total: 23028.000 (64.0% used). Free: 8293.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22566.500 Total: 23028.000 (98.0% used). Free: 461.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 7960.500 Total: 23028.000 (34.6% used). Free: 15067.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 7960.500 Total: 23028.000 (34.6% used). Free: 15067.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 7960.500 Total: 23028.000 (34.6% used). Free: 15067.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 7960.500 Total: 23028.000 (34.6% used). Free: 15067.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.2586, 'learning_rate': 0.0003600190023752969, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 7982.500 Total: 23028.000 (34.7% used). Free: 15045.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 7982.500 Total: 23028.000 (34.7% used). Free: 15045.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 7982.500 Total: 23028.000 (34.7% used). Free: 15045.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 7982.500 Total: 23028.000 (34.7% used). Free: 15045.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 7982.500 Total: 23028.000 (34.7% used). Free: 15045.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 7982.500 Total: 23028.000 (34.7% used). Free: 15045.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 7982.500 Total: 23028.000 (34.7% used). Free: 15045.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 7982.500 Total: 23028.000 (34.7% used). Free: 15045.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 7982.500 Total: 23028.000 (34.7% used). Free: 15045.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.2049, 'learning_rate': 0.0003200380047505938, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|| 4.20k/4.20k [00:00<00:00, 5.52MB/s]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2522701621055603, 'eval_accuracy': 0.9254587155963303, 'eval_runtime': 3.9748, 'eval_samples_per_second': 219.379, 'eval_steps_per_second': 13.837, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8000.500 Total: 23028.000 (34.7% used). Free: 15027.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8000.500 Total: 23028.000 (34.7% used). Free: 15027.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8000.500 Total: 23028.000 (34.7% used). Free: 15027.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8000.500 Total: 23028.000 (34.7% used). Free: 15027.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8000.500 Total: 23028.000 (34.7% used). Free: 15027.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8000.500 Total: 23028.000 (34.7% used). Free: 15027.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8000.500 Total: 23028.000 (34.7% used). Free: 15027.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8000.500 Total: 23028.000 (34.7% used). Free: 15027.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1779, 'learning_rate': 0.00028005700712589076, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8000.500 Total: 23028.000 (34.7% used). Free: 15027.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8000.500 Total: 23028.000 (34.7% used). Free: 15027.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8000.500 Total: 23028.000 (34.7% used). Free: 15027.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8000.500 Total: 23028.000 (34.7% used). Free: 15027.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8000.500 Total: 23028.000 (34.7% used). Free: 15027.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8000.500 Total: 23028.000 (34.7% used). Free: 15027.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8000.500 Total: 23028.000 (34.7% used). Free: 15027.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8000.500 Total: 23028.000 (34.7% used). Free: 15027.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.176, 'learning_rate': 0.00024007600950118766, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.24511310458183289, 'eval_accuracy': 0.9254587155963303, 'eval_runtime': 4.0129, 'eval_samples_per_second': 217.299, 'eval_steps_per_second': 13.706, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1519, 'learning_rate': 0.00020009501187648457, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1561, 'learning_rate': 0.00016011401425178147, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3108842372894287, 'eval_accuracy': 0.9254587155963303, 'eval_runtime': 3.7563, 'eval_samples_per_second': 232.146, 'eval_steps_per_second': 14.642, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1373, 'learning_rate': 0.00012013301662707839, 'epoch': 3.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1393, 'learning_rate': 8.01520190023753e-05, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3073118031024933, 'eval_accuracy': 0.9311926605504587, 'eval_runtime': 3.7427, 'eval_samples_per_second': 232.987, 'eval_steps_per_second': 14.695, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1248, 'learning_rate': 4.0171021377672214e-05, 'epoch': 4.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1202, 'learning_rate': 1.9002375296912116e-07, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8048.500 Total: 23028.000 (35.0% used). Free: 14979.500\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2921547293663025, 'eval_accuracy': 0.9323394495412844, 'eval_runtime': 3.8068, 'eval_samples_per_second': 229.063, 'eval_steps_per_second': 14.448, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 2469.8926, 'train_samples_per_second': 136.34, 'train_steps_per_second': 8.523, 'train_loss': 0.1646697431873539, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2921547293663025, 'eval_accuracy': 0.9323394495412844, 'eval_runtime': 3.7511, 'eval_samples_per_second': 232.462, 'eval_steps_per_second': 14.662, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m2023-09-18 16:12:57,788 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-18 16:12:57,788 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-18 16:12:57,789 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 2888\n",
      "Billable seconds: 868\n",
      "Managed Spot Training savings: 69.9%\n",
      "\n",
      "\n",
      "\n",
      "##### Log of pytorch-training-2023-09-18-15-23-58-943 (lora, 32):\n",
      "\n",
      "2023-09-18 15:44:14 Starting - Preparing the instances for training\n",
      "2023-09-18 15:44:14 Downloading - Downloading input data\n",
      "2023-09-18 15:44:14 Training - Training image download completed. Training in progress.\n",
      "2023-09-18 15:44:14 Uploading - Uploading generated training model\n",
      "2023-09-18 15:44:14 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:37,113 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:37,126 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:37,135 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:37,143 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:38,900 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for transformers from https://files.pythonhosted.org/packages/1a/06/3817f9bb923437ead9a794f0ac0d03b8b5e0478ab112db4c413dd37c09da/transformers-4.33.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\u001b[0m\n",
      "\u001b[34m 119.9/119.9 kB 6.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.22.0)\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m 81.4/81.4 kB 18.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for peft from https://files.pythonhosted.org/packages/37/1a/8d20e8704da9fa070eb909265584b960da57be1d833d550c59f50906dc5c/peft-0.5.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl.metadata (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m 53.1/53.1 kB 13.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.15.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for huggingface-hub<1.0,>=0.15.1 from https://files.pythonhosted.org/packages/72/21/51cddb8850ed3f4dbc21e57c3dabc49e64d5577857ddda7b2eb0ffc2ec0e/huggingface_hub-0.17.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.2-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/d1/df/460ca6171a8494fcf37af43f52f6fac23e38784bb4a26563f6fa01ef6faf/regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m 40.9/40.9 kB 11.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m 7.8/7.8 MB 70.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/6c/f0/c17bbdb1e5f9dab29d44cade445135789f75f8f08ea2728d04493ea8412b/safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (13.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.3)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xxhash from https://files.pythonhosted.org/packages/13/c3/e942893f4864a424514c81640f114980cfd5aff7e7414d1e0255f4571111/xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m 114.5/114.5 kB 30.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34m 268.8/268.8 kB 45.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\u001b[0m\n",
      "\u001b[34m 7.6/7.6 MB 108.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.5-py3-none-any.whl (519 kB)\u001b[0m\n",
      "\u001b[34m 519.6/519.6 kB 57.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34m 85.6/85.6 kB 20.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m 1.0/1.0 MB 84.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\u001b[0m\n",
      "\u001b[34m 294.9/294.9 kB 54.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\u001b[0m\n",
      "\u001b[34m 771.9/771.9 kB 71.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m 1.3/1.3 MB 79.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m 194.1/194.1 kB 36.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\u001b[0m\n",
      "\u001b[34m 225.7/225.7 kB 52.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.5 evaluate-0.4.0 frozenlist-1.4.0 huggingface-hub-0.17.2 multidict-6.0.4 peft-0.5.0 pynvml-11.5.0 regex-2023.8.8 responses-0.18.0 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2 xxhash-3.3.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:50,558 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:50,558 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:50,594 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:50,617 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:50,640 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:50,650 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"1\",\n",
      "        \"learning-rate\": \"0.0004\",\n",
      "        \"use-bf16\": \"0\",\n",
      "        \"use-hf-lora\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-09-18-15-23-58-943\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-23-58-943/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"1\",\"learning-rate\":\"0.0004\",\"use-bf16\":\"0\",\"use-hf-lora\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-23-58-943/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"1\",\"learning-rate\":\"0.0004\",\"use-bf16\":\"0\",\"use-hf-lora\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-09-18-15-23-58-943\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-23-58-943/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"1\",\"--learning-rate\",\"0.0004\",\"--use-bf16\",\"0\",\"--use-hf-lora\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-SCALE=1\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.0004\u001b[0m\n",
      "\u001b[34mSM_HP_USE-BF16=0\u001b[0m\n",
      "\u001b[34mSM_HP_USE-HF-LORA=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 1 --learning-rate 0.0004 --use-bf16 0 --use-hf-lora 1\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:50,672 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:__main__:transformers 4.33.2\u001b[0m\n",
      "\u001b[34mINFO:__main__:peft 0.5.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:torch 2.0.1\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 309.562 Total: 23028.000 (1.3% used). Free: 22718.438\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json: 100%|| 481/481 [00:00<00:00, 4.24MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   4%|         | 21.0M/499M [00:00<00:03, 147MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   8%|         | 41.9M/499M [00:00<00:03, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  13%|        | 62.9M/499M [00:00<00:04, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  17%|        | 83.9M/499M [00:00<00:03, 123MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  21%|        | 105M/499M [00:00<00:02, 136MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  25%|       | 126M/499M [00:00<00:02, 141MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  29%|       | 147M/499M [00:01<00:02, 149MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  34%|      | 168M/499M [00:01<00:02, 154MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  38%|      | 189M/499M [00:01<00:02, 150MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  42%|     | 210M/499M [00:01<00:02, 132MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  48%|     | 241M/499M [00:01<00:02, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  53%|    | 262M/499M [00:02<00:02, 86.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  55%|    | 273M/499M [00:02<00:03, 68.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  57%|    | 283M/499M [00:02<00:03, 62.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  59%|    | 294M/499M [00:03<00:03, 52.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  61%|    | 304M/499M [00:03<00:03, 53.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  63%|   | 315M/499M [00:03<00:04, 44.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  65%|   | 325M/499M [00:03<00:03, 49.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  67%|   | 336M/499M [00:04<00:03, 42.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  69%|   | 346M/499M [00:04<00:03, 48.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  71%|  | 357M/499M [00:04<00:02, 48.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  74%|  | 367M/499M [00:04<00:02, 46.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  76%|  | 377M/499M [00:05<00:02, 45.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  78%|  | 388M/499M [00:05<00:02, 44.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  80%|  | 398M/499M [00:05<00:02, 39.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  82%| | 409M/499M [00:05<00:02, 36.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  84%| | 419M/499M [00:06<00:02, 37.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  86%| | 430M/499M [00:06<00:01, 36.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  88%| | 440M/499M [00:06<00:01, 34.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  90%| | 451M/499M [00:07<00:01, 36.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  92%|| 461M/499M [00:07<00:00, 37.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  95%|| 472M/499M [00:07<00:00, 38.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  97%|| 482M/499M [00:07<00:00, 38.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  99%|| 493M/499M [00:08<00:00, 38.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|| 499M/499M [00:08<00:00, 35.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|| 499M/499M [00:08<00:00, 59.4MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json: 100%|| 899k/899k [00:00<00:00, 3.30MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json: 100%|| 899k/899k [00:00<00:00, 3.29MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt: 100%|| 456k/456k [00:00<00:00, 57.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json: 100%|| 1.36M/1.36M [00:00<00:00, 4.00MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json: 100%|| 1.36M/1.36M [00:00<00:00, 3.99MB/s]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:Parameters (name, tunable, count):\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.word_embeddings.weight                               0    38603520\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.position_embeddings.weight                           0      394752\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.token_type_embeddings.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.weight                                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.bias                                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.dense.weight                                 1      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.dense.bias                                   1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.weight                              1        1536\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.bias                                1           2\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.weight                         1      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.bias                           1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.weight                      1        1536\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.bias                        1           2\u001b[0m\n",
      "\u001b[34mINFO:__main__:Total parameters: 125,276,164, thereof learnable: 1,221,124 (0.9747%)\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 309.562 Total: 23028.000 (1.3% used). Free: 22718.438\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average train input length: 14.36\u001b[0m\n",
      "\u001b[34mUsing bf16: False, LoRA: True\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1792.500 Total: 23028.000 (7.8% used). Free: 21235.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.2948, 'learning_rate': 0.0003600190023752969, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.2431, 'learning_rate': 0.0003200380047505938, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|| 4.20k/4.20k [00:00<00:00, 4.32MB/s]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2864401638507843, 'eval_accuracy': 0.9323394495412844, 'eval_runtime': 1.5567, 'eval_samples_per_second': 560.166, 'eval_steps_per_second': 35.332, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.2204, 'learning_rate': 0.00028005700712589076, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.217, 'learning_rate': 0.00024007600950118766, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2262689769268036, 'eval_accuracy': 0.9311926605504587, 'eval_runtime': 1.1606, 'eval_samples_per_second': 751.347, 'eval_steps_per_second': 47.39, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.199, 'learning_rate': 0.00020009501187648457, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1977, 'learning_rate': 0.00016011401425178147, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.24235907196998596, 'eval_accuracy': 0.9346330275229358, 'eval_runtime': 1.2129, 'eval_samples_per_second': 718.94, 'eval_steps_per_second': 45.346, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1873, 'learning_rate': 0.00012013301662707839, 'epoch': 3.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1862, 'learning_rate': 8.01520190023753e-05, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.25079047679901123, 'eval_accuracy': 0.9346330275229358, 'eval_runtime': 1.1503, 'eval_samples_per_second': 758.037, 'eval_steps_per_second': 47.812, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1773, 'learning_rate': 4.0171021377672214e-05, 'epoch': 4.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1744, 'learning_rate': 1.9002375296912116e-07, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 1806.500 Total: 23028.000 (7.8% used). Free: 21221.500\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2521608769893646, 'eval_accuracy': 0.9323394495412844, 'eval_runtime': 1.1599, 'eval_samples_per_second': 751.767, 'eval_steps_per_second': 47.416, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 749.5809, 'train_samples_per_second': 449.244, 'train_steps_per_second': 28.082, 'train_loss': 0.20969432459307963, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.24235907196998596, 'eval_accuracy': 0.9346330275229358, 'eval_runtime': 1.1472, 'eval_samples_per_second': 760.121, 'eval_steps_per_second': 47.943, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m2023-09-18 15:43:57,428 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:43:57,428 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:43:57,428 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 1141\n",
      "Billable seconds: 343\n",
      "Managed Spot Training savings: 69.9%\n",
      "\n",
      "\n",
      "\n",
      "##### Log of pytorch-training-2023-09-18-15-24-00-434 (lora, 32):\n",
      "\n",
      "2023-09-18 16:33:29 Starting - Preparing the instances for training\n",
      "2023-09-18 16:33:29 Downloading - Downloading input data\n",
      "2023-09-18 16:33:29 Training - Training image download completed. Training in progress.\n",
      "2023-09-18 16:33:29 Uploading - Uploading generated training model\n",
      "2023-09-18 16:33:29 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:35,260 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:35,273 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:35,282 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:35,288 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:37,003 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for transformers from https://files.pythonhosted.org/packages/1a/06/3817f9bb923437ead9a794f0ac0d03b8b5e0478ab112db4c413dd37c09da/transformers-4.33.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\u001b[0m\n",
      "\u001b[34m 119.9/119.9 kB 10.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.22.0)\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m 81.4/81.4 kB 27.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for peft from https://files.pythonhosted.org/packages/37/1a/8d20e8704da9fa070eb909265584b960da57be1d833d550c59f50906dc5c/peft-0.5.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl.metadata (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m 53.1/53.1 kB 17.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.15.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for huggingface-hub<1.0,>=0.15.1 from https://files.pythonhosted.org/packages/72/21/51cddb8850ed3f4dbc21e57c3dabc49e64d5577857ddda7b2eb0ffc2ec0e/huggingface_hub-0.17.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.2-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/d1/df/460ca6171a8494fcf37af43f52f6fac23e38784bb4a26563f6fa01ef6faf/regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m 40.9/40.9 kB 11.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m 7.8/7.8 MB 112.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/6c/f0/c17bbdb1e5f9dab29d44cade445135789f75f8f08ea2728d04493ea8412b/safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (13.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.3)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xxhash from https://files.pythonhosted.org/packages/13/c3/e942893f4864a424514c81640f114980cfd5aff7e7414d1e0255f4571111/xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m 114.5/114.5 kB 27.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34m 268.8/268.8 kB 46.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\u001b[0m\n",
      "\u001b[34m 7.6/7.6 MB 123.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.5-py3-none-any.whl (519 kB)\u001b[0m\n",
      "\u001b[34m 519.6/519.6 kB 58.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34m 85.6/85.6 kB 23.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m 1.0/1.0 MB 83.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\u001b[0m\n",
      "\u001b[34m 294.9/294.9 kB 53.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\u001b[0m\n",
      "\u001b[34m 771.9/771.9 kB 79.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m 1.3/1.3 MB 100.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m 194.1/194.1 kB 35.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\u001b[0m\n",
      "\u001b[34m 225.7/225.7 kB 47.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.5 evaluate-0.4.0 frozenlist-1.4.0 huggingface-hub-0.17.2 multidict-6.0.4 peft-0.5.0 pynvml-11.5.0 regex-2023.8.8 responses-0.18.0 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2 xxhash-3.3.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:48,571 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:48,571 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:48,607 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:48,629 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:48,652 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:48,661 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"8\",\n",
      "        \"learning-rate\": \"0.0004\",\n",
      "        \"use-bf16\": \"0\",\n",
      "        \"use-hf-lora\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-09-18-15-24-00-434\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-24-00-434/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"0.0004\",\"use-bf16\":\"0\",\"use-hf-lora\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-24-00-434/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"0.0004\",\"use-bf16\":\"0\",\"use-hf-lora\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-09-18-15-24-00-434\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-24-00-434/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"8\",\"--learning-rate\",\"0.0004\",\"--use-bf16\",\"0\",\"--use-hf-lora\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-SCALE=8\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.0004\u001b[0m\n",
      "\u001b[34mSM_HP_USE-BF16=0\u001b[0m\n",
      "\u001b[34mSM_HP_USE-HF-LORA=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 8 --learning-rate 0.0004 --use-bf16 0 --use-hf-lora 1\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:48,684 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:__main__:transformers 4.33.2\u001b[0m\n",
      "\u001b[34mINFO:__main__:peft 0.5.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:torch 2.0.1\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 309.562 Total: 23028.000 (1.3% used). Free: 22718.438\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json: 100%|| 481/481 [00:00<00:00, 3.05MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   2%|         | 10.5M/499M [00:00<00:12, 37.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   6%|         | 31.5M/499M [00:00<00:05, 85.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  11%|         | 52.4M/499M [00:00<00:03, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  15%|        | 73.4M/499M [00:00<00:03, 119MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  19%|        | 94.4M/499M [00:00<00:02, 141MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  25%|       | 126M/499M [00:00<00:02, 167MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  29%|       | 147M/499M [00:01<00:04, 87.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  34%|      | 168M/499M [00:02<00:05, 63.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  36%|      | 178M/499M [00:02<00:05, 54.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  38%|      | 189M/499M [00:02<00:06, 49.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  40%|      | 199M/499M [00:02<00:06, 44.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  42%|     | 210M/499M [00:03<00:06, 44.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  44%|     | 220M/499M [00:03<00:06, 42.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  46%|     | 231M/499M [00:03<00:06, 40.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  48%|     | 241M/499M [00:03<00:06, 42.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  50%|     | 252M/499M [00:04<00:05, 44.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  53%|    | 262M/499M [00:04<00:05, 45.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  55%|    | 273M/499M [00:04<00:05, 42.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  57%|    | 283M/499M [00:04<00:04, 43.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  59%|    | 294M/499M [00:05<00:05, 40.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  61%|    | 304M/499M [00:05<00:04, 41.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  63%|   | 315M/499M [00:05<00:04, 41.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  65%|   | 325M/499M [00:05<00:04, 41.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  67%|   | 336M/499M [00:06<00:03, 43.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  69%|   | 346M/499M [00:06<00:03, 42.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  71%|  | 357M/499M [00:06<00:03, 43.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  74%|  | 367M/499M [00:06<00:03, 43.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  76%|  | 377M/499M [00:07<00:02, 43.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  78%|  | 388M/499M [00:07<00:02, 39.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  80%|  | 398M/499M [00:07<00:02, 38.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  82%| | 409M/499M [00:08<00:02, 36.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  84%| | 419M/499M [00:08<00:02, 37.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  86%| | 430M/499M [00:08<00:01, 35.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  88%| | 440M/499M [00:08<00:01, 35.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  90%| | 451M/499M [00:09<00:01, 36.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  92%|| 461M/499M [00:09<00:01, 37.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  95%|| 472M/499M [00:09<00:00, 38.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  97%|| 482M/499M [00:10<00:00, 38.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  99%|| 493M/499M [00:10<00:00, 34.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|| 499M/499M [00:10<00:00, 47.6MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json: 100%|| 899k/899k [00:00<00:00, 9.91MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt: 100%|| 456k/456k [00:00<00:00, 3.49MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt: 100%|| 456k/456k [00:00<00:00, 3.48MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json: 100%|| 1.36M/1.36M [00:00<00:00, 15.9MB/s]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:Parameters (name, tunable, count):\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.word_embeddings.weight                               0    38603520\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.position_embeddings.weight                           0      394752\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.token_type_embeddings.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.weight                                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.embeddings.LayerNorm.bias                                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.0.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.1.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.2.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.3.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.4.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.5.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.6.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.7.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.8.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.weight                       0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.key.bias                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.weight                     0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.bias                       0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight      1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.weight                   0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.dense.bias                     0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.weight               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.bias                 0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.weight                       0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.intermediate.dense.bias                         0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.weight                             0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.dense.bias                               0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.weight                         0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.9.output.LayerNorm.bias                           0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.10.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.weight                      0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.key.bias                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.weight                    0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.bias                      0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight     1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.weight                  0      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.dense.bias                    0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.weight              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.bias                0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.weight                      0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.intermediate.dense.bias                        0        3072\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.weight                            0     2359296\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.dense.bias                              0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.weight                        0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.roberta.encoder.layer.11.output.LayerNorm.bias                          0         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.dense.weight                                 1      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.dense.bias                                   1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.weight                              1        1536\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.original_module.out_proj.bias                                1           2\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.weight                         1      589824\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.dense.bias                           1         768\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.weight                      1        1536\u001b[0m\n",
      "\u001b[34mDEBUG:__main__: base_model.model.classifier.modules_to_save.default.out_proj.bias                        1           2\u001b[0m\n",
      "\u001b[34mINFO:__main__:Total parameters: 125,276,164, thereof learnable: 1,221,124 (0.9747%)\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 309.562 Total: 23028.000 (1.3% used). Free: 22718.438\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average train input length: 92.13\u001b[0m\n",
      "\u001b[34mUsing bf16: False, LoRA: True\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13814.500 Total: 23028.000 (60.0% used). Free: 9213.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21198.500 Total: 23028.000 (92.1% used). Free: 1829.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21198.500 Total: 23028.000 (92.1% used). Free: 1829.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9192.500 Total: 23028.000 (39.9% used). Free: 13835.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13404.500 Total: 23028.000 (58.2% used). Free: 9623.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 22764.500 Total: 23028.000 (98.9% used). Free: 263.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9812.500 Total: 23028.000 (42.6% used). Free: 13215.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9812.500 Total: 23028.000 (42.6% used). Free: 13215.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9812.500 Total: 23028.000 (42.6% used). Free: 13215.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9812.500 Total: 23028.000 (42.6% used). Free: 13215.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9812.500 Total: 23028.000 (42.6% used). Free: 13215.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9832.500 Total: 23028.000 (42.7% used). Free: 13195.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.2566, 'learning_rate': 0.0003600190023752969, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9832.500 Total: 23028.000 (42.7% used). Free: 13195.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9832.500 Total: 23028.000 (42.7% used). Free: 13195.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9832.500 Total: 23028.000 (42.7% used). Free: 13195.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9832.500 Total: 23028.000 (42.7% used). Free: 13195.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9832.500 Total: 23028.000 (42.7% used). Free: 13195.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9832.500 Total: 23028.000 (42.7% used). Free: 13195.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9832.500 Total: 23028.000 (42.7% used). Free: 13195.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9832.500 Total: 23028.000 (42.7% used). Free: 13195.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9832.500 Total: 23028.000 (42.7% used). Free: 13195.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9832.500 Total: 23028.000 (42.7% used). Free: 13195.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9832.500 Total: 23028.000 (42.7% used). Free: 13195.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9832.500 Total: 23028.000 (42.7% used). Free: 13195.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9832.500 Total: 23028.000 (42.7% used). Free: 13195.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.2061, 'learning_rate': 0.0003200380047505938, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|| 4.20k/4.20k [00:00<00:00, 4.28MB/s]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.263653427362442, 'eval_accuracy': 0.9254587155963303, 'eval_runtime': 5.8556, 'eval_samples_per_second': 148.917, 'eval_steps_per_second': 9.393, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1766, 'learning_rate': 0.00028005700712589076, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1757, 'learning_rate': 0.00024007600950118766, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.25134772062301636, 'eval_accuracy': 0.9288990825688074, 'eval_runtime': 5.7553, 'eval_samples_per_second': 151.514, 'eval_steps_per_second': 9.556, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1523, 'learning_rate': 0.00020009501187648457, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1547, 'learning_rate': 0.00016011401425178147, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.29935774207115173, 'eval_accuracy': 0.9243119266055045, 'eval_runtime': 5.7454, 'eval_samples_per_second': 151.773, 'eval_steps_per_second': 9.573, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1375, 'learning_rate': 0.00012013301662707839, 'epoch': 3.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9860.500 Total: 23028.000 (42.8% used). Free: 13167.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1375, 'learning_rate': 8.01520190023753e-05, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.28894466161727905, 'eval_accuracy': 0.9311926605504587, 'eval_runtime': 5.8006, 'eval_samples_per_second': 150.33, 'eval_steps_per_second': 9.482, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1218, 'learning_rate': 4.0171021377672214e-05, 'epoch': 4.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1223, 'learning_rate': 1.9002375296912116e-07, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 9908.500 Total: 23028.000 (43.0% used). Free: 13119.500\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3046474754810333, 'eval_accuracy': 0.9288990825688074, 'eval_runtime': 5.8318, 'eval_samples_per_second': 149.525, 'eval_steps_per_second': 9.431, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 3679.2251, 'train_samples_per_second': 91.526, 'train_steps_per_second': 5.721, 'train_loss': 0.16405935596966686, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.28894466161727905, 'eval_accuracy': 0.9311926605504587, 'eval_runtime': 5.736, 'eval_samples_per_second': 152.022, 'eval_steps_per_second': 9.589, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m2023-09-18 16:33:09,429 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-18 16:33:09,430 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-18 16:33:09,430 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 4098\n",
      "Billable seconds: 1231\n",
      "Managed Spot Training savings: 70.0%\n",
      "\n",
      "\n",
      "\n",
      "##### Log of pytorch-training-2023-09-18-15-24-01-863 (full, 16):\n",
      "\n",
      "2023-09-18 15:53:49 Starting - Preparing the instances for training\n",
      "2023-09-18 15:53:49 Downloading - Downloading input data\n",
      "2023-09-18 15:53:49 Training - Training image download completed. Training in progress.\n",
      "2023-09-18 15:53:49 Uploading - Uploading generated training model\n",
      "2023-09-18 15:53:49 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:38,350 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:38,363 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:38,372 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:38,379 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:40,064 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for transformers from https://files.pythonhosted.org/packages/1a/06/3817f9bb923437ead9a794f0ac0d03b8b5e0478ab112db4c413dd37c09da/transformers-4.33.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\u001b[0m\n",
      "\u001b[34m 119.9/119.9 kB 8.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.22.0)\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m 81.4/81.4 kB 15.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for peft from https://files.pythonhosted.org/packages/37/1a/8d20e8704da9fa070eb909265584b960da57be1d833d550c59f50906dc5c/peft-0.5.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl.metadata (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m 53.1/53.1 kB 17.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.15.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for huggingface-hub<1.0,>=0.15.1 from https://files.pythonhosted.org/packages/72/21/51cddb8850ed3f4dbc21e57c3dabc49e64d5577857ddda7b2eb0ffc2ec0e/huggingface_hub-0.17.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.2-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/d1/df/460ca6171a8494fcf37af43f52f6fac23e38784bb4a26563f6fa01ef6faf/regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m 40.9/40.9 kB 12.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m 7.8/7.8 MB 91.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/6c/f0/c17bbdb1e5f9dab29d44cade445135789f75f8f08ea2728d04493ea8412b/safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (13.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.3)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xxhash from https://files.pythonhosted.org/packages/13/c3/e942893f4864a424514c81640f114980cfd5aff7e7414d1e0255f4571111/xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m 114.5/114.5 kB 25.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34m 268.8/268.8 kB 54.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\u001b[0m\n",
      "\u001b[34m 7.6/7.6 MB 118.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.5-py3-none-any.whl (519 kB)\u001b[0m\n",
      "\u001b[34m 519.6/519.6 kB 58.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34m 85.6/85.6 kB 26.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m 1.0/1.0 MB 88.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\u001b[0m\n",
      "\u001b[34m 294.9/294.9 kB 62.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\u001b[0m\n",
      "\u001b[34m 771.9/771.9 kB 74.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m 1.3/1.3 MB 96.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m 194.1/194.1 kB 44.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\u001b[0m\n",
      "\u001b[34m 225.7/225.7 kB 42.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.5 evaluate-0.4.0 frozenlist-1.4.0 huggingface-hub-0.17.2 multidict-6.0.4 peft-0.5.0 pynvml-11.5.0 regex-2023.8.8 responses-0.18.0 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2 xxhash-3.3.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:51,585 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:51,585 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:51,616 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:51,639 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:51,662 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:51,672 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"1\",\n",
      "        \"learning-rate\": \"5e-05\",\n",
      "        \"use-bf16\": \"1\",\n",
      "        \"use-hf-lora\": \"0\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-09-18-15-24-01-863\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-24-01-863/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"1\",\"learning-rate\":\"5e-05\",\"use-bf16\":\"1\",\"use-hf-lora\":\"0\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-24-01-863/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"1\",\"learning-rate\":\"5e-05\",\"use-bf16\":\"1\",\"use-hf-lora\":\"0\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-09-18-15-24-01-863\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-24-01-863/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"1\",\"--learning-rate\",\"5e-05\",\"--use-bf16\",\"1\",\"--use-hf-lora\",\"0\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-SCALE=1\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=5e-05\u001b[0m\n",
      "\u001b[34mSM_HP_USE-BF16=1\u001b[0m\n",
      "\u001b[34mSM_HP_USE-HF-LORA=0\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 1 --learning-rate 5e-05 --use-bf16 1 --use-hf-lora 0\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:51,695 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:__main__:transformers 4.33.2\u001b[0m\n",
      "\u001b[34mINFO:__main__:peft 0.5.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:torch 2.0.1\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 309.562 Total: 23028.000 (1.3% used). Free: 22718.438\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json: 100%|| 481/481 [00:00<00:00, 4.46MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   2%|         | 10.5M/499M [00:00<00:06, 74.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   6%|         | 31.5M/499M [00:00<00:03, 126MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  11%|         | 52.4M/499M [00:00<00:02, 154MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  15%|        | 73.4M/499M [00:00<00:02, 170MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  19%|        | 94.4M/499M [00:00<00:02, 180MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  23%|       | 115M/499M [00:00<00:02, 186MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  27%|       | 136M/499M [00:00<00:01, 189MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  32%|      | 157M/499M [00:00<00:01, 192MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  36%|      | 178M/499M [00:01<00:01, 193MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  40%|      | 199M/499M [00:01<00:01, 195MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  44%|     | 220M/499M [00:01<00:01, 195MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  48%|     | 241M/499M [00:01<00:01, 195MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  53%|    | 262M/499M [00:01<00:01, 196MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  57%|    | 283M/499M [00:01<00:01, 196MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  61%|    | 304M/499M [00:01<00:00, 196MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  65%|   | 325M/499M [00:01<00:00, 194MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  69%|   | 346M/499M [00:01<00:00, 194MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  74%|  | 367M/499M [00:01<00:00, 194MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  78%|  | 388M/499M [00:02<00:00, 195MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  82%| | 409M/499M [00:02<00:00, 195MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  86%| | 430M/499M [00:02<00:00, 195MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  90%| | 451M/499M [00:02<00:00, 195MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  95%|| 472M/499M [00:02<00:00, 195MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  99%|| 493M/499M [00:02<00:00, 195MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|| 499M/499M [00:02<00:00, 188MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json: 100%|| 899k/899k [00:00<00:00, 12.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt: 100%|| 456k/456k [00:00<00:00, 3.37MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt: 100%|| 456k/456k [00:00<00:00, 3.36MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json: 100%|| 1.36M/1.36M [00:00<00:00, 18.4MB/s]\u001b[0m\n",
      "\u001b[34mINFO:__main__:Total parameters: 124,647,170, thereof learnable: 124,647,170 (100.0000%)\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average train input length: 14.36\u001b[0m\n",
      "\u001b[34mUsing bf16: True, LoRA: False\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.3337, 'learning_rate': 4.500237529691211e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.269, 'learning_rate': 4.000475059382423e-05, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|| 4.20k/4.20k [00:00<00:00, 6.00MB/s]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3343076705932617, 'eval_accuracy': 0.9071100917431193, 'eval_runtime': 1.2361, 'eval_samples_per_second': 705.47, 'eval_steps_per_second': 44.496, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.2005, 'learning_rate': 3.5007125890736345e-05, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.2062, 'learning_rate': 3.0009501187648458e-05, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.28203141689300537, 'eval_accuracy': 0.9094036697247706, 'eval_runtime': 1.1832, 'eval_samples_per_second': 737.001, 'eval_steps_per_second': 46.485, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1525, 'learning_rate': 2.501187648456057e-05, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.15, 'learning_rate': 2.0014251781472684e-05, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.33299052715301514, 'eval_accuracy': 0.9128440366972477, 'eval_runtime': 1.1955, 'eval_samples_per_second': 729.381, 'eval_steps_per_second': 46.005, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1096, 'learning_rate': 1.5016627078384798e-05, 'epoch': 3.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1059, 'learning_rate': 1.0019002375296913e-05, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.34382250905036926, 'eval_accuracy': 0.9151376146788991, 'eval_runtime': 1.1598, 'eval_samples_per_second': 751.838, 'eval_steps_per_second': 47.421, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.0779, 'learning_rate': 5.021377672209027e-06, 'epoch': 4.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3890.500 Total: 23028.000 (16.9% used). Free: 19137.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.0742, 'learning_rate': 2.3752969121140145e-08, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3493823707103729, 'eval_accuracy': 0.9185779816513762, 'eval_runtime': 1.1972, 'eval_samples_per_second': 728.387, 'eval_steps_per_second': 45.942, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 1259.2028, 'train_samples_per_second': 267.427, 'train_steps_per_second': 16.717, 'train_loss': 0.1678709905468936, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3493823707103729, 'eval_accuracy': 0.9185779816513762, 'eval_runtime': 1.1892, 'eval_samples_per_second': 733.257, 'eval_steps_per_second': 46.249, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m2023-09-18 15:52:21,488 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:52:21,488 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:52:21,488 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 1715\n",
      "Billable seconds: 515\n",
      "Managed Spot Training savings: 70.0%\n",
      "\n",
      "\n",
      "\n",
      "##### Log of pytorch-training-2023-09-18-15-24-03-176 (full, 16):\n",
      "\n",
      "2023-09-18 16:23:22 Starting - Preparing the instances for training\n",
      "2023-09-18 16:23:22 Downloading - Downloading input data\n",
      "2023-09-18 16:23:22 Training - Training image download completed. Training in progress.\n",
      "2023-09-18 16:23:22 Uploading - Uploading generated training model\n",
      "2023-09-18 16:23:22 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:38,560 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:38,573 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:38,582 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:38,588 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:40,235 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for transformers from https://files.pythonhosted.org/packages/1a/06/3817f9bb923437ead9a794f0ac0d03b8b5e0478ab112db4c413dd37c09da/transformers-4.33.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\u001b[0m\n",
      "\u001b[34m 119.9/119.9 kB 8.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.22.0)\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m 81.4/81.4 kB 14.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for peft from https://files.pythonhosted.org/packages/37/1a/8d20e8704da9fa070eb909265584b960da57be1d833d550c59f50906dc5c/peft-0.5.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl.metadata (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m 53.1/53.1 kB 14.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.15.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for huggingface-hub<1.0,>=0.15.1 from https://files.pythonhosted.org/packages/72/21/51cddb8850ed3f4dbc21e57c3dabc49e64d5577857ddda7b2eb0ffc2ec0e/huggingface_hub-0.17.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.2-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/d1/df/460ca6171a8494fcf37af43f52f6fac23e38784bb4a26563f6fa01ef6faf/regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m 40.9/40.9 kB 10.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m 7.8/7.8 MB 71.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/6c/f0/c17bbdb1e5f9dab29d44cade445135789f75f8f08ea2728d04493ea8412b/safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (13.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.3)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xxhash from https://files.pythonhosted.org/packages/13/c3/e942893f4864a424514c81640f114980cfd5aff7e7414d1e0255f4571111/xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m 114.5/114.5 kB 27.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34m 268.8/268.8 kB 53.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\u001b[0m\n",
      "\u001b[34m 7.6/7.6 MB 71.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.5-py3-none-any.whl (519 kB)\u001b[0m\n",
      "\u001b[34m 519.6/519.6 kB 61.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34m 85.6/85.6 kB 26.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m 1.0/1.0 MB 92.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\u001b[0m\n",
      "\u001b[34m 294.9/294.9 kB 46.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\u001b[0m\n",
      "\u001b[34m 771.9/771.9 kB 85.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m 1.3/1.3 MB 91.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m 194.1/194.1 kB 49.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\u001b[0m\n",
      "\u001b[34m 225.7/225.7 kB 51.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.5 evaluate-0.4.0 frozenlist-1.4.0 huggingface-hub-0.17.2 multidict-6.0.4 peft-0.5.0 pynvml-11.5.0 regex-2023.8.8 responses-0.18.0 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2 xxhash-3.3.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:51,835 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:51,835 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:51,872 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:51,895 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:51,918 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:51,927 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"8\",\n",
      "        \"learning-rate\": \"5e-05\",\n",
      "        \"use-bf16\": \"1\",\n",
      "        \"use-hf-lora\": \"0\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-09-18-15-24-03-176\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-24-03-176/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"5e-05\",\"use-bf16\":\"1\",\"use-hf-lora\":\"0\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-24-03-176/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"5e-05\",\"use-bf16\":\"1\",\"use-hf-lora\":\"0\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-09-18-15-24-03-176\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-24-03-176/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"8\",\"--learning-rate\",\"5e-05\",\"--use-bf16\",\"1\",\"--use-hf-lora\",\"0\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-SCALE=8\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=5e-05\u001b[0m\n",
      "\u001b[34mSM_HP_USE-BF16=1\u001b[0m\n",
      "\u001b[34mSM_HP_USE-HF-LORA=0\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 8 --learning-rate 5e-05 --use-bf16 1 --use-hf-lora 0\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:51,950 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:__main__:transformers 4.33.2\u001b[0m\n",
      "\u001b[34mINFO:__main__:peft 0.5.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:torch 2.0.1\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 309.562 Total: 23028.000 (1.3% used). Free: 22718.438\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json: 100%|| 481/481 [00:00<00:00, 4.86MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   2%|         | 10.5M/499M [00:00<00:04, 98.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   6%|         | 31.5M/499M [00:00<00:03, 129MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  11%|         | 52.4M/499M [00:00<00:03, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  15%|        | 73.4M/499M [00:00<00:04, 95.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  17%|        | 83.9M/499M [00:00<00:04, 95.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  21%|        | 105M/499M [00:00<00:03, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  25%|       | 126M/499M [00:01<00:03, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  29%|       | 147M/499M [00:01<00:02, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  34%|      | 168M/499M [00:01<00:02, 125MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  38%|      | 189M/499M [00:01<00:02, 112MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  42%|     | 210M/499M [00:01<00:02, 97.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  46%|     | 231M/499M [00:02<00:02, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  50%|     | 252M/499M [00:02<00:02, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  55%|    | 273M/499M [00:02<00:01, 115MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  59%|    | 294M/499M [00:02<00:01, 126MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  63%|   | 315M/499M [00:02<00:01, 132MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  67%|   | 336M/499M [00:02<00:01, 134MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  71%|  | 357M/499M [00:03<00:01, 80.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  76%|  | 377M/499M [00:04<00:02, 57.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  78%|  | 388M/499M [00:04<00:01, 55.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  80%|  | 398M/499M [00:04<00:01, 51.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  82%| | 409M/499M [00:04<00:01, 45.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  84%| | 419M/499M [00:05<00:01, 42.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  86%| | 430M/499M [00:05<00:01, 41.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  88%| | 440M/499M [00:05<00:01, 39.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  90%| | 451M/499M [00:05<00:01, 38.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  92%|| 461M/499M [00:06<00:00, 39.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  95%|| 472M/499M [00:06<00:00, 39.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  97%|| 482M/499M [00:06<00:00, 38.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  99%|| 493M/499M [00:07<00:00, 38.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|| 499M/499M [00:07<00:00, 37.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|| 499M/499M [00:07<00:00, 69.2MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json: 100%|| 899k/899k [00:00<00:00, 3.75MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json: 100%|| 899k/899k [00:00<00:00, 3.74MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt: 100%|| 456k/456k [00:00<00:00, 1.90MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt: 100%|| 456k/456k [00:00<00:00, 1.90MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json: 100%|| 1.36M/1.36M [00:00<00:00, 3.41MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json: 100%|| 1.36M/1.36M [00:00<00:00, 3.40MB/s]\u001b[0m\n",
      "\u001b[34mINFO:__main__:Total parameters: 124,647,170, thereof learnable: 124,647,170 (100.0000%)\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 309.562 Total: 23028.000 (1.3% used). Free: 22718.438\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average train input length: 92.13\u001b[0m\n",
      "\u001b[34mUsing bf16: True, LoRA: False\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 8010.500 Total: 23028.000 (34.8% used). Free: 15017.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10290.500 Total: 23028.000 (44.7% used). Free: 12737.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10290.500 Total: 23028.000 (44.7% used). Free: 12737.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 15060.500 Total: 23028.000 (65.4% used). Free: 7967.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 20460.500 Total: 23028.000 (88.9% used). Free: 2567.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10428.500 Total: 23028.000 (45.3% used). Free: 12599.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10428.500 Total: 23028.000 (45.3% used). Free: 12599.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10428.500 Total: 23028.000 (45.3% used). Free: 12599.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10428.500 Total: 23028.000 (45.3% used). Free: 12599.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10450.500 Total: 23028.000 (45.4% used). Free: 12577.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.3218, 'learning_rate': 4.500237529691211e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10450.500 Total: 23028.000 (45.4% used). Free: 12577.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10450.500 Total: 23028.000 (45.4% used). Free: 12577.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10450.500 Total: 23028.000 (45.4% used). Free: 12577.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10450.500 Total: 23028.000 (45.4% used). Free: 12577.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10450.500 Total: 23028.000 (45.4% used). Free: 12577.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10450.500 Total: 23028.000 (45.4% used). Free: 12577.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10450.500 Total: 23028.000 (45.4% used). Free: 12577.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10450.500 Total: 23028.000 (45.4% used). Free: 12577.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10450.500 Total: 23028.000 (45.4% used). Free: 12577.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10450.500 Total: 23028.000 (45.4% used). Free: 12577.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.2856, 'learning_rate': 4.000475059382423e-05, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|| 4.20k/4.20k [00:00<00:00, 5.28MB/s]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.34819144010543823, 'eval_accuracy': 0.9002293577981652, 'eval_runtime': 3.6102, 'eval_samples_per_second': 241.535, 'eval_steps_per_second': 15.234, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.2171, 'learning_rate': 3.5007125890736345e-05, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10478.500 Total: 23028.000 (45.5% used). Free: 12549.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.2001, 'learning_rate': 3.0009501187648458e-05, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2968972623348236, 'eval_accuracy': 0.8967889908256881, 'eval_runtime': 3.4519, 'eval_samples_per_second': 252.615, 'eval_steps_per_second': 15.933, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.146, 'learning_rate': 2.501187648456057e-05, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1412, 'learning_rate': 2.0014251781472684e-05, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3391381800174713, 'eval_accuracy': 0.9105504587155964, 'eval_runtime': 3.4727, 'eval_samples_per_second': 251.1, 'eval_steps_per_second': 15.838, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.0997, 'learning_rate': 1.5016627078384798e-05, 'epoch': 3.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.102, 'learning_rate': 1.0019002375296913e-05, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.34728899598121643, 'eval_accuracy': 0.908256880733945, 'eval_runtime': 3.3683, 'eval_samples_per_second': 258.887, 'eval_steps_per_second': 16.329, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.0668, 'learning_rate': 5.021377672209027e-06, 'epoch': 4.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 10548.500 Total: 23028.000 (45.8% used). Free: 12479.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.0632, 'learning_rate': 2.3752969121140145e-08, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.40246617794036865, 'eval_accuracy': 0.9071100917431193, 'eval_runtime': 3.5922, 'eval_samples_per_second': 242.748, 'eval_steps_per_second': 15.311, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 3008.6413, 'train_samples_per_second': 111.926, 'train_steps_per_second': 6.997, 'train_loss': 0.16429755226166967, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3391381800174713, 'eval_accuracy': 0.9105504587155964, 'eval_runtime': 3.4224, 'eval_samples_per_second': 254.791, 'eval_steps_per_second': 16.071, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m2023-09-18 16:21:56,655 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-18 16:21:56,655 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-18 16:21:56,656 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 3485\n",
      "Billable seconds: 1047\n",
      "Managed Spot Training savings: 70.0%\n",
      "\n",
      "\n",
      "\n",
      "##### Log of pytorch-training-2023-09-18-15-24-04-500 (full, 32):\n",
      "\n",
      "2023-09-18 15:54:16 Starting - Preparing the instances for training\n",
      "2023-09-18 15:54:16 Downloading - Downloading input data\n",
      "2023-09-18 15:54:16 Training - Training image download completed. Training in progress.\n",
      "2023-09-18 15:54:16 Uploading - Uploading generated training model\n",
      "2023-09-18 15:54:16 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:41,534 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:41,547 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:41,556 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:41,565 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:43,567 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for transformers from https://files.pythonhosted.org/packages/1a/06/3817f9bb923437ead9a794f0ac0d03b8b5e0478ab112db4c413dd37c09da/transformers-4.33.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\u001b[0m\n",
      "\u001b[34m 119.9/119.9 kB 10.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.22.0)\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m 81.4/81.4 kB 22.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for peft from https://files.pythonhosted.org/packages/37/1a/8d20e8704da9fa070eb909265584b960da57be1d833d550c59f50906dc5c/peft-0.5.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl.metadata (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m 53.1/53.1 kB 13.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.15.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for huggingface-hub<1.0,>=0.15.1 from https://files.pythonhosted.org/packages/72/21/51cddb8850ed3f4dbc21e57c3dabc49e64d5577857ddda7b2eb0ffc2ec0e/huggingface_hub-0.17.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.2-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/d1/df/460ca6171a8494fcf37af43f52f6fac23e38784bb4a26563f6fa01ef6faf/regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m 40.9/40.9 kB 12.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m 7.8/7.8 MB 83.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/6c/f0/c17bbdb1e5f9dab29d44cade445135789f75f8f08ea2728d04493ea8412b/safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (13.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.3)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xxhash from https://files.pythonhosted.org/packages/13/c3/e942893f4864a424514c81640f114980cfd5aff7e7414d1e0255f4571111/xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m 114.5/114.5 kB 24.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34m 268.8/268.8 kB 46.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\u001b[0m\n",
      "\u001b[34m 7.6/7.6 MB 81.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.5-py3-none-any.whl (519 kB)\u001b[0m\n",
      "\u001b[34m 519.6/519.6 kB 38.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34m 85.6/85.6 kB 7.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m 1.0/1.0 MB 62.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\u001b[0m\n",
      "\u001b[34m 294.9/294.9 kB 49.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\u001b[0m\n",
      "\u001b[34m 771.9/771.9 kB 71.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m 1.3/1.3 MB 80.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m 194.1/194.1 kB 44.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\u001b[0m\n",
      "\u001b[34m 225.7/225.7 kB 40.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.5 evaluate-0.4.0 frozenlist-1.4.0 huggingface-hub-0.17.2 multidict-6.0.4 peft-0.5.0 pynvml-11.5.0 regex-2023.8.8 responses-0.18.0 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2 xxhash-3.3.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:55,344 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:55,344 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:55,358 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:55,381 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:55,404 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:55,414 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"1\",\n",
      "        \"learning-rate\": \"5e-05\",\n",
      "        \"use-bf16\": \"0\",\n",
      "        \"use-hf-lora\": \"0\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-09-18-15-24-04-500\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-24-04-500/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"1\",\"learning-rate\":\"5e-05\",\"use-bf16\":\"0\",\"use-hf-lora\":\"0\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-24-04-500/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"1\",\"learning-rate\":\"5e-05\",\"use-bf16\":\"0\",\"use-hf-lora\":\"0\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-09-18-15-24-04-500\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-24-04-500/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"1\",\"--learning-rate\",\"5e-05\",\"--use-bf16\",\"0\",\"--use-hf-lora\",\"0\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-SCALE=1\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=5e-05\u001b[0m\n",
      "\u001b[34mSM_HP_USE-BF16=0\u001b[0m\n",
      "\u001b[34mSM_HP_USE-HF-LORA=0\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 1 --learning-rate 5e-05 --use-bf16 0 --use-hf-lora 0\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:55,437 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:__main__:transformers 4.33.2\u001b[0m\n",
      "\u001b[34mINFO:__main__:peft 0.5.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:torch 2.0.1\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 309.562 Total: 23028.000 (1.3% used). Free: 22718.438\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json: 100%|| 481/481 [00:00<00:00, 4.26MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   2%|         | 10.5M/499M [00:00<00:05, 84.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   4%|         | 21.0M/499M [00:00<00:05, 91.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   8%|         | 41.9M/499M [00:00<00:04, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  11%|         | 52.4M/499M [00:00<00:04, 97.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  15%|        | 73.4M/499M [00:00<00:03, 127MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  19%|        | 94.4M/499M [00:00<00:03, 122MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  23%|       | 115M/499M [00:00<00:03, 127MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  27%|       | 136M/499M [00:01<00:02, 141MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  32%|      | 157M/499M [00:01<00:02, 144MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  36%|      | 178M/499M [00:01<00:02, 142MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  40%|      | 199M/499M [00:01<00:02, 136MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  44%|     | 220M/499M [00:01<00:02, 133MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  48%|     | 241M/499M [00:01<00:01, 142MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  53%|    | 262M/499M [00:02<00:01, 141MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  57%|    | 283M/499M [00:02<00:01, 153MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  61%|    | 304M/499M [00:02<00:01, 150MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  65%|   | 325M/499M [00:02<00:01, 111MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  69%|   | 346M/499M [00:02<00:01, 114MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  74%|  | 367M/499M [00:02<00:01, 114MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  78%|  | 388M/499M [00:03<00:00, 113MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  82%| | 409M/499M [00:03<00:00, 96.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  86%| | 430M/499M [00:03<00:00, 85.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  90%| | 451M/499M [00:03<00:00, 96.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  95%|| 472M/499M [00:03<00:00, 110MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  99%|| 493M/499M [00:04<00:00, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|| 499M/499M [00:04<00:00, 120MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json: 100%|| 899k/899k [00:00<00:00, 11.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt: 100%|| 456k/456k [00:00<00:00, 36.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json: 100%|| 1.36M/1.36M [00:00<00:00, 9.29MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json: 100%|| 1.36M/1.36M [00:00<00:00, 9.24MB/s]\u001b[0m\n",
      "\u001b[34mINFO:__main__:Total parameters: 124,647,170, thereof learnable: 124,647,170 (100.0000%)\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average train input length: 14.36\u001b[0m\n",
      "\u001b[34mUsing bf16: False, LoRA: False\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.3511, 'learning_rate': 4.500237529691211e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.3042, 'learning_rate': 4.000475059382423e-05, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|| 4.20k/4.20k [00:00<00:00, 4.40MB/s]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.31085309386253357, 'eval_accuracy': 0.8944954128440367, 'eval_runtime': 1.3041, 'eval_samples_per_second': 668.676, 'eval_steps_per_second': 42.176, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.4154, 'learning_rate': 3.5007125890736345e-05, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.2796, 'learning_rate': 3.0009501187648458e-05, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3032878637313843, 'eval_accuracy': 0.9036697247706422, 'eval_runtime': 1.1547, 'eval_samples_per_second': 755.187, 'eval_steps_per_second': 47.632, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.201, 'learning_rate': 2.501187648456057e-05, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1834, 'learning_rate': 2.0014251781472684e-05, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3313967287540436, 'eval_accuracy': 0.9048165137614679, 'eval_runtime': 1.1411, 'eval_samples_per_second': 764.146, 'eval_steps_per_second': 48.197, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1583, 'learning_rate': 1.5016627078384798e-05, 'epoch': 3.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1376, 'learning_rate': 1.0019002375296913e-05, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3435561954975128, 'eval_accuracy': 0.9162844036697247, 'eval_runtime': 1.207, 'eval_samples_per_second': 722.436, 'eval_steps_per_second': 45.567, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1146, 'learning_rate': 5.021377672209027e-06, 'epoch': 4.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3866.500 Total: 23028.000 (16.8% used). Free: 19161.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3868.500 Total: 23028.000 (16.8% used). Free: 19159.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3868.500 Total: 23028.000 (16.8% used). Free: 19159.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 3868.500 Total: 23028.000 (16.8% used). Free: 19159.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.103, 'learning_rate': 2.3752969121140145e-08, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.35804975032806396, 'eval_accuracy': 0.9162844036697247, 'eval_runtime': 1.1582, 'eval_samples_per_second': 752.911, 'eval_steps_per_second': 47.489, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 1280.4795, 'train_samples_per_second': 262.984, 'train_steps_per_second': 16.439, 'train_loss': 0.22483037931618952, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3435561954975128, 'eval_accuracy': 0.9162844036697247, 'eval_runtime': 1.121, 'eval_samples_per_second': 777.876, 'eval_steps_per_second': 49.063, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m2023-09-18 15:52:49,214 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:52:49,214 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:52:49,215 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 1741\n",
      "Billable seconds: 523\n",
      "Managed Spot Training savings: 70.0%\n",
      "\n",
      "\n",
      "\n",
      "##### Log of pytorch-training-2023-09-18-15-24-05-911 (full, 32):\n",
      "\n",
      "2023-09-18 16:50:58 Starting - Preparing the instances for training\n",
      "2023-09-18 16:50:58 Downloading - Downloading input data\n",
      "2023-09-18 16:50:58 Training - Training image download completed. Training in progress.\n",
      "2023-09-18 16:50:58 Uploading - Uploading generated training model\n",
      "2023-09-18 16:50:58 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:35,255 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:35,268 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:35,277 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:35,284 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:37,001 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for transformers from https://files.pythonhosted.org/packages/1a/06/3817f9bb923437ead9a794f0ac0d03b8b5e0478ab112db4c413dd37c09da/transformers-4.33.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\u001b[0m\n",
      "\u001b[34m 119.9/119.9 kB 9.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.22.0)\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m 81.4/81.4 kB 21.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for peft from https://files.pythonhosted.org/packages/37/1a/8d20e8704da9fa070eb909265584b960da57be1d833d550c59f50906dc5c/peft-0.5.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl.metadata (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m 53.1/53.1 kB 16.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.15.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for huggingface-hub<1.0,>=0.15.1 from https://files.pythonhosted.org/packages/72/21/51cddb8850ed3f4dbc21e57c3dabc49e64d5577857ddda7b2eb0ffc2ec0e/huggingface_hub-0.17.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.2-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/d1/df/460ca6171a8494fcf37af43f52f6fac23e38784bb4a26563f6fa01ef6faf/regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m 40.9/40.9 kB 10.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m 7.8/7.8 MB 82.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/6c/f0/c17bbdb1e5f9dab29d44cade445135789f75f8f08ea2728d04493ea8412b/safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (13.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.3)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xxhash from https://files.pythonhosted.org/packages/13/c3/e942893f4864a424514c81640f114980cfd5aff7e7414d1e0255f4571111/xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m 114.5/114.5 kB 33.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34m 268.8/268.8 kB 47.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\u001b[0m\n",
      "\u001b[34m 7.6/7.6 MB 111.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.5-py3-none-any.whl (519 kB)\u001b[0m\n",
      "\u001b[34m 519.6/519.6 kB 64.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34m 85.6/85.6 kB 25.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m 1.0/1.0 MB 86.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\u001b[0m\n",
      "\u001b[34m 294.9/294.9 kB 66.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\u001b[0m\n",
      "\u001b[34m 771.9/771.9 kB 77.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m 1.3/1.3 MB 97.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m 194.1/194.1 kB 49.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\u001b[0m\n",
      "\u001b[34m 225.7/225.7 kB 42.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, safetensors, xxhash, regex, pynvml, multidict, frozenlist, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.5 evaluate-0.4.0 frozenlist-1.4.0 huggingface-hub-0.17.2 multidict-6.0.4 peft-0.5.0 pynvml-11.5.0 regex-2023.8.8 responses-0.18.0 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2 xxhash-3.3.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:48,488 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:48,488 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:48,526 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:48,549 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:48,572 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:48,582 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 16,\n",
      "        \"hf-ckp\": \"roberta-base\",\n",
      "        \"input-scale\": \"8\",\n",
      "        \"learning-rate\": \"5e-05\",\n",
      "        \"use-bf16\": \"0\",\n",
      "        \"use-hf-lora\": \"0\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-09-18-15-24-05-911\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-24-05-911/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"5e-05\",\"use-bf16\":\"0\",\"use-hf-lora\":\"0\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-24-05-911/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"hf-ckp\":\"roberta-base\",\"input-scale\":\"8\",\"learning-rate\":\"5e-05\",\"use-bf16\":\"0\",\"use-hf-lora\":\"0\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-09-18-15-24-05-911\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-753739741425/pytorch-training-2023-09-18-15-24-05-911/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--hf-ckp\",\"roberta-base\",\"--input-scale\",\"8\",\"--learning-rate\",\"5e-05\",\"--use-bf16\",\"0\",\"--use-hf-lora\",\"0\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_HF-CKP=roberta-base\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-SCALE=8\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=5e-05\u001b[0m\n",
      "\u001b[34mSM_HP_USE-BF16=0\u001b[0m\n",
      "\u001b[34mSM_HP_USE-HF-LORA=0\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --batch-size 16 --hf-ckp roberta-base --input-scale 8 --learning-rate 5e-05 --use-bf16 0 --use-hf-lora 0\u001b[0m\n",
      "\u001b[34m2023-09-18 15:30:48,605 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:__main__:transformers 4.33.2\u001b[0m\n",
      "\u001b[34mINFO:__main__:peft 0.5.0\u001b[0m\n",
      "\u001b[34mINFO:__main__:torch 2.0.1\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 309.562 Total: 23028.000 (1.3% used). Free: 22718.438\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()lve/main/config.json: 100%|| 481/481 [00:00<00:00, 5.38MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   2%|         | 10.5M/499M [00:00<00:06, 75.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:   6%|         | 31.5M/499M [00:00<00:03, 137MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  11%|         | 52.4M/499M [00:00<00:02, 162MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  15%|        | 73.4M/499M [00:00<00:02, 173MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  19%|        | 94.4M/499M [00:00<00:02, 181MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  23%|       | 115M/499M [00:00<00:02, 186MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  27%|       | 136M/499M [00:00<00:01, 189MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  32%|      | 157M/499M [00:00<00:01, 191MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  36%|      | 178M/499M [00:01<00:01, 193MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  40%|      | 199M/499M [00:01<00:01, 194MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  44%|     | 220M/499M [00:01<00:01, 194MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  48%|     | 241M/499M [00:01<00:01, 195MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  53%|    | 262M/499M [00:01<00:01, 195MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  57%|    | 283M/499M [00:01<00:01, 195MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  61%|    | 304M/499M [00:01<00:01, 195MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  65%|   | 325M/499M [00:01<00:00, 195MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  69%|   | 346M/499M [00:01<00:00, 196MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  74%|  | 367M/499M [00:01<00:00, 196MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  78%|  | 388M/499M [00:02<00:00, 196MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  82%| | 409M/499M [00:02<00:00, 194MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  86%| | 430M/499M [00:02<00:00, 195MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  90%| | 451M/499M [00:02<00:00, 195MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  95%|| 472M/499M [00:02<00:00, 196MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors:  99%|| 493M/499M [00:02<00:00, 196MB/s]\u001b[0m\n",
      "\u001b[34mDownloading model.safetensors: 100%|| 499M/499M [00:02<00:00, 189MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json: 100%|| 899k/899k [00:00<00:00, 3.72MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/vocab.json: 100%|| 899k/899k [00:00<00:00, 3.71MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()olve/main/merges.txt: 100%|| 456k/456k [00:00<00:00, 45.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json: 100%|| 1.36M/1.36M [00:00<00:00, 3.41MB/s]\u001b[0m\n",
      "\u001b[34mDownloading ()/main/tokenizer.json: 100%|| 1.36M/1.36M [00:00<00:00, 3.41MB/s]\u001b[0m\n",
      "\u001b[34mINFO:__main__:Total parameters: 124,647,170, thereof learnable: 124,647,170 (100.0000%)\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 309.562 Total: 23028.000 (1.3% used). Free: 22718.438\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average train input length: 92.13\u001b[0m\n",
      "\u001b[34mUsing bf16: False, LoRA: False\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 15484.500 Total: 23028.000 (67.2% used). Free: 7543.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21916.500 Total: 23028.000 (95.2% used). Free: 1111.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21916.500 Total: 23028.000 (95.2% used). Free: 1111.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 21916.500 Total: 23028.000 (95.2% used). Free: 1111.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 11640.500 Total: 23028.000 (50.5% used). Free: 11387.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 20324.500 Total: 23028.000 (88.3% used). Free: 2703.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13274.500 Total: 23028.000 (57.6% used). Free: 9753.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 13274.500 Total: 23028.000 (57.6% used). Free: 9753.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18266.500 Total: 23028.000 (79.3% used). Free: 4761.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18266.500 Total: 23028.000 (79.3% used). Free: 4761.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18266.500 Total: 23028.000 (79.3% used). Free: 4761.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18266.500 Total: 23028.000 (79.3% used). Free: 4761.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18266.500 Total: 23028.000 (79.3% used). Free: 4761.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18266.500 Total: 23028.000 (79.3% used). Free: 4761.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18288.500 Total: 23028.000 (79.4% used). Free: 4739.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.2941, 'learning_rate': 4.500237529691211e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18288.500 Total: 23028.000 (79.4% used). Free: 4739.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18288.500 Total: 23028.000 (79.4% used). Free: 4739.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18288.500 Total: 23028.000 (79.4% used). Free: 4739.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18288.500 Total: 23028.000 (79.4% used). Free: 4739.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18288.500 Total: 23028.000 (79.4% used). Free: 4739.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18288.500 Total: 23028.000 (79.4% used). Free: 4739.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18288.500 Total: 23028.000 (79.4% used). Free: 4739.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18288.500 Total: 23028.000 (79.4% used). Free: 4739.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18288.500 Total: 23028.000 (79.4% used). Free: 4739.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18288.500 Total: 23028.000 (79.4% used). Free: 4739.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18288.500 Total: 23028.000 (79.4% used). Free: 4739.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18288.500 Total: 23028.000 (79.4% used). Free: 4739.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18288.500 Total: 23028.000 (79.4% used). Free: 4739.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18288.500 Total: 23028.000 (79.4% used). Free: 4739.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18288.500 Total: 23028.000 (79.4% used). Free: 4739.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.2354, 'learning_rate': 4.000475059382423e-05, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18288.500 Total: 23028.000 (79.4% used). Free: 4739.500\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|| 4.20k/4.20k [00:00<00:00, 5.75MB/s]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2918681800365448, 'eval_accuracy': 0.9151376146788991, 'eval_runtime': 5.5999, 'eval_samples_per_second': 155.718, 'eval_steps_per_second': 9.822, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1735, 'learning_rate': 3.5007125890736345e-05, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1755, 'learning_rate': 3.0009501187648458e-05, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.42474210262298584, 'eval_accuracy': 0.8899082568807339, 'eval_runtime': 5.4442, 'eval_samples_per_second': 160.171, 'eval_steps_per_second': 10.103, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1338, 'learning_rate': 2.501187648456057e-05, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.1348, 'learning_rate': 2.0014251781472684e-05, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3660094141960144, 'eval_accuracy': 0.9094036697247706, 'eval_runtime': 5.4406, 'eval_samples_per_second': 160.276, 'eval_steps_per_second': 10.109, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.0969, 'learning_rate': 1.5016627078384798e-05, 'epoch': 3.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18330.500 Total: 23028.000 (79.6% used). Free: 4697.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.0955, 'learning_rate': 1.0019002375296913e-05, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3572072386741638, 'eval_accuracy': 0.9139908256880734, 'eval_runtime': 5.4396, 'eval_samples_per_second': 160.305, 'eval_steps_per_second': 10.111, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.0657, 'learning_rate': 5.021377672209027e-06, 'epoch': 4.5}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34m{'loss': 0.0593, 'learning_rate': 2.3752969121140145e-08, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34mINFO:__main__:GPU Usage. Used: 18386.500 Total: 23028.000 (79.8% used). Free: 4641.500\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.33587324619293213, 'eval_accuracy': 0.9174311926605505, 'eval_runtime': 5.5269, 'eval_samples_per_second': 157.774, 'eval_steps_per_second': 9.951, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 4614.776, 'train_samples_per_second': 72.971, 'train_steps_per_second': 4.561, 'train_loss': 0.1463868202591169, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.33587324619293213, 'eval_accuracy': 0.9174311926605505, 'eval_runtime': 5.4225, 'eval_samples_per_second': 160.81, 'eval_steps_per_second': 10.143, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m2023-09-18 16:48:38,811 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-09-18 16:48:38,811 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-09-18 16:48:38,811 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 5142\n",
      "Billable seconds: 1545\n",
      "Managed Spot Training savings: 70.0%\n"
     ]
    }
   ],
   "source": [
    "for type, bits, est, input_scale in estimators:\n",
    "    job_name = est.latest_training_job.job_name\n",
    "    sm.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\n",
    "    print(f'\\n\\n\\n##### Log of {job_name} ({type}, {bits}):\\n')\n",
    "    est.logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505b4f2-cfb9-4952-a94e-a0517ade8750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
